
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../../distributed_training_systems/">
      
      
        <link rel="next" href="../../../references/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Supervised Fine-Tuning (SFT) - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-what-sft-optimizes" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Supervised Fine-Tuning (SFT)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PEFT Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../finetuning_techniques/prefix_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prefix Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../finetuning_techniques/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../finetuning_techniques/qlora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supporting Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supporting Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/4bit_normal_float/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4-bit NormalFloat (NF4)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/blockwise_kbit_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Block-wise k-bit Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/accelerate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerate Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/bp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BP16
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/decoding_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding & Medusa
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supporting_topics/kv_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Training Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../foundation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Foundation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Pre-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/pre_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/byte_pair_encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Byte Pair Encoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/mixture_of_experts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts (MoE)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/RoPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RoPe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/sentence_piece_unigram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sentence Piece Unigram
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pre_training/advanced_transformer_components_and_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Transformer Components & Layers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_optimization_and_stability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Optimization and Stability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed_training_systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Training Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" checked>
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Post-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning (SFT)
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning (SFT)
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-sft-optimizes" class="md-nav__link">
    <span class="md-ellipsis">
      1. What SFT Optimizes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-instruction-tuning-and-task-formatting" class="md-nav__link">
    <span class="md-ellipsis">
      2. Instruction Tuning and Task Formatting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Instruction Tuning and Task Formatting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-and-prompt-formatting" class="md-nav__link">
    <span class="md-ellipsis">
      Task and Prompt Formatting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-formatting-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Formatting Matters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-diversity-as-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Diversity as Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prompt Diversity as Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#semantic-diversity" class="md-nav__link">
    <span class="md-ellipsis">
      Semantic Diversity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structural-diversity" class="md-nav__link">
    <span class="md-ellipsis">
      Structural Diversity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#513-human-vs-synthetic-supervision" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.3 Human vs Synthetic Supervision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.3 Human vs Synthetic Supervision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-labeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      Human-Labeled Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#synthetic-supervision" class="md-nav__link">
    <span class="md-ellipsis">
      Synthetic Supervision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Synthetic Supervision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Instruct
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evol-instruct-as-curriculum-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Evol-Instruct as Curriculum Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rejection-sampling-best-of-n" class="md-nav__link">
    <span class="md-ellipsis">
      Rejection Sampling (Best-of-N)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#514-data-quality-over-quantity" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.4 Data Quality over Quantity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.4 Data Quality over Quantity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-lima-hypothesis" class="md-nav__link">
    <span class="md-ellipsis">
      The LIMA Hypothesis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-sft-data-mix" class="md-nav__link">
    <span class="md-ellipsis">
      Typical SFT Data Mix
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#515-training-optimizations-and-stability" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.5 Training Optimizations and Stability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.5 Training Optimizations and Stability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#packing-vs-padding" class="md-nav__link">
    <span class="md-ellipsis">
      Packing vs Padding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Packing vs Padding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#padding" class="md-nav__link">
    <span class="md-ellipsis">
      Padding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#packing" class="md-nav__link">
    <span class="md-ellipsis">
      Packing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-packing-improves-gpu-utilization" class="md-nav__link">
    <span class="md-ellipsis">
      Why Packing Improves GPU Utilization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Important Implementation Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#padding-vs-packing-summary" class="md-nav__link">
    <span class="md-ellipsis">
      Padding vs Packing Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interview-insight" class="md-nav__link">
    <span class="md-ellipsis">
      Interview Insight
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#516-overfitting-and-catastrophic-forgetting" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.6 Overfitting and Catastrophic Forgetting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.6 Overfitting and Catastrophic Forgetting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#catastrophic-forgetting" class="md-nav__link">
    <span class="md-ellipsis">
      Catastrophic Forgetting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Overfitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#517-lora-vs-full-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.7 LoRA vs Full Fine-Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.7 LoRA vs Full Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lora-and-peft" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA and PEFT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Full Fine-Tuning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#518-common-failure-modes-after-sft" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.8 Common Failure Modes After SFT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.8 Common Failure Modes After SFT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#increased-hallucinations" class="md-nav__link">
    <span class="md-ellipsis">
      Increased Hallucinations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#519-sft-vs-pre-training-summary" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.9 SFT vs Pre-training Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5110-interview-level-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.10 Interview-Level Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-sft-optimizes" class="md-nav__link">
    <span class="md-ellipsis">
      1. What SFT Optimizes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-instruction-tuning-and-task-formatting" class="md-nav__link">
    <span class="md-ellipsis">
      2. Instruction Tuning and Task Formatting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Instruction Tuning and Task Formatting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-and-prompt-formatting" class="md-nav__link">
    <span class="md-ellipsis">
      Task and Prompt Formatting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-formatting-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Formatting Matters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-diversity-as-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Diversity as Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prompt Diversity as Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#semantic-diversity" class="md-nav__link">
    <span class="md-ellipsis">
      Semantic Diversity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structural-diversity" class="md-nav__link">
    <span class="md-ellipsis">
      Structural Diversity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#513-human-vs-synthetic-supervision" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.3 Human vs Synthetic Supervision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.3 Human vs Synthetic Supervision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-labeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      Human-Labeled Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#synthetic-supervision" class="md-nav__link">
    <span class="md-ellipsis">
      Synthetic Supervision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Synthetic Supervision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Instruct
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evol-instruct-as-curriculum-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Evol-Instruct as Curriculum Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rejection-sampling-best-of-n" class="md-nav__link">
    <span class="md-ellipsis">
      Rejection Sampling (Best-of-N)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#514-data-quality-over-quantity" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.4 Data Quality over Quantity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.4 Data Quality over Quantity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-lima-hypothesis" class="md-nav__link">
    <span class="md-ellipsis">
      The LIMA Hypothesis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typical-sft-data-mix" class="md-nav__link">
    <span class="md-ellipsis">
      Typical SFT Data Mix
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#515-training-optimizations-and-stability" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.5 Training Optimizations and Stability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.5 Training Optimizations and Stability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#packing-vs-padding" class="md-nav__link">
    <span class="md-ellipsis">
      Packing vs Padding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Packing vs Padding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#padding" class="md-nav__link">
    <span class="md-ellipsis">
      Padding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#packing" class="md-nav__link">
    <span class="md-ellipsis">
      Packing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-packing-improves-gpu-utilization" class="md-nav__link">
    <span class="md-ellipsis">
      Why Packing Improves GPU Utilization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Important Implementation Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#padding-vs-packing-summary" class="md-nav__link">
    <span class="md-ellipsis">
      Padding vs Packing Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interview-insight" class="md-nav__link">
    <span class="md-ellipsis">
      Interview Insight
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#516-overfitting-and-catastrophic-forgetting" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.6 Overfitting and Catastrophic Forgetting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.6 Overfitting and Catastrophic Forgetting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#catastrophic-forgetting" class="md-nav__link">
    <span class="md-ellipsis">
      Catastrophic Forgetting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Overfitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#517-lora-vs-full-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.7 LoRA vs Full Fine-Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.7 LoRA vs Full Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lora-and-peft" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA and PEFT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Full Fine-Tuning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#518-common-failure-modes-after-sft" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.8 Common Failure Modes After SFT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1.8 Common Failure Modes After SFT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#increased-hallucinations" class="md-nav__link">
    <span class="md-ellipsis">
      Increased Hallucinations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#519-sft-vs-pre-training-summary" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.9 SFT vs Pre-training Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5110-interview-level-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      5.1.10 Interview-Level Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Supervised Fine-Tuning (SFT)</h1>

<p>Supervised Fine-Tuning (SFT) is the stage where a <strong>pre-trained base model</strong> is transformed into a <strong>useful assistant</strong> that follows instructions, respects formats, and exhibits desired interaction behavior. While pre-training builds a broad world model, SFT shapes <em>how</em> that knowledge is expressed.</p>
<p>From an interview perspective, SFT is best understood as <strong>behavioral alignment via supervised learning</strong>.</p>
<hr />
<h2 id="1-what-sft-optimizes">1. What SFT Optimizes<a class="headerlink" href="#1-what-sft-optimizes" title="Permanent link">&para;</a></h2>
<p>Map broad knowledge into a consistent, controllable interface.</p>
<p><strong>Conceptual shift</strong></p>
<ul>
<li>Pre-training: “Continue the text”</li>
<li>SFT: “Respond appropriately to a user instruction”</li>
</ul>
<p>This is achieved without reinforcement learning. The training signal is fully supervised.</p>
<p>Within SFT:</p>
<ul>
<li><strong>Instruction Tuning</strong> defines what behavior you are teaching</li>
<li><strong>Task Formatting</strong> defines how that behavior is presented to the model</li>
</ul>
<hr />
<hr />
<h2 id="2-instruction-tuning-and-task-formatting">2. Instruction Tuning and Task Formatting<a class="headerlink" href="#2-instruction-tuning-and-task-formatting" title="Permanent link">&para;</a></h2>
<h3 id="instruction-tuning">Instruction Tuning<a class="headerlink" href="#instruction-tuning" title="Permanent link">&para;</a></h3>
<p>Instruction tuning teaches the model to condition its output on an explicit instruction rather than implicit continuation.</p>
<p><strong>Training objective</strong></p>
<p>Standard cross-entropy loss with <strong>selective loss masking</strong>.</p>
<p>If:</p>
<ul>
<li><span class="arithmatex">\(x\)</span> = prompt tokens (system + user)</li>
<li><span class="arithmatex">\(y\)</span> = assistant response tokens</li>
</ul>
<p>Then SFT minimizes:</p>
<div class="arithmatex">\[
\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{|y|} \log P(y_t \mid x, y_{&lt;t})
\]</div>
<p>Tokens belonging to <span class="arithmatex">\(x\)</span> are excluded from the loss.</p>
<p><strong>Why masking matters</strong></p>
<ul>
<li>Prevents memorization of prompts</li>
<li>Ensures gradients only optimize response generation</li>
<li>Stabilizes alignment behavior</li>
</ul>
<hr />
<h3 id="task-and-prompt-formatting">Task and Prompt Formatting<a class="headerlink" href="#task-and-prompt-formatting" title="Permanent link">&para;</a></h3>
<p>Modern SFT relies heavily on <strong>structured role-based templates</strong>, such as ChatML or LLaMA-style formats.</p>
<p><strong>Example</strong>
<div class="highlight"><pre><span></span><code>&lt;|system|&gt; You are a helpful assistant. &lt;|end_of_text|&gt;
&lt;|user|&gt; Summarize this article. &lt;|end_of_text|&gt;
&lt;|assistant|&gt; The article discusses...
</code></pre></div></p>
<h2 id="why-formatting-matters">Why Formatting Matters<a class="headerlink" href="#why-formatting-matters" title="Permanent link">&para;</a></h2>
<ul>
<li>Separates intent, context, and response  </li>
<li>Enables multi-turn dialogue modeling  </li>
<li>Reduces ambiguity during inference  </li>
</ul>
<p>Formatting does more than separate roles.</p>
<ul>
<li>
<p><strong>Implicit policy learning</strong><br />
  Role tokens and system messages act as soft constraints that the model internalizes as behavioral priors.</p>
</li>
<li>
<p><strong>Gradient routing</strong><br />
  Loss masking combined with role tokens ensures gradients primarily shape response behavior rather than prompt reconstruction.</p>
</li>
<li>
<p><strong>Inference controllability</strong><br />
  Well-designed templates allow downstream systems to inject safety, tools, or routing instructions without retraining.</p>
</li>
<li>
<p><strong>Multi-turn state compression</strong><br />
  Structured formatting helps the model compress dialogue history into latent state representations instead of treating each turn independently.</p>
</li>
</ul>
<hr />
<h2 id="prompt-diversity-as-regularization">Prompt Diversity as Regularization<a class="headerlink" href="#prompt-diversity-as-regularization" title="Permanent link">&para;</a></h2>
<p>Prompt diversity is best understood as a <strong>regularization strategy</strong>, not just data augmentation.</p>
<h3 id="semantic-diversity">Semantic Diversity<a class="headerlink" href="#semantic-diversity" title="Permanent link">&para;</a></h3>
<p>Maintains coverage of distinct internal circuits:
- Symbolic reasoning and math
- Program synthesis and execution-style reasoning
- Creative and stylistic generation
- Factual recall under instruction pressure
- Conversational grounding</p>
<h3 id="structural-diversity">Structural Diversity<a class="headerlink" href="#structural-diversity" title="Permanent link">&para;</a></h3>
<p>Reduces shortcut learning:
- Paraphrased intents prevent lexical memorization
- Variable verbosity avoids length priors
- Explicit vs implicit constraints force instruction parsing</p>
<p><strong>Key insight</strong><br />
Insufficient structural diversity causes the model to learn <em>response templates</em> instead of <em>instruction semantics</em>.</p>
<hr />
<h2 id="513-human-vs-synthetic-supervision">5.1.3 Human vs Synthetic Supervision<a class="headerlink" href="#513-human-vs-synthetic-supervision" title="Permanent link">&para;</a></h2>
<h3 id="human-labeled-data">Human-Labeled Data<a class="headerlink" href="#human-labeled-data" title="Permanent link">&para;</a></h3>
<p><strong>Strengths</strong></p>
<ul>
<li>High factual accuracy  </li>
<li>Strong alignment with human preferences  </li>
<li>Better safety and tone control  </li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Expensive  </li>
<li>Slow to scale  </li>
<li>Limited coverage of edge cases  </li>
</ul>
<hr />
<h3 id="synthetic-supervision">Synthetic Supervision<a class="headerlink" href="#synthetic-supervision" title="Permanent link">&para;</a></h3>
<p>Modern post-training pipelines rely heavily on synthetic data generation.</p>
<h4 id="self-instruct">Self-Instruct<a class="headerlink" href="#self-instruct" title="Permanent link">&para;</a></h4>
<ul>
<li>Start with a small human-curated seed set  </li>
<li>Use a strong model to generate new instructions and responses  </li>
<li>Filter for quality  </li>
</ul>
<h4 id="evol-instruct-as-curriculum-learning">Evol-Instruct as Curriculum Learning<a class="headerlink" href="#evol-instruct-as-curriculum-learning" title="Permanent link">&para;</a></h4>
<p>Evol-Instruct implicitly creates a <strong>difficulty curriculum</strong>:
- Base instruction
- Added constraints
- Multi-hop or multi-objective reasoning
- Strict formatting or safety requirements</p>
<p>This improves:
- Instruction decomposition
- Constraint satisfaction
- Planning depth</p>
<hr />
<h3 id="rejection-sampling-best-of-n">Rejection Sampling (Best-of-N)<a class="headerlink" href="#rejection-sampling-best-of-n" title="Permanent link">&para;</a></h3>
<p>A widely used but often overlooked step in SFT.</p>
<p><strong>Process</strong></p>
<ol>
<li>Generate <span class="arithmatex">\(K\)</span> responses per prompt  </li>
<li>Score them using:  </li>
<li>A reward model, or  </li>
<li>A stronger reference model  </li>
<li>Select the best response  </li>
<li>Fine-tune on the selected outputs  </li>
</ol>
<p><strong>Key properties</strong></p>
<ul>
<li>Sharpens instruction adherence without policy gradients</li>
<li>Reduces variance compared to RLHF</li>
<li>Biases the model toward high-reward modes</li>
</ul>
<p><strong>Advanced risk</strong></p>
<ul>
<li>Over-optimization toward the reward model</li>
<li>Reduced output diversity</li>
<li>Reward hacking if the scorer is weak</li>
</ul>
<hr />
<h2 id="514-data-quality-over-quantity">5.1.4 Data Quality over Quantity<a class="headerlink" href="#514-data-quality-over-quantity" title="Permanent link">&para;</a></h2>
<h3 id="the-lima-hypothesis">The LIMA Hypothesis<a class="headerlink" href="#the-lima-hypothesis" title="Permanent link">&para;</a></h3>
<p><strong>“Less Is More for Alignment”</strong></p>
<p><strong>Key insight:</strong>  </p>
<p>~1,000 extremely high-quality examples can outperform tens of thousands of noisy ones  </p>
<p><strong>Implications:</strong>  </p>
<ul>
<li>Careful curation matters more than scale  </li>
<li>Labeler expertise is critical  </li>
<li>Reduces overfitting and style bias  </li>
</ul>
<hr />
<h3 id="typical-sft-data-mix">Typical SFT Data Mix<a class="headerlink" href="#typical-sft-data-mix" title="Permanent link">&para;</a></h3>
<p>A strong SFT dataset often includes:</p>
<ul>
<li>Reasoning and step-by-step explanations  </li>
<li>Creative and stylistic writing  </li>
<li>Coding and math problems  </li>
<li>Safety and refusal examples  </li>
<li>Multi-turn conversations  </li>
</ul>
<hr />
<h2 id="515-training-optimizations-and-stability">5.1.5 Training Optimizations and Stability<a class="headerlink" href="#515-training-optimizations-and-stability" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Purpose</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Packing</td>
<td>Throughput</td>
<td>Concatenates multiple short samples into a single context window to avoid padding waste</td>
</tr>
<tr>
<td>Loss Masking</td>
<td>Correct gradients</td>
<td>Computes loss only on assistant tokens</td>
</tr>
<tr>
<td>NEFTune</td>
<td>Generalization</td>
<td>Adds noise to embeddings during SFT to prevent token-level overfitting</td>
</tr>
<tr>
<td>Low learning rate</td>
<td>Stability</td>
<td>Typical values are <span class="arithmatex">\(1e^{-6}\)</span> to <span class="arithmatex">\(5e^{-6}\)</span></td>
</tr>
<tr>
<td>Dropout</td>
<td>Regularization</td>
<td>Reduces stylistic memorization</td>
</tr>
</tbody>
</table>
<h3 id="packing-vs-padding">Packing vs Padding<a class="headerlink" href="#packing-vs-padding" title="Permanent link">&para;</a></h3>
<p>In Supervised Fine-Tuning, training samples vary widely in length. How these samples are batched has a <strong>direct impact on compute efficiency, gradient quality, and training stability</strong>.</p>
<hr />
<h4 id="padding">Padding<a class="headerlink" href="#padding" title="Permanent link">&para;</a></h4>
<p><strong>What it is</strong><br />
All sequences in a batch are padded to the length of the longest sequence using <code>[PAD]</code> tokens.</p>
<p><strong>Example</strong></p>
<ul>
<li><code>Seq 1: [x x x x x x]</code></li>
<li><code>Seq 2: [x x x PAD PAD PAD]</code></li>
<li><code>Seq 3: [x x x x PAD PAD]</code></li>
</ul>
<p><strong>Why it is inefficient</strong>
- Attention, feedforward layers, and layer norms still execute on padded tokens
- Memory bandwidth and FLOPs are wasted on tokens that contribute no gradient
- Effective tokens per batch can drop sharply when length variance is high</p>
<p><strong>Impact at scale</strong>
- Lowers tokens processed per second
- Increases training cost
- Reduces gradient signal density</p>
<hr />
<h4 id="packing">Packing<a class="headerlink" href="#packing" title="Permanent link">&para;</a></h4>
<p><strong>What it is</strong><br />
Multiple short samples are concatenated into a single long sequence up to the model’s maximum context length. Each sample is separated by an EOS or special boundary token, and loss masking prevents cross-sample leakage.</p>
<p><strong>Example</strong></p>
<p><code>[Prompt₁ → Response₁ &lt;EOS&gt; Prompt₂ → Response₂ &lt;EOS&gt; Prompt₃ → Response₃]</code></p>
<p><strong>Why it is efficient</strong>
- Nearly every token contributes to loss
- Attention computation is fully utilized
- Higher effective batch token count without increasing memory</p>
<hr />
<h4 id="why-packing-improves-gpu-utilization">Why Packing Improves GPU Utilization<a class="headerlink" href="#why-packing-improves-gpu-utilization" title="Permanent link">&para;</a></h4>
<p>Packing increases:
- <strong>Arithmetic intensity</strong> by reducing idle FLOPs
- <strong>Token density per batch</strong>, improving gradient signal-to-noise ratio
- <strong>Throughput</strong>, often by 2x to 3x in instruction-tuning workloads where samples are short</p>
<p>This is especially impactful for:
- Instruction datasets with short prompts
- Chat-style SFT data
- Small to medium batch sizes constrained by memory</p>
<hr />
<h4 id="important-implementation-details">Important Implementation Details<a class="headerlink" href="#important-implementation-details" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Loss masking</strong> must reset at each sample boundary</li>
<li><strong>Attention masking</strong> must prevent tokens from attending across examples</li>
<li><strong>EOS handling</strong> is critical to avoid information leakage</li>
<li><strong>Position indices</strong> may need resetting depending on the architecture</li>
</ul>
<p>Incorrect packing can cause:
- Cross-example contamination
- Training instability
- Spurious memorization</p>
<hr />
<h4 id="padding-vs-packing-summary">Padding vs Packing Summary<a class="headerlink" href="#padding-vs-packing-summary" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Padding</th>
<th>Packing</th>
</tr>
</thead>
<tbody>
<tr>
<td>Compute efficiency</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Token utilization</td>
<td>Sparse</td>
<td>Dense</td>
</tr>
<tr>
<td>Training speed</td>
<td>Slow</td>
<td>Fast</td>
</tr>
<tr>
<td>Implementation complexity</td>
<td>Simple</td>
<td>Moderate</td>
</tr>
<tr>
<td>Risk of leakage</td>
<td>None</td>
<td>Requires care</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="interview-insight">Interview Insight<a class="headerlink" href="#interview-insight" title="Permanent link">&para;</a></h4>
<p>Packing is not just an optimization. It changes the <strong>effective learning dynamics</strong> by increasing gradient density and stabilizing updates, which is why it is now standard practice in large-scale SFT pipelines.</p>
<hr />
<h2 id="516-overfitting-and-catastrophic-forgetting">5.1.6 Overfitting and Catastrophic Forgetting<a class="headerlink" href="#516-overfitting-and-catastrophic-forgetting" title="Permanent link">&para;</a></h2>
<h3 id="catastrophic-forgetting">Catastrophic Forgetting<a class="headerlink" href="#catastrophic-forgetting" title="Permanent link">&para;</a></h3>
<p>The model loses general reasoning or factual knowledge learned during pre-training.</p>
<p><strong>Causes</strong></p>
<ul>
<li>Narrow SFT domain  </li>
<li>High learning rates  </li>
<li>Full fine-tuning on small datasets  </li>
</ul>
<p><strong>Mitigations</strong></p>
<ul>
<li>Mix 5–10% pre-training style data  </li>
<li>Use PEFT methods such as LoRA  </li>
<li>Lower learning rates  </li>
<li>Shorter training schedules  </li>
</ul>
<hr />
<h3 id="overfitting">Overfitting<a class="headerlink" href="#overfitting" title="Permanent link">&para;</a></h3>
<p>The model learns labeler-specific style rather than task intent.</p>
<p><strong>Symptoms</strong></p>
<ul>
<li>Over-politeness  </li>
<li>Repetitive phrasing  </li>
<li>Template-like answers  </li>
</ul>
<p><strong>Mitigations</strong></p>
<ul>
<li>Prompt diversity  </li>
<li>Early stopping  </li>
<li>Noise injection such as NEFTune  </li>
</ul>
<hr />
<h2 id="517-lora-vs-full-fine-tuning">5.1.7 LoRA vs Full Fine-Tuning<a class="headerlink" href="#517-lora-vs-full-fine-tuning" title="Permanent link">&para;</a></h2>
<h3 id="lora-and-peft">LoRA and PEFT<a class="headerlink" href="#lora-and-peft" title="Permanent link">&para;</a></h3>
<ul>
<li>Low compute cost  </li>
<li>Preserves base model knowledge  </li>
<li>Lower risk of catastrophic forgetting  </li>
<li>Preferred for alignment and style shifts  </li>
</ul>
<h3 id="full-fine-tuning">Full Fine-Tuning<a class="headerlink" href="#full-fine-tuning" title="Permanent link">&para;</a></h3>
<ul>
<li>Needed for large domain shifts  </li>
<li>Higher risk of forgetting  </li>
<li>Requires careful regularization  </li>
</ul>
<p><strong>Rule of thumb:</strong>  </p>
<ul>
<li>Behavior change → LoRA  </li>
<li>Knowledge change → Full fine-tuning  </li>
</ul>
<hr />
<h2 id="518-common-failure-modes-after-sft">5.1.8 Common Failure Modes After SFT<a class="headerlink" href="#518-common-failure-modes-after-sft" title="Permanent link">&para;</a></h2>
<h3 id="increased-hallucinations">Increased Hallucinations<a class="headerlink" href="#increased-hallucinations" title="Permanent link">&para;</a></h3>
<p>Often caused by <strong>knowledge contradiction</strong>.</p>
<p>If SFT data conflicts with pre-training facts, the model may prioritize format compliance over correctness.</p>
<p><strong>Mitigations</strong></p>
<ul>
<li>Fact-consistent SFT data  </li>
<li>Retrieval-augmented generation  </li>
<li>Post-SFT preference optimization  </li>
</ul>
<hr />
<h2 id="519-sft-vs-pre-training-summary">5.1.9 SFT vs Pre-training Summary<a class="headerlink" href="#519-sft-vs-pre-training-summary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Pre-training</th>
<th>Supervised Fine-Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Objective</td>
<td>World modeling</td>
<td>Behavior alignment</td>
</tr>
<tr>
<td>Data scale</td>
<td>Trillions of tokens</td>
<td>10k to 100k samples</td>
</tr>
<tr>
<td>Loss</td>
<td>Full sequence NTP</td>
<td>Masked response NTP</td>
</tr>
<tr>
<td>Compute</td>
<td>Massive</td>
<td>Moderate</td>
</tr>
<tr>
<td>Primary risk</td>
<td>Under-training</td>
<td>Overfitting and forgetting</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5110-interview-level-takeaways">5.1.10 Interview-Level Takeaways<a class="headerlink" href="#5110-interview-level-takeaways" title="Permanent link">&para;</a></h2>
<ul>
<li>SFT aligns behavior rather than knowledge  </li>
<li>Loss masking is essential  </li>
<li>Data quality dominates data scale  </li>
<li>Synthetic data is now standard  </li>
<li>Rejection sampling is widely used  </li>
<li>Forgetting is a first-order concern  </li>
<li>SFT sets the foundation for RLHF and preference optimization  </li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>