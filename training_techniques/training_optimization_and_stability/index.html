
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../pre_training/advanced_transformer_components_and_layers/">
      
      
        <link rel="next" href="../distributed_training_systems/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Training Optimization and Stability - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-optimizers-and-schedules" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training Optimization and Stability
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PEFT Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/prefix_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prefix Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/qlora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supporting Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supporting Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/4bit_normal_float/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4-bit NormalFloat (NF4)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/blockwise_kbit_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Block-wise k-bit Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/accelerate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerate Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/bp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BP16
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/decoding_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding & Medusa
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supporting_topics/kv_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Training Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../foundation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Foundation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Pre-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/pre_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/byte_pair_encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Byte Pair Encoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/mixture_of_experts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts (MoE)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/RoPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RoPe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/sentence_piece_unigram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sentence Piece Unigram
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_training/advanced_transformer_components_and_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Transformer Components & Layers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Training Optimization and Stability
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Training Optimization and Stability
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-optimizers-and-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      1. Optimizers and Schedules
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Optimizers and Schedules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-adam-vs-adamw" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Adam vs AdamW
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1 Adam vs AdamW">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adam-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Adam Optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      AdamW Optimizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-learning-rate-warmup-and-decay" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Learning Rate Warmup and Decay
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 Learning Rate Warmup and Decay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-rate-warmup" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Warmup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Decay
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-weight-decay-and-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 Weight Decay and Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.3 Weight Decay and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Weight Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition-summary" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-regularization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Other Regularization Techniques
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      2 Numerical Stability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Numerical Stability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-fp16-vs-bf16" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 FP16 vs BF16
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 FP16 vs BF16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fp16-half-precision" class="md-nav__link">
    <span class="md-ellipsis">
      FP16 (Half Precision)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bf16-brain-floating-point" class="md-nav__link">
    <span class="md-ellipsis">
      BF16 (Brain Floating Point)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Mixed Precision Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Mixed Precision Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Scaling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Gradient Clipping
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Gradient Clipping">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      What is Gradient Clipping?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why it matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-loss-spikes-and-divergence-diagnosis" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 Loss Spikes and Divergence Diagnosis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 Loss Spikes and Divergence Diagnosis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-causes-of-loss-spikes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Causes of Loss Spikes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diagnosis-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Diagnosis Checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-debugging-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Debugging Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distributed_training_systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Training Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Post-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../post_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning (SFT)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../system2.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reasoning & Test-Time Compute (System 2)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_6" >
        
          
          <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Mid-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_6">
            <span class="md-nav__icon md-icon"></span>
            Mid-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mid_training/mid_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mid Training Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-optimizers-and-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      1. Optimizers and Schedules
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Optimizers and Schedules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-adam-vs-adamw" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Adam vs AdamW
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1 Adam vs AdamW">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adam-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Adam Optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      AdamW Optimizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-learning-rate-warmup-and-decay" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Learning Rate Warmup and Decay
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 Learning Rate Warmup and Decay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-rate-warmup" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Warmup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Decay
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-weight-decay-and-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 Weight Decay and Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.3 Weight Decay and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Weight Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition-summary" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-regularization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Other Regularization Techniques
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      2 Numerical Stability
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Numerical Stability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-fp16-vs-bf16" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 FP16 vs BF16
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 FP16 vs BF16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fp16-half-precision" class="md-nav__link">
    <span class="md-ellipsis">
      FP16 (Half Precision)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bf16-brain-floating-point" class="md-nav__link">
    <span class="md-ellipsis">
      BF16 (Brain Floating Point)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Mixed Precision Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Mixed Precision Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Scaling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Gradient Clipping
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Gradient Clipping">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      What is Gradient Clipping?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why it matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-loss-spikes-and-divergence-diagnosis" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 Loss Spikes and Divergence Diagnosis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 Loss Spikes and Divergence Diagnosis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-causes-of-loss-spikes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Causes of Loss Spikes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diagnosis-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Diagnosis Checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-debugging-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Debugging Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Training Optimization and Stability</h1>

<p>Training large neural networks, especially Transformers, requires careful choices of optimizers, learning rate schedules, and numerical techniques to ensure fast convergence, stable training, and good generalization. This section covers the most important concepts commonly discussed in interviews and used in practice.</p>
<hr />
<h2 id="1-optimizers-and-schedules">1. Optimizers and Schedules<a class="headerlink" href="#1-optimizers-and-schedules" title="Permanent link">&para;</a></h2>
<h3 id="11-adam-vs-adamw">1.1 Adam vs AdamW<a class="headerlink" href="#11-adam-vs-adamw" title="Permanent link">&para;</a></h3>
<h4 id="adam-optimizer">Adam Optimizer<a class="headerlink" href="#adam-optimizer" title="Permanent link">&para;</a></h4>
<p>Adam (Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning.</p>
<p><strong>Key ideas:</strong></p>
<ul>
<li>Maintains an exponential moving average of gradients (first moment)</li>
<li>Maintains an exponential moving average of squared gradients (second moment)</li>
<li>Uses bias correction for both moments</li>
<li>Adapts the learning rate per parameter</li>
</ul>
<p><strong>Update rule (simplified):</strong></p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\]</div>
<p><strong>Where:</strong></p>
<ul>
<li><span class="arithmatex">\(\theta_t\)</span>: Model parameters at training step <span class="arithmatex">\(t\)</span>.</li>
<li><span class="arithmatex">\(\theta_{t+1}\)</span>: Updated model parameters after applying one optimization step.</li>
<li><span class="arithmatex">\(\eta\)</span> (Learning Rate): Global step size that controls how large the parameter update is.</li>
<li><span class="arithmatex">\(\hat{m}_t\)</span> (Bias Corrected First Moment): Exponentially decayed moving average of past gradients, corrected for initialization bias.<br />
  Represents the estimated mean of the gradients.
  $$
  \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
  $$</li>
<li><span class="arithmatex">\(\hat{v}_t\)</span> (Bias Corrected Second Moment): Exponentially decayed moving average of squared gradients, corrected for initialization bias.<br />
  Represents the estimated variance of the gradients.
  $$
  \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
  $$</li>
<li><span class="arithmatex">\(\beta_1\)</span>: Exponential decay rate for the first moment estimate.<br />
  Typical value: 0.9</li>
<li><span class="arithmatex">\(\beta_2\)</span>: Exponential decay rate for the second moment estimate.<br />
  Typical value: 0.999</li>
<li><span class="arithmatex">\(\epsilon\)</span>: Small constant added for numerical stability to prevent division by zero.<br />
  Typical value: <span class="arithmatex">\(10^{-8}\)</span></li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>Fast convergence</li>
<li>Works well with sparse gradients</li>
<li>Requires minimal tuning compared to SGD</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Implicitly couples L2 regularization with adaptive learning rates</li>
<li>Often leads to worse generalization compared to SGD or AdamW in large models</li>
</ul>
<hr />
<h4 id="adamw-optimizer">AdamW Optimizer<a class="headerlink" href="#adamw-optimizer" title="Permanent link">&para;</a></h4>
<p>AdamW decouples weight decay from the gradient-based update.</p>
<p><strong>Key difference from Adam:</strong></p>
<ul>
<li>Weight decay is applied directly to parameters, not via the gradient</li>
</ul>
<p><strong>Update rule (conceptually):</strong></p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \eta \lambda \theta_t
\]</div>
<p><strong>where</strong>:</p>
<ul>
<li><span class="arithmatex">\(\lambda\)</span> (Weight Decay Coefficient): Controls the strength of weight decay regularization.<br />
  Larger values enforce stronger penalization of large parameter magnitudes.</li>
<li><span class="arithmatex">\(- \eta \lambda \theta_t\)</span> (Decoupled Weight Decay Term): Applies weight decay directly to the parameters, independent of the gradient-based update.<br />
  This ensures consistent regularization regardless of adaptive learning rates.</li>
</ul>
<p><strong>Why AdamW matters:</strong></p>
<ul>
<li>Correctly implements weight decay as regularization</li>
<li>Prevents adaptive learning rates from weakening regularization</li>
<li>Standard optimizer for modern Transformers (BERT, GPT, ViT)</li>
</ul>
<blockquote>
<p>Note: AdamW is almost always preferred over Adam for large-scale Transformer training.</p>
</blockquote>
<hr />
<h3 id="12-learning-rate-warmup-and-decay">1.2 Learning Rate Warmup and Decay<a class="headerlink" href="#12-learning-rate-warmup-and-decay" title="Permanent link">&para;</a></h3>
<h4 id="learning-rate-warmup">Learning Rate Warmup<a class="headerlink" href="#learning-rate-warmup" title="Permanent link">&para;</a></h4>
<p><strong>What it is:</strong></p>
<ul>
<li>Gradually increases the learning rate from a small value to the target value over the first few training steps</li>
</ul>
<p><strong>Why it is needed:</strong></p>
<ul>
<li>Large models have unstable gradients at initialization</li>
<li>Prevents divergence caused by large updates early in training</li>
<li>Especially critical for Transformers and mixed precision training</li>
</ul>
<p><strong>Common strategies:</strong></p>
<ul>
<li>Linear warmup</li>
<li>Warmup for a fixed number of steps (e.g., 1k to 10k steps)</li>
</ul>
<hr />
<h4 id="learning-rate-decay">Learning Rate Decay<a class="headerlink" href="#learning-rate-decay" title="Permanent link">&para;</a></h4>
<p>After warmup, the learning rate is gradually reduced to improve convergence.</p>
<p><strong>Common decay schedules:</strong></p>
<ul>
<li>Linear decay</li>
<li>Cosine decay</li>
<li>Step decay</li>
<li>Polynomial decay</li>
</ul>
<p><strong>Cosine decay (widely used):</strong></p>
<ul>
<li>Smoothly reduces learning rate</li>
<li>Avoids sudden drops that can destabilize training</li>
</ul>
<blockquote>
<p>Insight: Warmup handles early instability, decay improves late-stage convergence and generalization.</p>
</blockquote>
<hr />
<h3 id="13-weight-decay-and-regularization">1.3 Weight Decay and Regularization<a class="headerlink" href="#13-weight-decay-and-regularization" title="Permanent link">&para;</a></h3>
<h4 id="weight-decay">Weight Decay<a class="headerlink" href="#weight-decay" title="Permanent link">&para;</a></h4>
<ul>
<li>Penalizes large weights to prevent overfitting</li>
</ul>
<p><strong>Important distinction:</strong></p>
<ul>
<li>L2 regularization modifies the loss</li>
<li>Weight decay directly modifies the parameter update</li>
</ul>
<p><strong>Why decoupling matters:</strong></p>
<ul>
<li>With adaptive optimizers, L2 regularization is not equivalent to weight decay</li>
<li>AdamW fixes this mismatch</li>
</ul>
<details>
<summary><strong>Explanation: L2 regularization is not equivalent to weight decay</strong></summary>

1. L2 Regularization: It adds a penalty term to the loss function:

$$
\mathcal{L}' = \mathcal{L} + \frac{\lambda}{2} \|\theta\|^2
$$

Taking the gradient:

$$
\nabla_\theta \mathcal{L}' = \nabla_\theta \mathcal{L} + \lambda \theta
$$

So the optimizer update becomes:

$$
\theta_{t+1} = \theta_t - \eta \left( \nabla_\theta \mathcal{L} + \lambda \theta_t \right)
$$

This means regularization is applied through the gradient. <br><br>

2. Why This Works for SGD <br><br>

For SGD, the update is:

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L} - \eta \lambda \theta_t
$$

Here, L2 regularization is mathematically equivalent to weight decay because: <br>
- All parameters use the same learning rate <br>
- No per parameter scaling is applied <br><br>

So both methods shrink weights uniformly. <br><br>

3. What Breaks with Adam <br><br>

Adam modifies the update using adaptive, per parameter learning rates:

$$
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

When L2 regularization is added, the penalty term $\lambda \theta_t$ is also scaled by the adaptive denominator:

$$
\theta_{t+1} =
\theta_t -
\eta \frac{\hat{m}_t + \lambda \theta_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

Consequence <br><br>

- Parameters with large gradient variance receive less regularization<br>
- Parameters with small gradient variance receive more regularization<br>
- Regularization strength becomes parameter dependent and inconsistent<br><br>

This is not true weight decay.<br><br>

4. What Weight Decay Actually Means<br><br>

True weight decay directly shrinks parameters independently of gradients:

$$
\theta_{t+1} = (1 - \eta \lambda)\theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

Key properties:<br>

- Uniform shrinkage across parameters<br>
- Independent of gradient statistics<br>
- Matches the intended regularization behavior<br><br>

5. How AdamW Fixes the Problem <br><br>

AdamW explicitly decouples weight decay from the gradient update:

$$
\theta_{t+1} =
\theta_t -
\eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} -
\eta \lambda \theta_t
$$
<br><br>

6. Why this works <br><br>

- Weight decay is applied directly to parameters<br>
- Adaptive learning rates affect only the gradient update<br>
- Regularization strength remains consistent<br><br>

</details>

<h4 id="intuition-summary">Intuition Summary<br><br><a class="headerlink" href="#intuition-summary" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>How Regularization Is Applied</th>
<th>Problem</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD + L2</td>
<td>Uniform parameter shrinkage</td>
<td>None</td>
</tr>
<tr>
<td>Adam + L2</td>
<td>Scaled by adaptive learning rates</td>
<td>Inconsistent regularization</td>
</tr>
<tr>
<td>AdamW</td>
<td>Direct parameter decay</td>
<td>Correct behavior</td>
</tr>
</tbody>
</table>
<h4 id="other-regularization-techniques">Other Regularization Techniques<a class="headerlink" href="#other-regularization-techniques" title="Permanent link">&para;</a></h4>
<ul>
<li>Dropout</li>
<li>Label smoothing</li>
<li>Data augmentation</li>
<li>Early stopping</li>
</ul>
<p><strong>Transformer-specific note:</strong></p>
<ul>
<li>Biases and LayerNorm parameters often exclude weight decay</li>
<li>This is a common best practice in large models</li>
</ul>
<hr />
<h2 id="2-numerical-stability">2 Numerical Stability<a class="headerlink" href="#2-numerical-stability" title="Permanent link">&para;</a></h2>
<h3 id="21-fp16-vs-bf16">2.1 FP16 vs BF16<a class="headerlink" href="#21-fp16-vs-bf16" title="Permanent link">&para;</a></h3>
<h4 id="fp16-half-precision">FP16 (Half Precision)<a class="headerlink" href="#fp16-half-precision" title="Permanent link">&para;</a></h4>
<p><strong>Characteristics:</strong></p>
<ul>
<li>16-bit floating point</li>
<li>Limited exponent range</li>
<li>Higher risk of overflow and underflow</li>
</ul>
<p><strong>Challenges:</strong></p>
<ul>
<li>Gradient underflow for small values</li>
<li>Requires loss scaling for stability</li>
</ul>
<hr />
<h4 id="bf16-brain-floating-point">BF16 (Brain Floating Point)<a class="headerlink" href="#bf16-brain-floating-point" title="Permanent link">&para;</a></h4>
<p><strong>Characteristics:</strong></p>
<ul>
<li>16-bit floating point with larger exponent range</li>
<li>Same exponent range as FP32</li>
<li>Lower mantissa precision than FP16</li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>Much more numerically stable than FP16</li>
<li>Usually does not require loss scaling</li>
<li>Widely supported on TPUs and newer GPUs</li>
</ul>
<blockquote>
<p>Insight: BF16 is preferred when hardware supports it due to better stability with minimal complexity.</p>
</blockquote>
<hr />
<h3 id="22-mixed-precision-training">2.2 Mixed Precision Training<a class="headerlink" href="#22-mixed-precision-training" title="Permanent link">&para;</a></h3>
<p>This technique uses both FP16/BF16 and FP32 to get the "best of both worlds": the speed of 16-bit and the accuracy of 32-bit.</p>
<ul>
<li><strong>Forward Pass:</strong> Done in FP16 for speed.</li>
<li><strong>Loss Scaling:</strong> Since FP16 has a narrow range, gradients can "underflow" (become zero). We multiply the loss by a large scale factor to push gradients into a representable range.</li>
<li><strong>Master Weights:</strong> A copy of the weights is kept in FP32. The gradients are converted to FP32 to update these master weights, ensuring precision isn't lost over time.</li>
</ul>
<h4 id="benefits">Benefits<a class="headerlink" href="#benefits" title="Permanent link">&para;</a></h4>
<ul>
<li>Faster training</li>
<li>Lower memory usage</li>
<li>Enables larger batch sizes and models</li>
</ul>
<h4 id="loss-scaling">Loss Scaling<a class="headerlink" href="#loss-scaling" title="Permanent link">&para;</a></h4>
<p><strong>Why it is needed (mainly for FP16):</strong></p>
<ul>
<li>Prevents gradients from underflowing to zero</li>
</ul>
<p><strong>How it works:</strong></p>
<ul>
<li>Multiply loss by a scale factor before backprop</li>
<li>Divide gradients by the same factor before optimizer step</li>
</ul>
<p><strong>Dynamic loss scaling:</strong></p>
<ul>
<li>Automatically adjusts scale based on overflow detection</li>
<li>Common in frameworks like PyTorch AMP</li>
</ul>
<hr />
<h3 id="23-gradient-clipping">2.3 Gradient Clipping<a class="headerlink" href="#23-gradient-clipping" title="Permanent link">&para;</a></h3>
<h4 id="what-is-gradient-clipping">What is Gradient Clipping?<a class="headerlink" href="#what-is-gradient-clipping" title="Permanent link">&para;</a></h4>
<p>To prevent "Exploding Gradients" (where a large update ruins the model weights), we cap the gradients.</p>
<ul>
<li><strong>Value Clipping:</strong> Caps each element of the gradient at a min/max.</li>
<li><strong>Norm Clipping:</strong> Scales the entire gradient vector so its <span class="arithmatex">\(L_2\)</span> norm does not exceed a threshold. This preserves the direction of the gradient while limiting the magnitude.</li>
</ul>
<p>Norm Clipping is more common.</p>
<div class="arithmatex">\[
g \leftarrow g \cdot \min\left(1, \frac{\tau}{\|g\|}\right)
\]</div>
<p>where <span class="arithmatex">\(\tau\)</span> is the clipping threshold.</p>
<hr />
<h4 id="why-it-matters">Why it matters<a class="headerlink" href="#why-it-matters" title="Permanent link">&para;</a></h4>
<ul>
<li>Prevents exploding gradients</li>
<li>Stabilizes training in deep or recurrent models</li>
<li>Especially important for large learning rates or noisy gradients</li>
</ul>
<p><strong>Typical values:</strong></p>
<ul>
<li>Global norm between 0.5 and 1.0 for Transformers</li>
</ul>
<hr />
<h3 id="24-loss-spikes-and-divergence-diagnosis">2.4 Loss Spikes and Divergence Diagnosis<a class="headerlink" href="#24-loss-spikes-and-divergence-diagnosis" title="Permanent link">&para;</a></h3>
<h4 id="common-causes-of-loss-spikes">Common Causes of Loss Spikes<a class="headerlink" href="#common-causes-of-loss-spikes" title="Permanent link">&para;</a></h4>
<ul>
<li>Learning rate too high</li>
<li>Insufficient warmup</li>
<li>Numerical overflow in FP16</li>
<li>Poor initialization</li>
<li>Data outliers or corrupted batches</li>
</ul>
<hr />
<h4 id="diagnosis-checklist">Diagnosis Checklist<a class="headerlink" href="#diagnosis-checklist" title="Permanent link">&para;</a></h4>
<ul>
<li>Check learning rate schedule and warmup length</li>
<li>Monitor gradient norms</li>
<li>Enable gradient clipping</li>
<li>Inspect loss scaling behavior</li>
<li>Compare FP16 vs BF16 runs</li>
<li>Verify data preprocessing and labels</li>
</ul>
<hr />
<h4 id="practical-debugging-tips">Practical Debugging Tips<a class="headerlink" href="#practical-debugging-tips" title="Permanent link">&para;</a></h4>
<ul>
<li>Reduce learning rate and re-run</li>
<li>Increase warmup steps</li>
<li>Switch from FP16 to BF16 if possible</li>
<li>Enable anomaly detection for NaNs and Infs</li>
<li>Log per-layer gradient norms</li>
</ul>
<hr />
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<ul>
<li>AdamW is the default optimizer for modern large models</li>
<li>Learning rate warmup is critical for early stability</li>
<li>Weight decay must be decoupled from adaptive updates</li>
<li>BF16 offers better numerical stability than FP16</li>
<li>Mixed precision improves efficiency but requires care</li>
<li>Gradient clipping and monitoring are essential debugging tools</li>
</ul>
<p>These concepts form the backbone of stable and efficient training for large-scale neural networks and are frequently tested in machine learning interviews.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>