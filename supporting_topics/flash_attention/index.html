
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../bp16/">
      
      
        <link rel="next" href="../decoding_strategies/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Flash Attention - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#flash-attention" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Flash Attention
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PEFT Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/prefix_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prefix Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../finetuning_techniques/qlora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supporting Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supporting Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4bit_normal_float/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4-bit NormalFloat (NF4)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blockwise_kbit_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Block-wise k-bit Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accelerate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerate Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BP16
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-standard-attention-and-its-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      3. Standard Attention and Its Limitations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Standard Attention and Its Limitations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-quadratic-memory-growth" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Quadratic Memory Growth
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-excessive-memory-traffic" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Excessive Memory Traffic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-inefficient-for-long-sequences-code-example" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Inefficient for Long Sequences (Code Example)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-numerical-issues-with-low-precision" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Numerical Issues with Low Precision
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-what-is-flashattention-and-how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      4. What Is FlashAttention and How It Works
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. What Is FlashAttention and How It Works">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-fused-computation" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Fused Computation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-single-pass-attention-and-online-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Single-Pass Attention and Online Softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-practical-impact" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Practical Impact
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-when-flashattention-helps-and-when-it-does-not" class="md-nav__link">
    <span class="md-ellipsis">
      5. When FlashAttention Helps (and When It Does Not)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-why-is-online-softmax-needed" class="md-nav__link">
    <span class="md-ellipsis">
      6. Why is online softmax needed?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Why is online softmax needed?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-numerical-stability-problem" class="md-nav__link">
    <span class="md-ellipsis">
      6.1. Numerical Stability Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-why-online-softmax-helps" class="md-nav__link">
    <span class="md-ellipsis">
      6.2. Why “Online” Softmax Helps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Key Benefits
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-end-to-end-flashattention-example" class="md-nav__link">
    <span class="md-ellipsis">
      7. End-to-End FlashAttention Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. End-to-End FlashAttention Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-prepare-q-k-v" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Prepare Q, K, V
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-split-into-tiles" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Split into Tiles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-process-tile-1" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Process Tile 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-process-tile-2-incrementally" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Process Tile 2 Incrementally
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-accumulate-output" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5: Accumulate Output
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decoding_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding & Medusa
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kv_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vllm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fsdp_deepspeed/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FSDP & Deep Speed
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../memory_considerations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Consideration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Training Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/foundation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Foundation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Pre-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/pre_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/byte_pair_encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Byte Pair Encoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/mixture_of_experts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts (MoE)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/RoPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RoPe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/sentence_piece_unigram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sentence Piece Unigram
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/pre_training/advanced_transformer_components_and_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Transformer Components & Layers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/training_optimization_and_stability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Optimization and Stability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/distributed_training_systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Training Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Post-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/post_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning (SFT)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/system2.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reasoning & Test-Time Compute (System 2)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_6" >
        
          
          <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Mid-Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_6">
            <span class="md-nav__icon md-icon"></span>
            Mid-Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/mid_training/mid_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mid Training Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/thinking_llms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Reasoning & Thinking Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-standard-attention-and-its-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      3. Standard Attention and Its Limitations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Standard Attention and Its Limitations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-quadratic-memory-growth" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Quadratic Memory Growth
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-excessive-memory-traffic" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Excessive Memory Traffic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-inefficient-for-long-sequences-code-example" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Inefficient for Long Sequences (Code Example)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-numerical-issues-with-low-precision" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Numerical Issues with Low Precision
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-what-is-flashattention-and-how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      4. What Is FlashAttention and How It Works
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. What Is FlashAttention and How It Works">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-fused-computation" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Fused Computation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-single-pass-attention-and-online-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Single-Pass Attention and Online Softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-practical-impact" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Practical Impact
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-when-flashattention-helps-and-when-it-does-not" class="md-nav__link">
    <span class="md-ellipsis">
      5. When FlashAttention Helps (and When It Does Not)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-why-is-online-softmax-needed" class="md-nav__link">
    <span class="md-ellipsis">
      6. Why is online softmax needed?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Why is online softmax needed?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-numerical-stability-problem" class="md-nav__link">
    <span class="md-ellipsis">
      6.1. Numerical Stability Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-why-online-softmax-helps" class="md-nav__link">
    <span class="md-ellipsis">
      6.2. Why “Online” Softmax Helps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    <span class="md-ellipsis">
      Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Key Benefits
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-end-to-end-flashattention-example" class="md-nav__link">
    <span class="md-ellipsis">
      7. End-to-End FlashAttention Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. End-to-End FlashAttention Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-prepare-q-k-v" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Prepare Q, K, V
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-split-into-tiles" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Split into Tiles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-process-tile-1" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Process Tile 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-process-tile-2-incrementally" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Process Tile 2 Incrementally
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-accumulate-output" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5: Accumulate Output
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="flash-attention">Flash Attention<a class="headerlink" href="#flash-attention" title="Permanent link">&para;</a></h1>
<h3 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h3>
<p>FlashAttention is a fast and memory-efficient implementation of the attention mechanism used in Transformer models. This repository explains what FlashAttention is, why it is faster than standard attention, and how it works under the hood, with a focus on interview preparation and practical understanding.</p>
<hr />
<h3 id="2-motivation">2. Motivation<a class="headerlink" href="#2-motivation" title="Permanent link">&para;</a></h3>
<p>Attention is the core operation behind Transformers, but standard attention becomes a major bottleneck for long sequences. The main problem is not only compute, but <strong>memory movement</strong>, which is often the true limiter on modern GPUs.</p>
<p>FlashAttention was introduced to:</p>
<ul>
<li>Reduce memory usage  </li>
<li>Minimize expensive GPU memory reads and writes  </li>
<li>Scale efficiently to long sequences  </li>
</ul>
<hr />
<h3 id="3-standard-attention-and-its-limitations">3. Standard Attention and Its Limitations<a class="headerlink" href="#3-standard-attention-and-its-limitations" title="Permanent link">&para;</a></h3>
<p>Given query, key, and value matrices:</p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]</div>
<p>While simple and elegant, this formulation has serious performance and memory issues for long sequences.</p>
<h4 id="31-quadratic-memory-growth">3.1 Quadratic Memory Growth<a class="headerlink" href="#31-quadratic-memory-growth" title="Permanent link">&para;</a></h4>
<p>Assume:</p>
<ul>
<li>Sequence length <span class="arithmatex">\(N = 16{,}384\)</span>  </li>
<li>FP16 precision (2 bytes per element)  </li>
</ul>
<p>The attention score matrix <span class="arithmatex">\(QK^T\)</span> has:</p>
<div class="arithmatex">\[
N^2 = 16{,}384^2 \approx 268 \text{ million elements}
\]</div>
<p>Memory required just for the attention matrix:</p>
<div class="arithmatex">\[
268\text{M} \times 2 \text{ bytes} \approx 512 \text{ MB}
\]</div>
<p>This does not include the softmax output, gradients during training, or activations from other layers, which can easily exceed GPU memory limits.</p>
<hr />
<h4 id="32-excessive-memory-traffic">3.2 Excessive Memory Traffic<a class="headerlink" href="#32-excessive-memory-traffic" title="Permanent link">&para;</a></h4>
<p>Standard attention performs multiple memory-heavy steps:</p>
<ol>
<li>Compute <span class="arithmatex">\(QK^T\)</span> and write to GPU global memory  </li>
<li>Read <span class="arithmatex">\(QK^T\)</span> back to apply softmax  </li>
<li>Write softmax output back to memory  </li>
<li>Read softmax output again to compute weighted sum with <span class="arithmatex">\(V\)</span>  </li>
</ol>
<p>Even with fast compute, repeated <strong>global memory reads and writes</strong> dominate runtime, making GPUs often memory-bound rather than compute-bound.</p>
<hr />
<h4 id="33-inefficient-for-long-sequences-code-example">3.3 Inefficient for Long Sequences (Code Example)<a class="headerlink" href="#33-inefficient-for-long-sequences-code-example" title="Permanent link">&para;</a></h4>
<p>A simplified PyTorch-style implementation:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># Q, K, V shape: (batch, seq_len, num_heads, head_dim)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div>
<p>What happens internally:</p>
<ul>
<li>scores materializes a full <span class="arithmatex">\(N×N\)</span> tensor</li>
<li>attn creates another <span class="arithmatex">\(N×N\)</span> tensor</li>
<li>Both tensors live in global memory</li>
</ul>
<p>As N grows, memory usage and latency grow quadratically.</p>
<h4 id="34-numerical-issues-with-low-precision">3.4 Numerical Issues with Low Precision<a class="headerlink" href="#34-numerical-issues-with-low-precision" title="Permanent link">&para;</a></h4>
<p>With FP16 or BF16:</p>
<ul>
<li>Large dot products in <span class="arithmatex">\(QK^T\)</span> can overflow</li>
<li>Small values can underflow to zero</li>
</ul>
<p>Standard attention often requires casting to FP32 for stability, which further increases memory usage and slows execution.</p>
<h3 id="4-what-is-flashattention-and-how-it-works">4. What Is FlashAttention and How It Works<a class="headerlink" href="#4-what-is-flashattention-and-how-it-works" title="Permanent link">&para;</a></h3>
<p>FlashAttention is an <strong>exact, memory-efficient attention algorithm</strong>. It computes the same result as standard attention but avoids materializing the full <span class="arithmatex">\(N \times N\)</span> attention matrix. This makes it much faster and reduces GPU memory usage, especially for long sequences.</p>
<p>Key advantages:</p>
<ul>
<li>Handles long sequences efficiently (e.g., 4k+ tokens)  </li>
<li>Works in FP16 and BF16 without numerical issues  </li>
<li>Reduces memory bandwidth usage with minimal extra compute  </li>
</ul>
<p>FlashAttention achieves this through three main ideas: <strong>tiling</strong>, <strong>fused computation</strong>, and <strong>single-pass attention with online softmax</strong>.</p>
<hr />
<h4 id="41-tiling">4.1 Tiling<a class="headerlink" href="#41-tiling" title="Permanent link">&para;</a></h4>
<p>Instead of computing attention for the full sequence at once, FlashAttention splits the query, key, and value matrices into <strong>small tiles</strong> that fit into GPU shared memory.</p>
<p><strong>Example:</strong></p>
<ul>
<li>Sequence length: <span class="arithmatex">\(N = 16{,}384\)</span></li>
<li>Tile size: <span class="arithmatex">\(B = 128\)</span>  </li>
</ul>
<p>Memory usage for a tile: <span class="arithmatex">\(128 \times 128 = 16{,}384\)</span> elements (much smaller than <span class="arithmatex">\((16{,}384)^2\)</span>)  </p>
<p><strong>Code-style intuition:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># pseudo-code for tiling</span>
<span class="k">for</span> <span class="n">q_tile</span> <span class="ow">in</span> <span class="n">Q_tiles</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">k_tile</span><span class="p">,</span> <span class="n">v_tile</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">K_tiles</span><span class="p">,</span> <span class="n">V_tiles</span><span class="p">):</span>
        <span class="n">partial_scores</span> <span class="o">=</span> <span class="n">q_tile</span> <span class="o">@</span> <span class="n">k_tile</span><span class="o">.</span><span class="n">T</span>
        <span class="c1"># accumulate results incrementally</span>
</code></pre></div>
<p>Benefit: Only a small block is in memory at a time, reducing GPU memory footprint dramatically.</p>
<h4 id="42-fused-computation">4.2 Fused Computation<a class="headerlink" href="#42-fused-computation" title="Permanent link">&para;</a></h4>
<p>FlashAttention fuses multiple steps into a single kernel:</p>
<ol>
<li>Matrix multiplication <span class="arithmatex">\((Q \cdot K^T)\)</span>  </li>
<li>Scaling by <span class="arithmatex">\((1/\sqrt{d})\)</span>  </li>
<li>Softmax computation  </li>
<li>Weighted sum with <span class="arithmatex">\((V)\)</span>  </li>
</ol>
<p><strong>Why this matters:</strong>  </p>
<ul>
<li>Standard attention performs each step separately, writing intermediate results to global memory.  </li>
<li>FlashAttention keeps all intermediate computations <strong>in shared memory</strong>, avoiding costly reads/writes.</li>
</ul>
<p><strong>Example intuition:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># pseudo-code for fused attention</span>
<span class="n">output_tile</span> <span class="o">=</span> <span class="n">flash_attention</span><span class="p">(</span><span class="n">q_tile</span><span class="p">,</span> <span class="n">k_tile</span><span class="p">,</span> <span class="n">v_tile</span><span class="p">)</span>
</code></pre></div>
<p>Here, flash_attention does all four steps at once, producing the final output for that tile.</p>
<h4 id="43-single-pass-attention-and-online-softmax">4.3 Single-Pass Attention and Online Softmax<a class="headerlink" href="#43-single-pass-attention-and-online-softmax" title="Permanent link">&para;</a></h4>
<p>FlashAttention computes attention in one streaming pass:</p>
<ul>
<li>Compute partial scores for each tile</li>
<li>Update running maximum and normalization term for softmax</li>
<li>Accumulate output incrementally</li>
</ul>
<p>This allows numerically stable softmax in FP16/BF16 without ever storing the full attention matrix.</p>
<p>Example numerical intuition:</p>
<ul>
<li>Tile 1 contributes scores [0.1, 0.5, 0.3]</li>
<li>Tile 2 contributes [0.2, 0.4, 0.1]</li>
<li>Running softmax computes the final normalized weights across tiles incrementally</li>
</ul>
<p>Benefit:</p>
<ul>
<li>Exact same result as full attention</li>
<li>Avoids overflow/underflow in low precision</li>
<li>Reduces memory reads/writes drastically</li>
</ul>
<h4 id="44-practical-impact">4.4 Practical Impact<a class="headerlink" href="#44-practical-impact" title="Permanent link">&para;</a></h4>
<ul>
<li>Memory complexity reduced from <span class="arithmatex">\(O(N^2) → O(N⋅B)\)</span> where <span class="arithmatex">\(B\)</span> is tile size</li>
<li>Enables training with longer sequences or larger batch sizes</li>
<li>Provides 2–4x speedups for long sequences on modern GPUs</li>
</ul>
<p>Code example using PyTorch API:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">flash_attn</span> <span class="kn">import</span> <span class="n">flash_attn_func</span>

<span class="c1"># q, k, v shape: (batch, seq_len, num_heads, head_dim)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></p>
<p>This produces exact attention results while being faster and more memory-efficient than standard attention.</p>
<h3 id="5-when-flashattention-helps-and-when-it-does-not">5. When FlashAttention Helps (and When It Does Not)<a class="headerlink" href="#5-when-flashattention-helps-and-when-it-does-not" title="Permanent link">&para;</a></h3>
<p>Works best when:</p>
<ul>
<li>Sequence length is large (typically 2k tokens or more)</li>
<li>Using FP16 or BF16</li>
<li>Running on modern NVIDIA GPUs with fast shared memory</li>
</ul>
<p>Less useful when:</p>
<ul>
<li>Sequence length is very short</li>
<li>CPU-based inference</li>
<li>Custom attention patterns not supported by FlashAttention kernels</li>
</ul>
<h3 id="6-why-is-online-softmax-needed">6. Why is online softmax needed?<a class="headerlink" href="#6-why-is-online-softmax-needed" title="Permanent link">&para;</a></h3>
<h4 id="61-numerical-stability-problem">6.1. Numerical Stability Problem<a class="headerlink" href="#61-numerical-stability-problem" title="Permanent link">&para;</a></h4>
<p>Standard softmax is computed as:</p>
<div class="arithmatex">\[
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]</div>
<p><strong>Issue in FP16/BF16:</strong></p>
<ul>
<li>FP16 has limited precision (~3–4 decimal digits) and a small exponent range.  </li>
<li>Large values of <span class="arithmatex">\((x_i)\)</span> (e.g., 50) cause <span class="arithmatex">\(e^{x_i}\)</span> to <strong>overflow</strong>.  </li>
<li>Very negative values of <span class="arithmatex">\((x_i)\)</span> (e.g., -50) cause <span class="arithmatex">\((e^{x_i})\)</span> to <strong>underflow</strong> to zero.  </li>
<li>Long sequences exacerbate the problem because summing hundreds or thousands of exponentials increases the risk of overflow/underflow.  </li>
</ul>
<p>Without precautions, computing softmax in FP16 can produce <strong>NaNs or zeros</strong>, breaking both training and inference.</p>
<h4 id="62-why-online-softmax-helps">6.2. Why “Online” Softmax Helps<a class="headerlink" href="#62-why-online-softmax-helps" title="Permanent link">&para;</a></h4>
<p>FlashAttention computes attention <strong>tile by tile</strong>, so it cannot store the full <span class="arithmatex">\(N \times N\)</span> attention matrix. To compute softmax correctly across the entire sequence in FP16/BF16, it uses <strong>online softmax</strong>.</p>
<h4 id="how-it-works">How It Works<a class="headerlink" href="#how-it-works" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p>Maintain a <strong>running maximum</strong> <span class="arithmatex">\(m\)</span> across tiles.</p>
<ul>
<li>Shift scores before exponentiating: <span class="arithmatex">\(e^{x_i - m}\)</span></li>
<li>Prevents overflow in exponential.</li>
</ul>
</li>
<li>
<p>Maintain a <strong>running sum</strong> of exponentials across tiles.</p>
<ul>
<li>Partial sums from each tile are combined incrementally.  </li>
<li>Ensures correct normalization for the softmax over the full sequence.</li>
</ul>
</li>
<li>
<p>Compute the weighted sum with <span class="arithmatex">\(V\)</span> <strong>incrementally</strong>.</p>
<ul>
<li>No full softmax matrix is stored in memory.  </li>
<li>Output is accumulated as each tile is processed.</li>
</ul>
</li>
</ol>
<hr />
<h4 id="example">Example<a class="headerlink" href="#example" title="Permanent link">&para;</a></h4>
<p>Suppose we have <strong>2 tiles</strong> with attention scores:</p>
<ul>
<li>Tile 1: <code>[0.1, 0.5, 0.3]</code>  </li>
<li>Tile 2: <code>[0.2, 0.4, 0.1]</code></li>
</ul>
<p><strong>Standard softmax</strong> (if we could store all scores):</p>
<div class="arithmatex">\[
\text{softmax}([0.1, 0.5, 0.3, 0.2, 0.4, 0.1])
\]</div>
<p><strong>Online softmax computation:</strong></p>
<ol>
<li>
<p><strong>Tile 1</strong>  </p>
<ul>
<li>Running max <span class="arithmatex">\(m = 0.5\)</span>  </li>
<li>Compute shifted exponentials: <code>[exp(0.1-0.5), exp(0.5-0.5), exp(0.3-0.5)] ≈ [0.67, 1.0, 0.82]</code>  </li>
<li>Running sum <span class="arithmatex">\(s = 0.67 + 1.0 + 0.82 = 2.49\)</span>  </li>
<li>Partial weighted sum with <span class="arithmatex">\(V\)</span> stored in output</li>
</ul>
</li>
<li>
<p><strong>Tile 2</strong>  </p>
<ul>
<li>New max <span class="arithmatex">\(m = \max(0.5, 0.4) = 0.5\)</span> (same in this case)  </li>
<li>Shifted exponentials: <code>[exp(0.2-0.5), exp(0.4-0.5), exp(0.1-0.5)] ≈ [0.74, 0.90, 0.61]</code>  </li>
<li>Update running sum: <span class="arithmatex">\(s = 2.49 + 0.74 + 0.90 + 0.61 = 4.74\)</span>  </li>
<li>Accumulate weighted sum with <span class="arithmatex">\(V\)</span></li>
</ul>
</li>
<li>
<p><strong>Normalization</strong>  </p>
<ul>
<li>Each accumulated output is divided by the final sum <span class="arithmatex">\(s = 4.74\)</span>  </li>
<li>Produces <strong>exact same softmax result</strong> as computing on the full sequence</li>
</ul>
</li>
</ol>
<hr />
<h4 id="key-benefits">Key Benefits<a class="headerlink" href="#key-benefits" title="Permanent link">&para;</a></h4>
<ul>
<li>Computes <strong>exact attention</strong> even in FP16/BF16  </li>
<li>Works efficiently with <strong>long sequences</strong> and <strong>large tiles</strong>  </li>
<li>Avoids storing huge intermediate matrices  </li>
<li>Reduces GPU memory usage and memory bandwidth overhead</li>
</ul>
<blockquote>
<p>In short: Online softmax allows FlashAttention to compute attention tile by tile while staying numerically stable and memory-efficient.</p>
</blockquote>
<h3 id="7-end-to-end-flashattention-example">7. End-to-End FlashAttention Example<a class="headerlink" href="#7-end-to-end-flashattention-example" title="Permanent link">&para;</a></h3>
<p>Suppose we have:</p>
<ul>
<li>Sequence length <span class="arithmatex">\(N = 8\)</span> (small for simplicity)  </li>
<li>Head dimension <span class="arithmatex">\(d = 2\)</span>  </li>
<li>Tile size <span class="arithmatex">\(B = 4\)</span>  </li>
</ul>
<p>We want to compute attention for a single head:</p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]</div>
<hr />
<h4 id="step-1-prepare-q-k-v">Step 1: Prepare Q, K, V<a class="headerlink" href="#step-1-prepare-q-k-v" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>  <span class="c1"># shape: (8, 2)</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># for simplicity</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># dummy value matrix</span>
</code></pre></div>
<h4 id="step-2-split-into-tiles">Step 2: Split into Tiles<a class="headerlink" href="#step-2-split-into-tiles" title="Permanent link">&para;</a></h4>
<p>To reduce memory usage, FlashAttention splits the sequence into smaller <strong>tiles</strong> that fit into GPU shared memory.</p>
<ul>
<li>Tile size <span class="arithmatex">\(B=4\)</span> → 2 tiles along the sequence  </li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Split Q, K, V into tiles</span>
<span class="n">Q_tiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">Q</span><span class="p">[</span><span class="mi">4</span><span class="p">:]]</span>  <span class="c1"># tile 1 and tile 2</span>
<span class="n">K_tiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">K</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">K</span><span class="p">[</span><span class="mi">4</span><span class="p">:]]</span>
<span class="n">V_tiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">V</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">V</span><span class="p">[</span><span class="mi">4</span><span class="p">:]]</span>
</code></pre></div>
<p>Benefit: Only a small portion of the sequence is in memory at a time, avoiding the need to materialize the full attention matrix.</p>
<h4 id="step-3-process-tile-1">Step 3: Process Tile 1<a class="headerlink" href="#step-3-process-tile-1" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p>Compute partial scores in shared memory:</p>
<div class="arithmatex">\[
\text{scores} = Q_\text{tile1} \cdot K_\text{tile1}^T / \sqrt{d}
\]</div>
<div class="highlight"><pre><span></span><code><span class="n">scores_tile1</span> <span class="o">=</span> <span class="n">Q_tiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">K_tiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p>Apply online softmax:</p>
<ul>
<li>Compute max of scores: m = scores_tile1.max(dim=1)</li>
<li>Shift and exponentiate: exp_scores = torch.exp(scores_tile1 - m)</li>
<li>Running sum: s = exp_scores.sum(dim=1)</li>
<li>Partial weighted sum with V: output_tile1 = (exp_scores @ V_tiles[0]) / s</li>
</ul>
</li>
</ol>
<p>Memory benefit: only a 4×4 matrix exists at a time.</p>
<h4 id="step-4-process-tile-2-incrementally">Step 4: Process Tile 2 Incrementally<a class="headerlink" href="#step-4-process-tile-2-incrementally" title="Permanent link">&para;</a></h4>
<ul>
<li>Compute partial scores of Q_tile1 × K_tile2^T</li>
<li>Update running max and running sum for online softmax</li>
<li>Accumulate weighted outputs with V_tile2</li>
<li>Repeat for Q_tile2 × K_tile1^T and Q_tile2 × K_tile2^T</li>
</ul>
<p>No full 8×8 attention matrix is ever materialized.</p>
<h4 id="step-5-accumulate-output">Step 5: Accumulate Output<a class="headerlink" href="#step-5-accumulate-output" title="Permanent link">&para;</a></h4>
<ul>
<li>Incrementally compute the weighted sum across all tiles</li>
<li>Resulting output shape (8, 2) matches standard attention</li>
<li>Softmax computed exactly using online normalization</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>