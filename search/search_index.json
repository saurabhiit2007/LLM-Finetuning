{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"lora/","title":"LoRA (Low-Rank Adaptation)","text":"<p>LoRA introduces trainable low-rank adapters into pretrained model layers.</p>"},{"location":"lora/#key-idea","title":"Key Idea","text":"<p>Instead of fine-tuning all weights  W , we express the update as:</p>  \\Delta W = A \\times B  <p>where rank(A, B) &lt;&lt; rank(W).</p>"},{"location":"lora/#advantages","title":"Advantages","text":"<ul> <li>Reduces trainable parameters</li> <li>Enables adapter fusion and reuse</li> <li>Works well with transformer architectures</li> </ul>"}]}