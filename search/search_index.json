{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"references/","title":"References","text":"<ul> <li>Hu, E. J., et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 </li> <li>Dettmers, T., et al. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 </li> <li>Hugging Face PEFT documentation: https://huggingface.co/docs/peft </li> <li>Community writeups and practical tips (e.g., blog posts and notebooks on LoRA/QLoRA setups).</li> <li>\"4-bit NormalFloat (NF4) Quantization\" \u2013 EmergentMind. Link </li> <li>Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs (2023)</li> <li>Tim Dettmers, bitsandbytes GitHub: https://github.com/TimDettmers/bitsandbytes</li> <li>Hugging Face PEFT Documentation: https://huggingface.co/docs/peft</li> <li>Manal Elaidouni, 4-Bit Quantization in QLoRA Explained</li> <li> <p>D. Prasad, QLoRA Explained \u2013 A Deep Dive into Parameter Efficient Fine-Tuning</p> </li> <li> <p>Paper: QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</p> </li> <li>Hugging Face PEFT: https://huggingface.co/docs/peft</li> <li>Bitsandbytes Library: https://github.com/TimDettmers/bitsandbytes</li> </ul>"},{"location":"distributed_training/distributed_training_systems/","title":"Distributed Training System","text":"<p>This page covers the system architecture, parallelism strategies, and engineering trade offs required to train large language models at scale.</p>"},{"location":"distributed_training/distributed_training_systems/#1-data-parallelism-dp","title":"1. Data Parallelism (DP)","text":"<p>The most widely used baseline parallelism strategy where the full model is replicated across \\(N\\) workers and each worker processes a different shard of the input batch.</p>"},{"location":"distributed_training/distributed_training_systems/#core-mechanism","title":"Core Mechanism","text":"<ul> <li>Each GPU performs forward and backward passes on its local mini batch.</li> <li>Gradients are synchronized across all workers using All Reduce.</li> <li>After synchronization, every replica applies the same optimizer update, keeping parameters identical.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#pytorch-distributed-data-parallel-ddp-details","title":"PyTorch Distributed Data Parallel (DDP) Details","text":""},{"location":"distributed_training/distributed_training_systems/#gradient-bucketing","title":"Gradient Bucketing","text":"<ul> <li>Model parameters are grouped into buckets based on size (The default bucket size is about 25 MB, configurable via bucket_cap_mb).</li> <li>Instead of waiting for all gradients to be computed, DDP starts communication as soon as a bucket is ready.</li> <li>This reduces idle time by avoiding one large synchronization at the end of backward pass.</li> </ul> <p>Why this matters: Smaller, incremental communication keeps GPUs busy and reduces the impact of network latency.</p>"},{"location":"distributed_training/distributed_training_systems/#asynchronous-gradient-reduction","title":"Asynchronous Gradient Reduction","text":"<ul> <li>When gradients for a bucket are computed, DDP launches an asynchronous All Reduce operation.</li> <li>While communication is happening, the backward pass continues computing gradients for later layers.</li> <li>This overlaps computation and communication.</li> </ul> <p>Key insight: Ideally, by the time backward computation finishes, most gradient communication is already done.</p>"},{"location":"distributed_training/distributed_training_systems/#synchronization-timing","title":"Synchronization Timing","text":"<ul> <li>Gradients are synchronized once per training iteration, not per layer.</li> <li>Each parameter\u2019s gradient is reduced exactly once after it is computed.</li> <li>The optimizer step is performed only after all gradient reductions complete.</li> </ul> <p>Common misconception: DDP does not pause after every layer to synchronize. Synchronization is event driven and overlaps with backpropagation.</p>"},{"location":"distributed_training/distributed_training_systems/#limitations","title":"Limitations","text":"<ul> <li>Model parameters, gradients, and optimizer states must fit on a single GPU.</li> <li>Scaling is limited by global batch size and optimizer stability.</li> <li>Communication cost grows linearly with the number of GPUs.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#2-tensor-parallelism-tp","title":"2. Tensor Parallelism (TP)","text":"<p>Tensor Parallelism splits individual layers across devices so that a single layer computation is distributed.</p>"},{"location":"distributed_training/distributed_training_systems/#21-core-mechanism","title":"2.1 Core Mechanism","text":"<p>Large weight matrices are partitioned across GPUs.</p> <p>Consider a linear layer:</p> <p>Y = XW</p> <p>where:</p> <ul> <li>X has shape <code>[batch, hidden_in]</code></li> <li>W has shape <code>[hidden_in, hidden_out]</code></li> <li>Y has shape <code>[batch, hidden_out]</code></li> <li>Column parallel splits <code>W</code> by output dimension.</li> <li>Row parallel splits <code>W</code> by input dimension.</li> <li>Each GPU computes a partial result which is combined using collective communication.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#column-parallelism-output-dimension-split","title":"Column Parallelism (Output Dimension Split)","text":"<p>In column parallelism, the weight matrix W is split by columns:</p> <p>W = [W\u2081 | W\u2082 | ... | W\u2096]</p> <p>Each GPU holds:</p> <ul> <li>W\u1d62 with shape <code>[hidden_in, hidden_out / k]</code></li> </ul> <p>Each GPU computes:</p> <ul> <li>Y\u1d62 = X \u00b7 W\u1d62</li> </ul> <p>At this point:</p> <ul> <li>Each Y\u1d62 is only a partial output.</li> <li>The full output Y is formed by concatenating all Y\u1d62 along the feature dimension.</li> </ul> <p>Communication Pattern</p> <ul> <li>An All Gather is used to assemble the full Y across GPUs.</li> <li>This happens during the forward pass.</li> <li>During backpropagation, gradients w.r.t. X require an All Reduce.</li> </ul> <p>Key Insight</p> <p>Column parallelism parallelizes the output features and is commonly used in feed forward layers.</p>"},{"location":"distributed_training/distributed_training_systems/#row-parallelism-input-dimension-split","title":"Row Parallelism (Input Dimension Split)","text":"<p>In row parallelism, the weight matrix W is split by rows:</p> <p>W = [ W\u2081       W\u2082       ...       W\u2096 ]</p> <p>Each GPU holds:</p> <ul> <li>W\u1d62 with shape <code>[hidden_in / k, hidden_out]</code></li> </ul> <p>The input X is also split:</p> <ul> <li>X = [X\u2081 | X\u2082 | ... | X\u2096]</li> </ul> <p>Each GPU computes:</p> <ul> <li>Y\u1d62 = X\u1d62 \u00b7 W\u1d62</li> </ul> <p>Now:</p> <ul> <li>Each Y\u1d62 is a partial sum of the final output.</li> </ul> <p>Communication Pattern</p> <ul> <li>An All Reduce is required to sum Y\u1d62 across GPUs.</li> <li>The final Y is identical on all GPUs after reduction.</li> <li>Backward pass mirrors this communication pattern.</li> </ul> <p>Key Insight</p> <p>Row parallelism parallelizes the input features and avoids an All Gather in the forward pass.</p>"},{"location":"distributed_training/distributed_training_systems/#why-two-parallelization-schemes-exist","title":"Why Two Parallelization Schemes Exist","text":"<p>Column and row parallelism are complementary: - Column parallelism produces partial outputs that must be gathered. - Row parallelism produces partial sums that must be reduced.</p> <p>Modern transformer implementations alternate between them: - Column parallel for linear projections. - Row parallel for output projections.</p> <p>This design minimizes total communication while keeping memory balanced.</p>"},{"location":"distributed_training/distributed_training_systems/#22-communication-characteristics","title":"2.2 Communication Characteristics","text":"<ul> <li>Requires All Reduce or All Gather inside the forward and backward pass.</li> <li>Communication is frequent and latency sensitive.</li> <li>Performance depends heavily on interconnect bandwidth.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#insights","title":"Insights","text":"<p>Strengths</p> <ul> <li>Enables training when individual layers do not fit on a single GPU.</li> <li>Reduces per GPU activation and parameter memory.</li> </ul> <p>Constraints</p> <ul> <li>Communication overhead is high.</li> <li>Usually restricted within a node due to bandwidth requirements.</li> <li>Sensitive to imbalance across tensor shards.</li> </ul> <p>Practical Usage</p> <ul> <li>Often combined with Data Parallelism.</li> <li>Popularized by Megatron LM style architectures.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#3-pipeline-parallelism-pp","title":"3. Pipeline Parallelism (PP)","text":"<p>Pipeline Parallelism splits the model by layer depth across devices.</p>"},{"location":"distributed_training/distributed_training_systems/#core-mechanism_1","title":"Core Mechanism","text":"<ul> <li>Each GPU owns a contiguous block of layers.</li> <li>Activations flow forward through the pipeline.</li> <li>Gradients flow backward in reverse order.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#the-pipeline-bubble","title":"The Pipeline Bubble","text":"<ul> <li>Without micro batching, only one stage is active at a time.</li> <li>GPUs sit idle waiting for inputs or gradients.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#micro-batching","title":"Micro Batching","text":"<ul> <li>The global batch is split into micro batches.</li> <li>Multiple micro batches are in flight simultaneously.</li> <li>Improves utilization at the cost of activation memory.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#insights_1","title":"Insights","text":"<p>Strengths</p> <ul> <li>Reduces per GPU parameter memory.</li> <li>Enables training extremely deep models.</li> </ul> <p>Trade-offs</p> <ul> <li>Increased activation memory footprint.</li> <li>Pipeline schedule complexity.</li> <li>Backward pass latency increases.</li> </ul> <p>Systems Insight</p> <p>Pipeline parallelism improves memory scaling but hurts latency sensitive workloads.</p>"},{"location":"distributed_training/distributed_training_systems/#4-zero-zero-redundancy-optimizer","title":"4. ZeRO (Zero Redundancy Optimizer)","text":"<p>ZeRO is designed to eliminate memory redundancy in Data Parallel training. In standard DP, every GPU stores:</p> <ol> <li>Model parameters</li> <li>Gradients</li> <li>Optimizer states (e.g., Adam\u2019s momentum and variance)</li> </ol> <p>This redundancy quickly becomes a bottleneck for very large models. ZeRO partitions these states across GPUs so that each GPU only stores a fraction of the total memory, enabling training of models that would otherwise not fit.</p>"},{"location":"distributed_training/distributed_training_systems/#zero-stages","title":"ZeRO Stages","text":"Stage Partitioned States Memory Savings Key Idea Stage 1 Optimizer states ~4\u00d7 Each GPU keeps a shard of the optimizer states instead of a full copy. Gradients and parameters are still replicated. Stage 2 Gradients ~8\u00d7 Gradients are also sharded. Each GPU contributes its shard to global All Reduce during backprop. Stage 3 Parameters N\u00d7 (number of GPUs) Even the model parameters are partitioned. Each GPU only holds a subset and fetches other shards when needed for computation. <p>Example: Suppose you have a 10B parameter model across 8 GPUs: - Stage 1: Each GPU holds 1/1 of parameters but 1/8 of optimizer states. - Stage 2: Each GPU holds 1/8 of gradients. - Stage 3: Each GPU holds 1/8 of parameters, gradients, and optimizer states.  </p>"},{"location":"distributed_training/distributed_training_systems/#key-insight","title":"Key Insight","text":"<ul> <li>Stage 1 and 2 mainly reduce memory replication.</li> <li>Stage 3 reduces the largest memory cost: the parameters themselves.</li> <li>At higher stages, computation must fetch shards of parameters or gradients from other GPUs on the fly.</li> <li>Communication becomes the primary bottleneck, requiring overlap with computation for efficiency.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#zero-offload","title":"ZeRO Offload","text":"<p>ZeRO can also offload states to CPU RAM or even NVMe storage to free GPU memory:</p> <ul> <li>Only the portion of optimizer states or parameters actively needed resides on the GPU.</li> <li>Trades GPU memory pressure for PCIe / NVMe bandwidth.</li> <li>Makes training extremely large models possible even with limited GPU memory (e.g., 10B+ parameters on 4\u00d7A100 40GB).</li> </ul> <p>Practical Note: Offloading is especially useful when the model is too big for Stage 3 alone or when using lower-end GPU clusters.</p>"},{"location":"distributed_training/distributed_training_systems/#communication-patterns","title":"Communication Patterns","text":"<ul> <li>Stage 1: Minimal communication; only optimizer state shards are reduced.</li> <li>Stage 2: Requires All Reduce for gradient shards.</li> <li>Stage 3: Each forward/backward pass may require fetching remote parameter shards.</li> <li>Communication-computation overlap is critical to avoid GPU idling.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#summary","title":"Summary","text":"<ul> <li>ZeRO scales memory linearly with the number of GPUs.</li> <li>Stage 3 maximizes memory saving but communication overhead increases.</li> <li>Poor network bandwidth or high latency can dominate runtime.</li> <li>Elastic / hybrid parallelism often combines ZeRO with DP, TP, or PP for large-scale training.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#5-communication-and-memory-trade-offs","title":"5. Communication and Memory Trade-offs","text":"<p>Distributed training is fundamentally constrained by a three way trade off between memory capacity, compute throughput, and communication bandwidth. Improving one dimension often degrades another, and real systems are designed to balance all three.</p>"},{"location":"distributed_training/distributed_training_systems/#51-the-memorycomputecommunication-triangle","title":"5.1 The Memory\u2013Compute\u2013Communication Triangle","text":"<ul> <li>Memory limits how large a model and batch can fit on a single GPU.</li> <li>Compute determines how fast forward and backward passes can be executed.</li> <li>Communication determines how quickly GPUs can synchronize parameters, gradients, or activations.</li> </ul> <p>For large models, memory is usually the first bottleneck. Techniques that reduce memory usage often increase either compute or communication cost.</p>"},{"location":"distributed_training/distributed_training_systems/#52-memory-accounting-with-adam-and-fp16","title":"5.2 Memory Accounting with Adam and FP16","text":"<p>Assume a model with <code>\u03a8</code> parameters trained using Adam and mixed precision.</p>"},{"location":"distributed_training/distributed_training_systems/#parameter-storage","title":"Parameter Storage","text":"<ul> <li>Model weights stored in FP16 or BF16: <code>2\u03a8</code> bytes</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#gradient-storage","title":"Gradient Storage","text":"<ul> <li>Gradients stored in FP16 or BF16 during backpropagation: <code>2\u03a8</code> bytes</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#optimizer-states","title":"Optimizer States","text":"<p>Adam maintains:</p> <ul> <li>FP32 master weights: <code>4\u03a8</code> bytes  </li> <li>First moment (momentum): <code>4\u03a8</code> bytes  </li> <li>Second moment (variance): <code>4\u03a8</code> bytes  </li> </ul> <p>Total optimizer memory: <code>12\u03a8</code> bytes</p>"},{"location":"distributed_training/distributed_training_systems/#total-training-memory","title":"Total Training Memory","text":"<p>Adding all components:</p> <ul> <li>Parameters: <code>2\u03a8</code></li> <li>Gradients: <code>2\u03a8</code></li> <li>Optimizer states: <code>12\u03a8</code></li> </ul> <p>Total: approximately <code>16\u03a8</code> bytes per parameter</p>"},{"location":"distributed_training/distributed_training_systems/#concrete-example","title":"Concrete Example","text":"<p>For a 7B parameter model:</p> <ul> <li>7B \u00d7 16 bytes \u2248 112 GB</li> </ul> <p>This number excludes:</p> <ul> <li>Activation memory</li> <li>Temporary buffers</li> <li>Communication workspaces</li> </ul> <p>This explains why even a single training replica of a 7B model cannot fit on an 80 GB GPU without memory optimization techniques.</p>"},{"location":"distributed_training/distributed_training_systems/#53-why-communication-becomes-the-bottleneck","title":"5.3 Why Communication Becomes the Bottleneck","text":"<p>As memory saving techniques reduce per GPU state:</p> <ul> <li>More parameter shards must be fetched remotely.</li> <li>Gradients must be synchronized more frequently.</li> <li>Communication moves from being infrequent and bulk to frequent and latency sensitive.</li> </ul> <p>High bandwidth interconnects and overlap with compute become critical.</p>"},{"location":"distributed_training/distributed_training_systems/#54-memory-reduction-techniques-and-their-trade-offs","title":"5.4 Memory Reduction Techniques and Their Trade-offs","text":""},{"location":"distributed_training/distributed_training_systems/#activation-checkpointing","title":"Activation Checkpointing","text":"<ul> <li>Saves memory by discarding activations during forward pass.</li> <li>Recomputes activations during backward pass.</li> <li>Trades memory for additional compute, typically 20 to 40 percent overhead.</li> </ul> <p>When to use: </p> <p>Memory constrained training where compute is not the bottleneck.</p>"},{"location":"distributed_training/distributed_training_systems/#mixed-precision-training","title":"Mixed Precision Training","text":"<ul> <li>Uses FP16 or BF16 for forward and backward computation.</li> <li>Keeps optimizer states in FP32 for numerical stability.</li> <li>Reduces memory usage and communication bandwidth.</li> </ul> <p>Key benefit: </p> <p>Improves both memory efficiency and throughput with minimal accuracy loss.</p>"},{"location":"distributed_training/distributed_training_systems/#parameter-sharding","title":"Parameter Sharding","text":"<ul> <li>Splits parameters, gradients, or optimizer states across GPUs.</li> <li>Removes redundancy present in Data Parallelism.</li> <li>Increases communication during forward and backward passes.</li> </ul> <p>Typical examples: </p> <p>ZeRO Stage 2 and Stage 3.</p> <p>Takeaway: Large scale training is constrained by a memory\u2013compute\u2013communication trade off. Techniques like mixed precision, activation checkpointing, and parameter sharding reduce memory pressure but introduce additional compute or communication costs. Efficient systems overlap communication with computation to avoid performance collapse.</p>"},{"location":"distributed_training/distributed_training_systems/#6-checkpointing-and-fault-tolerance","title":"6. Checkpointing and Fault Tolerance","text":"<p>At large cluster scale, hardware and network failures are expected. Training systems must be designed to recover quickly with minimal loss of progress.</p>"},{"location":"distributed_training/distributed_training_systems/#61-checkpointing-strategies","title":"6.1 Checkpointing Strategies","text":"<p>Full Checkpoints</p> <ul> <li>Store model parameters, optimizer states, and RNG state.</li> <li>Enable exact training resumption.</li> <li>Expensive in terms of storage size and write time.</li> </ul>  RNG State (Random Number Generator State)   1. Why RNG State Matters  Randomness is used in multiple parts of training:  - Weight initialization - Dropout masks - Data shuffling - Data augmentation - Stochastic layers or kernels  If training is resumed **without restoring RNG state**:  - Dropout patterns change - Data order may differ - Gradient noise changes  As a result, training can diverge from the original run, making debugging and reproducibility difficult.  2. What Is Typically Included  A full checkpoint usually stores RNG states for:  - Python `random` - NumPy RNG - PyTorch CPU RNG - PyTorch CUDA RNG (per GPU)  In distributed training, each worker maintains its own RNG state, which must be saved and restored independently.   <p>Sharded Checkpoints</p> <ul> <li>Each worker writes only its shard of parameters or optimizer state.</li> <li>Reduces checkpoint time and I/O contention.</li> <li>Requires coordinated restore logic.</li> </ul> <p>Asynchronous Checkpointing</p> <ul> <li>Checkpointing runs in the background while training continues.</li> <li>Avoids blocking GPUs on slow storage.</li> <li>Slightly increases complexity and memory usage.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#62-fault-tolerance","title":"6.2 Fault Tolerance","text":"<p>Elastic Training</p> <ul> <li>Allows workers to join or leave during training.</li> <li>Automatically rebalances data and workloads.</li> <li>Commonly implemented using PyTorch Elastic or TorchRun.</li> </ul> <p>Health Checks</p> <ul> <li>Detect hung, slow, or non communicating workers.</li> <li>Prevents a single faulty GPU from stalling the entire job.</li> </ul> <p>Straggler Detection</p> <ul> <li>Identifies workers that are consistently slower.</li> <li>Helps avoid synchronization delays in collective operations.</li> </ul> <p>Takeaway: Checkpointing is fundamentally an I/O and systems problem. Efficient training requires minimizing checkpoint overhead while ensuring fast and correct recovery from failures.</p>"},{"location":"distributed_training/distributed_training_systems/#7-advanced-parallelism-and-optimization-topics","title":"7. Advanced Parallelism and Optimization Topics","text":"<p>These topics often differentiate strong systems candidates.</p>"},{"location":"distributed_training/distributed_training_systems/#71-flashattention","title":"7.1 FlashAttention","text":"<ul> <li>Computes attention in tiles to avoid materializing full attention matrices.</li> <li>Reduces memory from quadratic to linear in sequence length.</li> <li>Improves both speed and memory usage.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#72-mixture-of-experts-moe","title":"7.2 Mixture of Experts (MoE)","text":"<ul> <li>Sparse activation where only a subset of parameters are used per token.</li> <li>Requires expert parallelism and routing strategies.</li> <li>Trades compute efficiency for model capacity.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#73-sequence-parallelism","title":"7.3 Sequence Parallelism","text":"<p>Sequence parallelism splits the sequence length dimension of the input across multiple devices, rather than splitting model parameters or batch elements.</p>"},{"location":"distributed_training/distributed_training_systems/#core-idea","title":"Core Idea","text":"<p>For an input tensor with shape:</p> <p><code>[batch, sequence_length, hidden_dim]</code></p> <ul> <li>The sequence length dimension is partitioned across GPUs.</li> <li>Each GPU processes a contiguous chunk of tokens from the sequence.</li> <li>This reduces per GPU activation memory, which scales linearly with sequence length.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#why-it-is-useful","title":"Why It Is Useful","text":"<ul> <li>Attention and activation memory grow with sequence length.</li> <li>Very long context models (e.g., 32k, 64k, or 128k tokens) quickly exceed GPU memory limits.</li> <li>Sequence parallelism allows long sequences to fit by distributing token level computation.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#communication-pattern","title":"Communication Pattern","text":"<ul> <li>Certain operations, such as attention and layer normalization, require communication across sequence shards.</li> <li>Communication typically uses All Gather or All Reduce.</li> <li>Efficient overlap with computation is necessary to avoid performance loss.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#relationship-with-tensor-parallelism","title":"Relationship with Tensor Parallelism","text":"<ul> <li>Sequence parallelism is often combined with tensor parallelism.</li> <li>Tensor parallelism splits hidden dimensions, while sequence parallelism splits tokens.</li> <li>This combination balances memory usage and communication overhead.</li> </ul> <p>Takeaway: Sequence parallelism distributes tokens across GPUs to reduce activation memory for long context models, trading additional communication for the ability to train with very large sequence lengths.</p>"},{"location":"distributed_training/distributed_training_systems/#74-low-precision-training","title":"7.4 Low Precision Training","text":"<ul> <li>FP8 reduces bandwidth and memory.</li> <li>Requires careful scaling and error management.</li> <li>Hardware dependent and increasingly common on newer accelerators.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#8-additional-common-interview-topics","title":"8. Additional Common Interview Topics","text":"<p>These are frequently asked in senior level ML systems interviews.</p>"},{"location":"distributed_training/distributed_training_systems/#81-hybrid-parallelism","title":"8.1 Hybrid Parallelism","text":"<ul> <li>Real systems combine DP, TP, PP, and ZeRO.</li> <li>Interviewers often ask how to scale from 1 GPU to hundreds or thousands.</li> <li>A strong answer mentions hierarchical parallelism.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#82-throughput-vs-latency","title":"8.2 Throughput vs Latency","text":"<p>Throughput and latency represent two different optimization goals, and distributed systems make very different design choices depending on which one is prioritized.</p>"},{"location":"distributed_training/distributed_training_systems/#training-throughput-oriented","title":"Training: Throughput Oriented","text":"<ul> <li>The primary goal in training is maximum tokens processed per second.</li> <li>Large batch sizes are used to fully utilize GPUs.</li> <li>Parallelism strategies favor efficiency even if individual requests are slow.</li> </ul> <p>Common choices in training</p> <ul> <li>Data parallelism with large global batches.</li> <li>Pipeline parallelism with micro batching.</li> <li>Aggressive overlap of communication and computation.</li> <li>Higher tolerance for end to end latency per batch.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#inference-latency-oriented","title":"Inference: Latency Oriented","text":"<ul> <li>The primary goal in inference is fast response time per request.</li> <li>Batch sizes are often small or dynamic.</li> <li>Minimizing synchronization and communication is critical.</li> </ul> <p>Common choices in inference</p> <ul> <li>Replication rather than sharding for small models.</li> <li>Limited or no pipeline parallelism due to bubble overhead.</li> <li>Kernel fusion and caching over global synchronization.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#why-parallelism-strategies-differ","title":"Why Parallelism Strategies Differ","text":"<ul> <li>Training can amortize communication over large batches.</li> <li>Inference cannot hide communication latency as easily.</li> <li>Techniques like tensor or pipeline parallelism may improve throughput but often increase per request latency.</li> </ul> <p>Takeaway: Training systems optimize for throughput using large batches and heavy parallelism, while inference systems optimize for low latency by minimizing synchronization and communication, leading to fundamentally different parallelism strategies.</p>"},{"location":"distributed_training/distributed_training_systems/#83-gradient-accumulation","title":"8.3 Gradient Accumulation","text":"<p>Gradient accumulation is a technique used to simulate a larger batch size by accumulating gradients over multiple forward and backward passes before performing an optimizer update.</p>"},{"location":"distributed_training/distributed_training_systems/#core-idea_1","title":"Core Idea","text":"<ul> <li>Instead of updating model parameters after every mini batch, gradients are accumulated over <code>K</code> steps.</li> <li>The optimizer step is executed only once after all <code>K</code> gradients are accumulated.</li> <li>This creates an effective batch size of <code>K \u00d7 mini_batch_size</code>.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#why-it-saves-memory","title":"Why It Saves Memory","text":"<ul> <li>Each step processes a small mini batch that fits in GPU memory.</li> <li>Activations are freed after each backward pass.</li> <li>Only gradients are accumulated, avoiding the need to store a large batch at once.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#communication-benefits-in-distributed-training","title":"Communication Benefits in Distributed Training","text":"<ul> <li>In Data Parallel training, gradient synchronization normally happens every step.</li> <li>With gradient accumulation, synchronization happens only once every <code>K</code> steps.</li> <li>This reduces communication frequency and improves scalability.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#common-confusion-with-data-parallelism","title":"Common Confusion with Data Parallelism","text":"<ul> <li>Data parallelism distributes different data samples across GPUs.</li> <li>Gradient accumulation repeats multiple steps on the same GPU before synchronization.</li> <li>The two techniques are complementary and often used together.</li> </ul>"},{"location":"distributed_training/distributed_training_systems/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Learning rate schedules often need adjustment for large effective batch sizes.</li> <li>Loss values are usually scaled to avoid gradient magnitude changes.</li> <li>Very large accumulation steps can slow convergence.</li> </ul> <p>Takeaway: Gradient accumulation increases effective batch size by accumulating gradients across multiple steps, reducing memory usage and communication frequency, and is often combined with data parallelism to scale training efficiently.</p>"},{"location":"distributed_training/fsdp_deepspeed/","title":"FSDP & DeepSpeed","text":""},{"location":"distributed_training/fsdp_deepspeed/#1-executive-summary","title":"1. Executive Summary","text":"<p>In the era of Large Language Models (LLMs), the primary bottleneck is fitting billions of parameters into GPU VRAM. Both DeepSpeed and FSDP (Fully Sharded Data Parallel) address this by moving from standard Data Parallelism (where every GPU has a full copy) to Sharded Data Parallelism.</p> <ul> <li>DeepSpeed: A Microsoft library implementing the ZeRO (Zero Redundancy Optimizer) algorithm. It is highly flexible and specialized for extreme scale and offloading.</li> <li>FSDP: PyTorch's native implementation of sharding. It is optimized for the PyTorch ecosystem, offering better integration with features like <code>torch.compile</code>.</li> </ul>"},{"location":"distributed_training/fsdp_deepspeed/#2-core-concept-the-zero-algorithm","title":"2. Core Concept: The ZeRO Algorithm","text":"<p>To understand both frameworks, we must understand the three stages of memory consumption (Model States):</p> <ol> <li>Optimizer States (e.g., Adam momentum/variance)</li> <li>Gradients</li> <li>Parameters (Weights)</li> </ol>"},{"location":"distributed_training/fsdp_deepspeed/#zero-stages-breakdown","title":"ZeRO Stages Breakdown","text":"Stage Description Memory Impact Stage 1 Shards only Optimizer States. Reduces memory by ~4x. Stage 2 Shards Optimizer States + Gradients. Reduces memory by ~8x. Stage 3 Shards Optimizer + Gradients + Parameters. Memory scales linearly with the number of GPUs. <p>ZeRO-3 and FSDP both aim to minimize peak memory by materializing parameters only when needed.</p>"},{"location":"distributed_training/fsdp_deepspeed/#3-deepspeed-specifics","title":"3. DeepSpeed Specifics","text":"<p>DeepSpeed is the \"power user\" choice for massive models.</p>"},{"location":"distributed_training/fsdp_deepspeed/#key-features","title":"Key Features","text":"<ul> <li> <p>ZeRO-Offload: Moves optimizer states and gradients to CPU RAM. This allows training a 13B model on a single consumer GPU.</p> </li> <li> <p>ZeRO-Infinity: Extends offloading to NVMe SSDs, theoretically supporting trillion-parameter models.</p> </li> <li> <p>Communication Overlap: Highly optimized C++ kernels that overlap data transfer with computation to hide latency.</p> </li> </ul>"},{"location":"distributed_training/fsdp_deepspeed/#optimizer-and-precision-support","title":"Optimizer and Precision Support","text":"<ul> <li>FP32 master weights with FP16 or BF16 compute.</li> <li>Native support for 8-bit optimizers.</li> <li>Aggressive memory savings at the cost of increased system complexity.</li> </ul>"},{"location":"distributed_training/fsdp_deepspeed/#4-pytorch-fsdp-specifics","title":"4. PyTorch FSDP Specifics","text":"<p>FSDP is the native, more \"modern\" approach for PyTorch users.</p>"},{"location":"distributed_training/fsdp_deepspeed/#key-features_1","title":"Key Features","text":"<ul> <li> <p>Hybrid Sharding (HSDP): Shards parameters within a node (fast NVLink) but replicates them across nodes. This is crucial for avoiding network bottlenecks in large clusters.</p> </li> <li> <p>FSDP2 (Recent): Built on DTensors. Unlike the original FSDP, it doesn't \"flatten\" parameters into a 1D buffer. This makes it compatible with torch.compile for faster execution.</p> </li> <li> <p>Transformer Wrapping: Allows \"wrapping\" specific layers so they are sharded/unsharded independently, keeping the peak memory low.</p> </li> </ul>"},{"location":"distributed_training/fsdp_deepspeed/#5-comparative-analysis","title":"5. Comparative Analysis","text":"Feature DeepSpeed FSDP Ecosystem Third-party (Microsoft) Native (PyTorch) Ease of Use JSON configuration files Pythonic API / Wrappers Offloading Advanced (CPU/NVMe) Basic (CPU only) Throughput Best for 100B+ models Best for &lt;20B models &amp; <code>torch.compile</code>"},{"location":"distributed_training/fsdp_deepspeed/#6-system-design-scenarios","title":"6. System Design Scenarios","text":""},{"location":"distributed_training/fsdp_deepspeed/#scenario-a-the-memory-calculation","title":"Scenario A: The Memory Calculation","text":"<p>Q: \"How much memory do we need to train a 7B parameter model in FP16?\"</p> <p>A: * Parameters: 7B * 2 bytes = 14 GB</p> <ul> <li>Gradients: 7B * 2 bytes = 14 GB</li> <li>Optimizer (Adam): 7B * 12 bytes = 84 GB</li> <li>Total Model State: ~112 GB. </li> <li>Conclusion: This won't fit on an 80GB A100. Mention that ZeRO-2/FSDP is required to shard the 84GB of optimizer states across multiple GPUs.</li> </ul>"},{"location":"distributed_training/fsdp_deepspeed/#scenario-b-debugging-oom-out-of-memory","title":"Scenario B: Debugging OOM (Out of Memory)","text":"<p>Q: \"You are using FSDP but still hitting OOM. What do you check?\"</p> <p>A:</p> <ol> <li>Activation Memory: Sharding only handles model states. Large batch sizes or long sequences create massive activations. Use Activation Checkpointing.</li> <li>Wrapping Policy: If the model isn't wrapped in sub-units, FSDP treats the whole model as one shard, effectively un-sharding everything at once.</li> <li>Fragmentation: Check if the PyTorch memory allocator is fragmented (use <code>torch.cuda.memory_summary()</code>).</li> </ol>"},{"location":"distributed_training/fsdp_deepspeed/#7-recent-trends-2025-2026","title":"7. Recent Trends (2025-2026)","text":"<ul> <li>Communication Compression: New methods to compress the gradients being moved during the Reduce-Scatter phase.</li> <li>Unified Strategies: Tools like Hugging Face Accelerate now allow switching between DeepSpeed and FSDP with a single flag, making the choice more about performance tuning than code rewriting.</li> </ul>"},{"location":"frameworks/accelerate/","title":"\u26a1 Accelerate: Efficient Training for Large Language Models","text":""},{"location":"frameworks/accelerate/#1-overview","title":"1. Overview","text":"<p>Accelerate is a lightweight framework by Hugging Face that simplifies distributed and mixed-precision training for large models, including LLMs. It abstracts device placement, process coordination, and backend integration so developers can scale from single GPU to multi-node setups with minimal code changes.</p> <p>Accelerate works as an orchestration layer on top of PyTorch DDP, FSDP, DeepSpeed ZeRO, and TPU/XLA, without introducing new training algorithms.</p>"},{"location":"frameworks/accelerate/#key-features","title":"Key Features","text":"<ul> <li>Multi-GPU, multi-node, and TPU training with minimal code changes</li> <li>Mixed precision support (FP16, BF16)</li> <li>Gradient accumulation</li> <li>Integration with FSDP and DeepSpeed ZeRO for memory efficiency</li> <li>Distributed-safe checkpointing and logging</li> </ul>"},{"location":"frameworks/accelerate/#2-problem-statement","title":"2. Problem Statement","text":"<p>Training large transformer models introduces key challenges:</p> <ol> <li>Memory limits - Models often exceed single-GPU memory.</li> <li>Distributed complexity - Manual DDP setup is error-prone.</li> <li>Scaling - Efficient multi-GPU or multi-node scaling is non-trivial.</li> <li>Numerical stability - Mixed-precision training requires careful handling.</li> </ol> <p>Accelerate addresses these by providing a unified, backend-agnostic interface for distributed training.</p>"},{"location":"frameworks/accelerate/#3-core-components","title":"3. Core Components","text":""},{"location":"frameworks/accelerate/#31-accelerator","title":"\ud83e\udde9 3.1. <code>Accelerator</code>","text":"<p>The central abstraction that manages:</p> <ul> <li>Device placement</li> <li>Distributed backend setup</li> <li>Mixed precision</li> <li>Gradient accumulation</li> <li>Process coordination for logging and checkpointing</li> </ul> <p>Initialization: <pre><code>from accelerate import Accelerator\naccelerator = Accelerator()\n\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n</code></pre></p> <p>What Accelerate does automatically:</p> <ul> <li>Moves models and data to the correct device </li> <li>Wraps models with DDP, FSDP, or DeepSpeed </li> <li>Handles mixed-precision context and gradient scaling</li> </ul>"},{"location":"frameworks/accelerate/#32-device-management","title":"\u2699\ufe0f 3.2. Device Management","text":"<p>Accelerate auto-detects available hardware and exposes a unified device handle.</p> <ul> <li>Supports CPU, CUDA GPUs, and TPUs</li> <li>Avoids manual .cuda() or rank-specific logic</li> </ul> <pre><code>inputs = inputs.to(accelerator.device)\n</code></pre> <p>Benefit: Prevents CUDA placement errors and maximizes hardware utilization.</p>"},{"location":"frameworks/accelerate/#33-distributed-data-parallelism-ddp","title":"\ud83d\udd01 3.3. Distributed Data Parallelism (DDP)","text":"<p>Each device holds a replica of the model and processes a shard of data.</p>"},{"location":"frameworks/accelerate/#workflow","title":"\u2699\ufe0f Workflow","text":"<ol> <li>Each GPU computes gradients on its local data shard.  </li> <li>Gradients are averaged across all GPUs.  </li> <li>Parameter updates are synchronized globally.</li> </ol>"},{"location":"frameworks/accelerate/#mathematical-representation","title":"\ud83e\uddee Mathematical Representation","text":"\\[ g = \\frac{1}{D} \\sum_{d=1}^{D} g_d \\] <p>Where:</p> <ul> <li>\\( D \\): Number of devices  </li> <li>\\( g_d \\): Gradient computed on device \\( d \\)</li> </ul> <p>Accelerate provides:</p> <ul> <li>Simple configuration for DDP</li> <li>Support for FSDP and DeepSpeed ZeRO</li> <li>Efficient gradient synchronization using PyTorch primitives </li> <li> <p>Gradient bucketing - combining many small gradient updates into a few larger batches before sharing them between GPUs - this reduces communication time and makes training faster.</p> Difference b/w Gradient Accumulation and Bucketing <ul> <li>Gradient Accumulation helps with memory limits \u2014 it adds up gradients over several mini-batches before taking an optimizer step, so you can simulate larger batch sizes on limited GPU memory. </li> <li>Gradient Bucketing helps with communication overhead \u2014 it groups many small gradients together before synchronizing across GPUs, so data exchange between devices is faster and more efficient.</li> </ul> </li> </ul>"},{"location":"frameworks/accelerate/#34-gradient-accumulation","title":"\ud83d\udcbe 3.4. Gradient Accumulation","text":"<p>Simulates large batch sizes without exceeding GPU memory limits by accumulating gradients over multiple mini-batches before performing an optimizer step.</p>"},{"location":"frameworks/accelerate/#mathematical-formulation","title":"\ud83e\uddee Mathematical Formulation","text":"\\[ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i \\] <p>Where:</p> <ul> <li>\\( N \\): Number of mini-batches accumulated  </li> <li>\\( g_i \\): Gradient from the \\( i^{th} \\) mini-batch</li> </ul>"},{"location":"frameworks/accelerate/#implementation-example","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation Example","text":"<p><pre><code>with accelerator.accumulate(model):\n    loss = model(**batch).loss\n    accelerator.backward(loss)\n</code></pre> \ud83d\ude80 Benefits</p> <ul> <li>Enables stable training even on smaller GPUs.</li> <li>Effectively increases batch size without additional memory requirements.</li> </ul>"},{"location":"frameworks/accelerate/#35-mixed-precision-training","title":"\ud83e\uddee 3.5. Mixed Precision Training","text":"<p>Accelerate integrates Automatic Mixed Precision (AMP) for performing computations using FP16 or BF16, while maintaining numerical stability and high throughput.</p>"},{"location":"frameworks/accelerate/#mechanism","title":"\u2699\ufe0f Mechanism","text":"<ul> <li>Forward Pass: Forward pass uses lower precision FP16  </li> <li>Backward Pass: Applies dynamic loss scaling to prevent gradient underflow (mainly FP16).  </li> <li>Optimizer Step: Performed in FP32 for numerical stability during parameter updates.</li> </ul>"},{"location":"frameworks/accelerate/#outcomes","title":"\ud83d\ude80 Outcomes","text":"<ul> <li>2\u00d7 faster training  </li> <li>~50% less GPU memory usage  </li> <li>Comparable accuracy to full FP32 training  </li> </ul>"},{"location":"frameworks/accelerate/#36-optimizer-and-scheduler-wrappers","title":"\u26a1 3.6. Optimizer and Scheduler Wrappers","text":"<p>Accelerate automatically scales and synchronizes optimizers and schedulers.</p> <pre><code>optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n</code></pre> <p>Key Functions:</p> <ul> <li>Synchronizes state across distributed workers. </li> <li>Compatibility with sharded optimizers (FSDP, ZeRO)  </li> <li>Works with common optimizers like AdamW and Adafactor</li> </ul>"},{"location":"frameworks/accelerate/#37-checkpointing-and-state-management","title":"\ud83e\uddf1 3.7. Checkpointing and State Management","text":"<p>Manages distributed checkpointing with process coordination:</p> <ul> <li>Consolidates multi-GPU state into single checkpoints. </li> <li>Includes model weights, optimizer states, RNG, and scheduler. </li> <li>Compatible with FSDP and ZeRO partitioned states.</li> </ul> <p>Example:</p> <pre><code>accelerator.save_state(output_dir=\"checkpoints/\")\n</code></pre> <p>Benefit: Fault-tolerant and restart-safe training in multi-node clusters.</p>"},{"location":"frameworks/accelerate/#38-logging-and-monitoring","title":"\ud83d\udd0d 3.8. Logging and Monitoring","text":"<p>Supports built-in and third-party loggers:</p> <ul> <li>TensorBoard, Weights &amp; Biases, MLflow, or custom. </li> <li>Ensures only the main process logs globally aggregated metrics.</li> <li>Built-in accelerator.print() avoids duplicate console output.</li> </ul> <pre><code>accelerator.log({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n</code></pre>"},{"location":"frameworks/accelerate/#39-memory-and-compute-efficiency-tools","title":"\ud83e\udde0 3.9. Memory and Compute Efficiency Tools","text":"<p>Accelerate provides hooks for reducing memory footprint:</p> <ul> <li>Gradient Checkpointing: Recomputes intermediate activations during backprop.</li> <li>Model Parameter Sharding (FSDP/ZeRO): Splits model weights across GPUs.</li> <li>Dynamic Padding: Reduces unnecessary computation on padded tokens.</li> </ul> <p>Useful for long-sequence transformer models where input lengths vary widely.</p>"},{"location":"frameworks/accelerate/#310-backend-support","title":"\ud83c\udf10 3.10. Backend Support","text":"<p>Accelerate integrates seamlessly across various distributed backends:</p> Backend Description Typical Use PyTorch DDP Default distributed backend Multi-GPU training FSDP Fully sharded parameter and optimizer state Memory-constrained setups DeepSpeed ZeRO Offloads parameters to CPU/NVMe Ultra-large LLMs (10B\u2013100B+) TPU/XLA TPU support via PyTorch/XLA Cloud TPU pods"},{"location":"frameworks/accelerate/#4-accelerate-training-workflow","title":"4. Accelerate Training Workflow","text":"<pre><code>from accelerate import Accelerator\n\n# Initialize Accelerator\naccelerator = Accelerator()\n\n# Prepare model, optimizer, dataloader\nmodel, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n\n# Training Loop\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        with accelerator.accumulate(model):\n            with accelerator.autocast():\n                outputs = model(**batch)\n                loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n\n    # Save checkpoint and log metrics\n    accelerator.save_state(f\"checkpoints/epoch_{epoch}\")\n    accelerator.log({\"epoch\": epoch, \"loss\": loss.item()})\n</code></pre>"},{"location":"optimization/4bit_normal_float/","title":"\ud83d\udcd0 4-bit NormalFloat (NF4) Quantization","text":""},{"location":"optimization/4bit_normal_float/#1-overview","title":"1. Overview","text":"<p>4-bit NormalFloat (NF4) is a quantization scheme designed for large language models (LLMs) to achieve maximum compression with minimal performance loss. It represents each weight with only 4 bits (16 levels) and leverages the normal distribution of model weights to allocate quantization levels more effectively than uniform schemes.</p> <p>NF4 is most effective when used with block-wise quantization and QLoRA fine-tuning, where adapter weights are trained on top of quantized base weights.</p>"},{"location":"optimization/4bit_normal_float/#2-key-concepts","title":"2. Key Concepts","text":""},{"location":"optimization/4bit_normal_float/#21-gaussian-aware-quantization","title":"2.1. Gaussian-Aware Quantization","text":"<ul> <li>Neural network weights approximately follow a zero-mean, Gaussian distribution.  </li> <li>NF4\u2019s quantization codebook is optimized for this shape - placing denser quantization levels near 0, where most weights reside.  </li> <li>This allows NF4 to maintain precision in the region that matters most.</li> </ul>"},{"location":"optimization/4bit_normal_float/#22-block-wise-normalization-high-level","title":"2.2. Block-Wise Normalization (High-Level)","text":"<p>NF4 typically operates per block of weights (e.g., 64\u2013256 elements) rather than over an entire layer. Each block computes: $$ \\mu_b = \\text{mean}(W_b), \\quad \\sigma_b = \\text{std}(W_b) $$</p> <p>Weights are normalized within the block before quantization: $$ \\hat{W}_b = \\frac{W_b - \\mu_b}{\\sigma_b} $$</p> <p>This local normalization:</p> <ul> <li>Prevents outliers from distorting scaling.</li> <li>Keeps data within a roughly standard normal distribution - perfectly matching NF4\u2019s codebook assumptions.  </li> </ul> <p>For deeper details on the block structure and scale storage, refer to Blockwise &amp; Double Quantization doc.</p>"},{"location":"optimization/4bit_normal_float/#3-quantization-and-dequantization","title":"3. Quantization and Dequantization","text":"<p>NF4 maps normalized weights to 4-bit integers using a precomputed normal-distribution codebook or a linear approximation.  </p> <p>Each block thus stores:</p> <ul> <li>4-bit quantized values \\(q_b\\)</li> <li>Its local mean \\(\\mu_b\\) and scale \\(\\sigma_b\\)</li> </ul>"},{"location":"optimization/4bit_normal_float/#31-quantization","title":"3.1 Quantization","text":"\\[ q_b = \\text{clip}\\big(\\text{round}(\\hat{W}_b \\times 7), -8, 7\\big) \\]"},{"location":"optimization/4bit_normal_float/#32-dequantization","title":"3.2 Dequantization","text":"\\[ W_b^{\\text{dequant}} = \\frac{q_b}{7} \\cdot \\sigma_b + \\mu_b \\] <p>This operation ensures minimal reconstruction error between the quantized and original weights.</p>"},{"location":"optimization/4bit_normal_float/#4-working-example-python","title":"4. Working Example (Python)","text":"<pre><code>import torch\n\n# Example weight block\nW_block = torch.tensor([0.12, -0.34, 0.56, -1.2, 0.9])\n\n# Compute block stats\nmu = W_block.mean()\nsigma = W_block.std()\n\n# Normalize and quantize\nW_norm = (W_block - mu) / sigma\nq = torch.clamp(torch.round(W_norm * 7), -8, 7)\n\n# Dequantize\nW_dequant = q / 7 * sigma + mu\n\nprint(\"Original Weights:\", W_block.numpy())\nprint(\"Quantized 4-bit:\", q.numpy())\nprint(\"Dequantized:\", W_dequant.numpy())\n</code></pre> <pre><code>Output: \n\nOriginal Weights: [ 0.12 -0.34  0.56 -1.2   0.9 ]\nQuantized 4-bit: [ 0 -2  3 -8  5 ]\nDequantized: [ 0.14 -0.33 0.57 -1.21 0.88 ]\n</code></pre>"},{"location":"optimization/4bit_normal_float/#5-nf4-calibration-drift","title":"5. \u2696\ufe0f NF4 Calibration Drift","text":"<p>While NF4 quantization provides highly efficient compression, it can suffer from a subtle issue known as calibration drift.</p> <p>Calibration drift occurs when the effective operating distribution of activations shifts relative to the original weight calibration used for NF4 quantization. </p> <p>Although NF4 uses the mean and standard deviation of each weight block for quantization and the base weights are frozen, the LoRA adapters introduce low-rank updates that alter the inputs (activations) flowing through the quantized layers. This can change the regions of the quantized bins that are being used, effectively causing a mismatch between the quantized weights\u2019 calibration and their new operating regime.</p>"},{"location":"optimization/4bit_normal_float/#51-why-it-happens","title":"5.1. Why It Happens","text":"<p>Even though the base model weights \\(W_q\\) are frozen: $$ h = (W_q + \\frac{\\alpha}{r} B A) x $$ the LoRA adapters \\(A, B\\) shift the activations \\(x \\to x' = x + \\Delta x\\), which modifies the pre-activation distribution seen by the quantized weights. This does not change the quantized weights themselves, but it can reduce the effective precision in the computation because the quantized bins may now be used differently than during calibration.</p>"},{"location":"optimization/4bit_normal_float/#52-effects","title":"5.2. Effects","text":"<ul> <li>Slight reduction in representational fidelity of quantized weights  </li> <li>Minor degradation in numerical stability or perplexity  </li> <li>Potential loss of precision in downstream layers if activation shifts are large  </li> </ul>"},{"location":"optimization/4bit_normal_float/#53-mitigation-strategies","title":"5.3. Mitigation Strategies","text":"<ul> <li>Recalibrate quantization scales after fine-tuning or periodically during long runs  </li> <li>Apply SmoothQuant to shift scaling between weights and activations  </li> <li>Use Quantization-Aware Fine-Tuning (QAFT) to make adapters robust to quantization noise  </li> <li>Limit LoRA influence via smaller rank \\(r\\) or scaling factor \\(\\alpha\\)</li> <li>Use larger block sizes (e.g., 128) to reduce sensitivity to local activation shifts  </li> </ul> <p>In practice, QLoRA\u2019s frozen-weight design and low-rank adapters keep drift minimal, but understanding this effect is important for advanced fine-tuning and quantization-aware training workflows.</p>"},{"location":"optimization/4bit_normal_float/#6-practical-notes","title":"6. Practical Notes","text":"<ul> <li>Precision Trade-off: <code>4-bit NF4</code> achieves near-float accuracy while reducing memory up to <code>4x</code>.</li> <li>Block Dependency: <code>NF4</code> inherently requires per-block normalization (mean &amp; std). Without it, a global scale would fail due to outliers.</li> <li>Compatibility: Used in QLoRA, bitsandbytes, and PEFT libraries for efficient 4-bit fine-tuning.</li> <li>Performance: Empirical studies (Dettmers et al., 2023) show NF4 retains &gt;99.5% of FP16 accuracy for LLaMA-like models with up to <code>8.1x</code> faster training throughput.</li> </ul>"},{"location":"optimization/4bit_normal_float/#7-summary","title":"7. Summary","text":"Aspect Description Bit-width 4 bits Quantization type Non-uniform (NormalFloat codebook) Normalization Per block (mean &amp; std) Key benefit Precision around zero preserved Typical use QLoRA / LoRA fine-tuning Dependency Requires block-wise normalization"},{"location":"optimization/blockwise_kbit_quantization/","title":"\u2699\ufe0f Block-wise k-bit Quantization","text":""},{"location":"optimization/blockwise_kbit_quantization/#1-overview","title":"1. Overview","text":"<p>Block-wise k-bit quantization is a technique that compresses model weights into low-bit representations (e.g., 4-bit, 8-bit) while preserving performance and minimizing quantization error. Instead of quantizing each value independently, block-wise quantization divides the weight matrix into smaller blocks (chunks) and performs quantization relative to local statistics (like scale and zero-point) of each block.</p> <p>This local normalization significantly reduces quantization error caused by outlier values \u2014 a common issue in transformer weights.</p>"},{"location":"optimization/blockwise_kbit_quantization/#2-motivation","title":"2. Motivation","text":""},{"location":"optimization/blockwise_kbit_quantization/#problem-outliers-in-weight-distributions","title":"\ud83e\udde0 Problem: Outliers in Weight Distributions","text":"<p>Weights in large models (especially in attention layers) often follow heavy-tailed distributions \u2014 a few large values coexist with many small ones. In global quantization, a single scale \\( s_{\\text{global}} = \\frac{\\max(|W|)}{2^{k-1}-1} \\) is used for all weights. Large outliers force the scale up, making most small weights collapse to zero after quantization.</p> <p>Example</p> <p>Consider: $$ W = [0.01, 0.02, -0.03, 0.05, 3.0] $$</p> <p>With 4-bit global quantization: $$ s_{\\text{global}} = \\frac{3.0}{7} \\approx 0.43 $$</p> <p>Quantized weights \u2192 <code>[0, 0, 0, 0, 7]</code> \u2014 almost all small weights vanish due to the single large outlier.</p>"},{"location":"optimization/blockwise_kbit_quantization/#solution-block-wise-quantization","title":"\u2705 Solution: Block-wise Quantization","text":"<p>Split weights into small blocks (e.g., 64\u2013256 values each), and compute a separate scale per block: $$ s_b = \\frac{\\max(|W_b|)}{2^{k-1}-1} $$ Each block adapts to its local range, preserving fine details while still compressing efficiently.</p> <p>By partitioning weights into blocks and computing scale/offset per block, quantization adapts to local statistics and better preserves precision.</p>"},{"location":"optimization/blockwise_kbit_quantization/#3-mathematical-formulation","title":"3. Mathematical Formulation","text":"<p>\ud83d\udcd8 Steps for block quantization</p> <p>Let:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\): full-precision weight matrix</li> <li>\\(B_i \\subset W\\): the i-th block of size \\(n_b\\)</li> <li>\\(k\\): number of bits used for quantization (e.g., 4 or 8)</li> </ul>"},{"location":"optimization/blockwise_kbit_quantization/#step-1-compute-local-scale-and-zero-point","title":"Step 1: Compute Local Scale and Zero-Point","text":"<p>For each block \\(B_i\\):</p> \\[ s_i = \\frac{\\max(B_i) - \\min(B_i)}{2^k - 1} \\] \\[ z_i = \\text{round}\\left(-\\frac{\\min(B_i)}{s_i}\\right) \\] <p>Where:</p> <ul> <li>\\(s_i\\): scale factor for block \\(i\\)</li> <li>\\(z_i\\): zero-point (offset)</li> </ul>"},{"location":"optimization/blockwise_kbit_quantization/#step-2-quantization","title":"Step 2: Quantization","text":"<p>Quantized integer representation:</p> \\[ q_i = \\text{clip}\\left(\\text{round}\\left(\\frac{B_i}{s_i}\\right) + z_i, 0, 2^k - 1\\right) \\]"},{"location":"optimization/blockwise_kbit_quantization/#step-3-dequantization-reconstruction","title":"Step 3: Dequantization (Reconstruction)","text":"\\[ \\hat{B_i} = s_i \\times (q_i - z_i) \\] <p>The final reconstructed weight matrix:</p> \\[ \\hat{W} = \\bigcup_i \\hat{B_i} \\]"},{"location":"optimization/blockwise_kbit_quantization/#4-double-quantization","title":"4. Double Quantization","text":"<p>Double quantization is a secondary compression layer designed to reduce the overhead of storing multiple block-wise scales. Instead of storing each block\u2019s scaling factor \\( s_j \\) as a 16-bit or 32-bit float, these scale values themselves are quantized into a lower precision representation (e.g., 8-bit or 4-bit).</p> <p>\ud83d\udcd8 Details</p>"},{"location":"optimization/blockwise_kbit_quantization/#41-concept","title":"4.1. Concept","text":"<p>If there are \\(N\\) blocks, each with a scale \\(s_j\\):</p> \\[ \\tilde{s_j} = \\text{quantize}(s_j, s_{\\text{meta}}, q_{\\min}, q_{\\max}) \\] <p>Here, \\(s_{\\text{meta}}\\) is a higher-level scale shared across a group of block-scales.</p> <p>At dequantization:</p> \\[ s_j = s_{\\text{meta}} \\cdot \\tilde{s_j} \\] \\[ \\hat{x_i} = s_j \\cdot q_i \\] <p>This approach can yield 20\u201330% memory savings, especially when using small block sizes where the number of stored scales is large.</p>"},{"location":"optimization/blockwise_kbit_quantization/#42-example","title":"4.2. Example","text":"<p>Consider a model layer with 10,000 blocks of weights. Each block has one scale \\( s_i \\).</p> Parameter Value Number of blocks 10,000 Scale per block (FP16) 2 bytes Memory (without double quantization) 20 KB Quantized scale (8-bit) 1 byte Memory (with double quantization) 10 KB <p>So double quantization reduces metadata memory by 50% with negligible degradation (typically &lt; 0.1% accuracy loss).</p>"},{"location":"optimization/blockwise_kbit_quantization/#43-implementation-in-bitsandbytes","title":"4.3. Implementation in Bitsandbytes","text":"<p>In bitsandbytes 0.39+, both block-wise quantization and double quantization are implemented jointly:</p> <ul> <li>Each weight block is quantized in NF4 format.</li> <li>Each block\u2019s scale value is quantized using 8-bit quantization.</li> <li>The quantized scales are stored alongside the 4-bit codes.</li> <li>Dequantization happens transparently during forward passes.</li> </ul> <p>This enables models like LLaMA-2 70B to be fine-tuned on single 48GB GPUs.</p>"},{"location":"optimization/blockwise_kbit_quantization/#44-key-notes","title":"4.4 Key Notes","text":"<ul> <li>Double quantization is orthogonal but complementary to block-wise quantization.  </li> <li>It primarily targets metadata compression, not model accuracy.  </li> <li>Used in QLoRA to compress per-block scales efficiently.</li> </ul> Aspect Effect Memory Efficiency Up to 2\u00d7 reduction in metadata storage Accuracy Impact Negligible (&lt; 0.1% degradation) Computation Overhead Minimal (scales dequantized once per block) Compatibility Fully supported in <code>bitsandbytes</code> &amp; <code>QLoRA</code> stack"},{"location":"optimization/blockwise_kbit_quantization/#5-implementation-details-pseudo-code","title":"5. Implementation Details (Pseudo-Code)","text":"<pre><code>def blockwise_quantize(weights, block_size=64, num_bits=4):\n    q_blocks, scales, zeros = [], [], []\n    n = len(weights)\n    for i in range(0, n, block_size):\n        block = weights[i:i+block_size]\n        min_val, max_val = block.min(), block.max()\n        scale = (max_val - min_val) / (2 ** num_bits - 1)\n        zero_point = -min_val / scale\n        q_block = np.round(block / scale + zero_point).clip(0, 2 ** num_bits - 1)\n        q_blocks.append(q_block)\n        scales.append(scale)\n        zeros.append(zero_point)\n    return q_blocks, scales, zeros\n</code></pre>"},{"location":"optimization/blockwise_kbit_quantization/#6-example-4-bit-quantization","title":"6. Example (4-bit Quantization)","text":"<p>\ud83d\udcd8 Working example</p> <p>Consider a block of weights:</p> \\[ B_i = [-0.9, -0.3, 0.2, 0.5, 1.0] \\] <p>For \\(k = 4\\) bits:</p> <ul> <li>\\(\\min = -0.9, \\max = 1.0\\)</li> <li>\\(s_i = (1.0 - (-0.9)) / 15 = 0.1267\\)</li> <li>\\(z_i = -(-0.9) / 0.1267 = 7.1 \\approx 7\\)</li> </ul> <p>Quantized values:</p> \\[ q_i = \\text{round}(B_i / s_i + z_i) = [0, 5, 9, 11, 15] \\] <p>Dequantized:</p> \\[ \\hat{B_i} = s_i \\times (q_i - 7) = [-0.9, -0.26, 0.32, 0.51, 1.01] \\] <p>The reconstruction closely approximates the original block.</p>"},{"location":"optimization/blockwise_kbit_quantization/#7-advantages","title":"7. Advantages","text":"Aspect Benefit Local scaling Reduces sensitivity to outliers Memory Lower storage cost (e.g., 4-bit = 8\u00d7 compression) Compute Enables efficient GPU matrix-multiplication with custom kernels Accuracy Closer performance to full precision"},{"location":"optimization/blockwise_kbit_quantization/#8-hardware-implementation","title":"8. Hardware Implementation","text":"<ul> <li>Most modern inference frameworks (e.g., bitsandbytes, TensorRT) store the scale and zero-point per block.</li> <li>For 4-bit quantization, typical block sizes: 32, 64, or 128.</li> <li>Scales are stored in FP16 to balance precision and storage.</li> </ul>"},{"location":"optimization/blockwise_kbit_quantization/#9-visualization","title":"9. Visualization","text":"<p>A conceptual diagram of block-wise quantization:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Weight Matrix      \u2502\n\u2502  [w\u2081, w\u2082, \u2026, w\u2099]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 Split into Blocks\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Block 1      \u2502 Block 2      \u2502 ...\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193                 \u2193\nCompute s\u2081,z\u2081      Compute s\u2082,z\u2082\n     \u2193                 \u2193\nQuantize each block separately\n     \u2193                 \u2193\nStore q\u2081,s\u2081,z\u2081,...,q\u2099,s\u2099,z\u2099\n</code></pre> <p>Each block retains its own quantization scale and offset, enabling more accurate low-bit representation.</p>"},{"location":"optimization/blockwise_kbit_quantization/#10-relationship-to-qlora","title":"10. Relationship to QLoRA","text":"<p>QLoRA uses 4-bit NormalFloat (NF4) quantization with block-wise statistics:</p> <ul> <li>Each block (typically 64 elements) uses local mean and std for normalization.</li> <li>NF4 values are quantized into [-1, 1] with learned scales.</li> <li>This approach allows fine-tuning large LLMs on a single GPU without significant accuracy loss.</li> </ul>"},{"location":"optimization/bp16/","title":"\ud83e\uddee BF16 (BFloat16): Mixed Precision for Stable LLM Training","text":""},{"location":"optimization/bp16/#1-overview","title":"1. Overview","text":"<p>BF16 (Brain Floating Point 16) is a 16-bit floating-point format designed specifically for numerically stable neural network training.</p> <p>The core idea behind BF16 is simple:</p> <p>Preserve the numerical range required for training, while reducing memory and compute cost.</p> <p>This makes BF16 especially suitable for training large models such as LLMs, where values can vary widely during forward and backward passes.</p>"},{"location":"optimization/bp16/#2-motivation-what-goes-wrong-with-lower-precision","title":"2. Motivation: What Goes Wrong with Lower Precision","text":"<p>Training deep neural networks involves: - Very large values in activations and gradients - Very small gradient updates - Accumulation of numerical error over long runs</p> <p>Standard FP16 improves speed and memory usage but introduces a major issue: - Its limited exponent range causes gradient underflow and overflow</p> <p>BF16 was introduced to solve these stability issues without reverting to FP32.</p>"},{"location":"optimization/bp16/#3-bf16-vs-fp16-vs-fp32","title":"3. BF16 vs FP16 vs FP32","text":"Format Bits Exponent Bits Mantissa Bits Dynamic Range Precision FP32 32 8 23 High High FP16 16 5 10 Low Medium BF16 16 8 7 High (same as FP32) Lower <p>Justification - Training stability depends more on range than fine-grained precision - Small rounding errors are usually tolerable - Underflow and overflow are not</p> <p>BF16 keeps the FP32 exponent, which directly addresses instability.</p>"},{"location":"optimization/bp16/#4-fp16-vs-bf16-precision-vs-range-through-examples","title":"4. FP16 vs BF16: Precision vs Range Through Examples","text":"<p>These examples illustrate the core tradeoff between FP16 and BF16: FP16 has higher precision, while BF16 has larger numerical range.</p> <p>Example 1: Small but representable value</p> <p>Original value: <code>0.0001</code></p> <ul> <li>FP16: <code>0.00010001659393</code>   (10-bit mantissa, 5-bit exponent)</li> <li>BF16: <code>0.00010013580322</code>   (7-bit mantissa, 8-bit exponent)</li> </ul> <p>Explanation</p> <p>Both formats can represent this value, but FP16 is closer to the original because it has more mantissa bits. This shows FP16\u2019s advantage in precision when the value lies within its representable range.</p> <p>Example 2: Very small value</p> <p>Original value: <code>1e-08</code></p> <ul> <li>FP16: <code>0.0</code>   (underflow)</li> <li>BF16: <code>0.00000001001172</code></li> </ul> <p>Explanation</p> <p>FP16 cannot represent this value because its binary exponent range is too small, even though it has 10 mantissa bits. The number of decimal zeros is irrelevant \u2014 FP16\u2019s minimum normalized positive number is about <code>6.1e-5</code>, and numbers smaller than this either underflow or rely on very low-precision subnormals. BF16 succeeds because it has the same exponent range as FP32, allowing much smaller numbers to be represented reliably.</p> <p>This is a key reason BF16 is more stable during training, especially for gradients that can be extremely small.</p> <p>Example 3: Large value</p> <p>Original value: <code>100000.00001</code></p> <ul> <li>FP16: <code>inf</code>   (overflow)</li> <li>BF16: <code>99840.0</code></li> </ul> <p>Explanation</p> <p>FP16 overflows because all exponent bits are exhausted. BF16 can still represent the value, though with reduced precision, because it has more exponent bits.</p> <p>Key takeaway</p> <ul> <li>FP16 represents values more accurately within its limited range  </li> <li>BF16 represents values more reliably across a wide range  </li> <li>Training prefers range over precision, which is why BF16 is often safer for large models</li> </ul>"},{"location":"optimization/bp16/#5-why-bf16-is-stable-during-training","title":"5. Why BF16 Is Stable During Training","text":"<p>During backpropagation: - Gradients can span many orders of magnitude - Values that fall outside representable range collapse to zero or NaN</p> <p>Because BF16 has the same exponent range as FP32: - Gradients rarely underflow - Overflow is significantly reduced</p> <p>This leads to more predictable and stable optimization behavior.</p>"},{"location":"optimization/bp16/#6-loss-scaling-and-why-bf16-usually-does-not-need-it","title":"6. Loss Scaling and Why BF16 Usually Does Not Need It","text":"<p>Loss scaling multiplies the loss to push gradients into a representable range.</p> <ul> <li>FP16 requires loss scaling because its exponent range is small</li> <li>BF16 usually does not, because the range is already sufficient</li> </ul> <p>Implication - BF16 simplifies training pipelines - Fewer tuning knobs - Fewer silent numerical failures</p> Loss Scaling (click to expand) In mixed-precision training with FP16, gradients can become too small to be represented and underflow to zero, which stops learning.  Loss scaling prevents this by temporarily increasing gradient magnitudes without changing the true optimization objective.   How it works 1. Multiply the loss by a scale factor   2. Backpropagate using the scaled loss   3. Divide gradients by the same factor before the optimizer step   **Example** A true gradient:  $$ g = 1 \\times 10^{-9} $$  In FP16, this value underflows to zero. With a scale factor \\( S = 1024 \\):  $$ g' = S \\cdot g = 1.024 \\times 10^{-6} $$  This value is representable in FP16. Before the optimizer update, gradients are divided by \\( S \\), preserving the correct update.  Key point - FP16 requires loss scaling due to limited exponent range   - BF16 usually does not, because it preserves FP32\u2019s range"},{"location":"optimization/bp16/#7-performance-and-memory-characteristics","title":"7. Performance and Memory Characteristics","text":"<p>BF16 reduces memory usage by half compared to FP32, which: - Allows larger batch sizes - Reduces memory bandwidth pressure</p> <p>On supported hardware, BF16: - Uses tensor cores - Achieves near-FP16 throughput - Converges similarly to FP32 in practice</p> <p>The slight loss in mantissa precision rarely affects convergence for deep models.</p>"},{"location":"optimization/bp16/#8-bf16-in-practice-pytorch","title":"8. BF16 in Practice (PyTorch)","text":"<p>```python import torch</p> <p>model = model.to(dtype=torch.bfloat16)</p> <p>with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):     outputs = model(inputs)     loss = outputs.loss</p>"},{"location":"optimization/memory_considerations/","title":"Memory Consideration","text":""},{"location":"optimization/memory_considerations/#1-gpu-and-cpu-memory-and-transfer-rates","title":"1. GPU and CPU Memory and Transfer Rates","text":""},{"location":"optimization/memory_considerations/#11-gpu-memory","title":"1.1 GPU Memory","text":"<p>GPU memory, often called HBM (High Bandwidth Memory) or VRAM, is high bandwidth memory physically attached to the GPU package. It is designed to feed thousands of parallel compute cores efficiently.</p> <p>Key characteristics:</p> <ul> <li>Very high bandwidth</li> <li>Low access latency relative to CPU memory</li> <li>Limited capacity compared to system RAM</li> </ul> <p>Typical values for NVIDIA A100:</p> <ul> <li>Capacity: 40GB or 80GB HBM2e</li> <li>Peak bandwidth: ~1.6 TB/s</li> </ul> <p>GPU memory stores:</p> <ul> <li>Model weights</li> <li>Activations</li> <li>Gradients</li> <li>Optimizer states</li> <li>KV cache during inference and decoding</li> </ul>"},{"location":"optimization/memory_considerations/#12-cpu-memory","title":"1.2 CPU Memory","text":"<p>CPU memory refers to system RAM, typically DDR4 or DDR5, located on the motherboard.</p> <p>Key characteristics:</p> <ul> <li>Much larger capacity</li> <li>Significantly lower bandwidth</li> <li>Higher latency compared to GPU memory</li> </ul> <p>Typical values:</p> <ul> <li>Capacity: 64GB to 1TB+</li> <li>Bandwidth: ~50 to 100 GB/s per socket</li> </ul> <p>CPU memory is commonly used for:</p> <ul> <li>Data loading and preprocessing</li> <li>Checkpoint storage before transfer</li> <li>Offloaded parameters or optimizer states in memory constrained setups</li> </ul>"},{"location":"optimization/memory_considerations/#13-gpu-to-cpu-transfer-rates","title":"1.3 GPU to CPU Transfer Rates","text":"<p>Data movement between GPU and CPU happens over interconnects.</p> <p>Approximate peak transfer rates:</p> <ul> <li>PCIe Gen4: ~32 GB/s</li> <li>PCIe Gen5: ~64 GB/s</li> <li>NVLink (A100): ~300 GB/s</li> </ul> <p>Even with NVLink, transfer bandwidth is far lower than on-device GPU memory bandwidth, making frequent transfers expensive.</p>"},{"location":"optimization/memory_considerations/#2-bandwidth-latency-and-compute","title":"2. Bandwidth, Latency, and Compute","text":""},{"location":"optimization/memory_considerations/#21-bandwidth","title":"2.1 Bandwidth","text":"<p>Bandwidth measures how much data can be transferred per unit time, typically in GB/s or TB/s.</p> <p>In training and inference:</p> <ul> <li>Large tensors are streamed repeatedly</li> <li>Sustained bandwidth determines throughput</li> <li>Many transformer operations are memory bandwidth bound</li> </ul>"},{"location":"optimization/memory_considerations/#22-latency","title":"2.2 Latency","text":"<p>Latency is the delay to access the first byte of data.</p> <ul> <li>GPU memory latency is lower than CPU memory</li> <li>PCIe transfers have high latency relative to on-device access</li> </ul> <p>Latency matters for:</p> <ul> <li>Small tensor operations</li> <li>Kernel launch overhead</li> <li>Synchronization points</li> </ul> <p>For large matrix operations, bandwidth dominates over latency.</p>"},{"location":"optimization/memory_considerations/#23-compute","title":"2.3 Compute","text":"<p>Compute refers to the raw arithmetic capability of the processor, measured in FLOPs.</p> <p>A100 peak performance: - FP16 or BF16 Tensor Core: ~312 TFLOPs</p> <p>In practice: - Many LLM workloads are not compute bound - Compute units often wait on memory due to limited data reuse</p>"},{"location":"optimization/memory_considerations/#24-compute-vs-memory-bound-regimes","title":"2.4 Compute vs Memory Bound Regimes","text":"<ul> <li>Compute bound: performance limited by FLOPs</li> <li>Memory bound: performance limited by data movement</li> </ul> <p>Transformers often become memory bound during:</p> <ul> <li>Attention</li> <li>Layer normalization</li> <li>Optimizer updates during training</li> </ul>"},{"location":"optimization/memory_considerations/#3-training-considerations-for-a-7b-model-on-a-single-a100","title":"3. Training Considerations for a 7B Model on a Single A100","text":""},{"location":"optimization/memory_considerations/#31-memory-components-during-training","title":"3.1 Memory Components During Training","text":"<p>Training requires storing four major components in GPU memory:</p> <ol> <li>Model weights</li> <li>Gradients</li> <li>Optimizer states</li> <li>Activations</li> </ol> <p>We quantify each component explicitly.</p>"},{"location":"optimization/memory_considerations/#parameter-definitions","title":"Parameter Definitions","text":"<p>Let:</p> <ul> <li>\\(P = 7 \\times 10^9\\) parameters  </li> <li>BF16 precision for weights and gradients: 2 bytes  </li> <li>FP32 precision for optimizer states: 4 bytes  </li> </ul>"},{"location":"optimization/memory_considerations/#311-model-weights","title":"3.1.1 Model Weights","text":"\\[ M_{\\text{weights}} = P \\times 2 \\text{ bytes} \\] \\[ = 7 \\times 10^9 \\times 2 = 14 \\text{ GB} \\]"},{"location":"optimization/memory_considerations/#312-gradients","title":"3.1.2 Gradients","text":"<p>Each trainable parameter produces one gradient tensor.</p> \\[ M_{\\text{grads}} = P \\times 2 \\text{ bytes} \\] \\[ = 14 \\text{ GB} \\] <p>Running total so far:</p> \\[ 28 \\text{ GB} \\]"},{"location":"optimization/memory_considerations/#313-optimizer-states-adam-or-adamw","title":"3.1.3 Optimizer States (Adam or AdamW)","text":"<p>Adam maintains two FP32 states per parameter:</p> <ul> <li>First moment</li> <li>Second moment</li> </ul> \\[ M_{\\text{optimizer}} = P \\times 2 \\times 4 \\text{ bytes} \\] \\[ = 7 \\times 10^9 \\times 8 = 56 \\text{ GB} \\] <p>Running total:</p> \\[ 14 + 14 + 56 = 84 \\text{ GB} \\] <p>This already exceeds the memory of an A100 80GB.</p>"},{"location":"optimization/memory_considerations/#314-activations","title":"3.1.4 Activations","text":"<p>Activation memory depends on:</p> <ul> <li>Number of layers \\(L\\)</li> <li>Batch size \\(B\\)</li> <li>Sequence length \\(T\\)</li> <li>Hidden dimension \\(H\\)</li> </ul> <p>A simplified scaling relation:</p> \\[ M_{\\text{act}} = O(L \\times B \\times T \\times H \\times 2 \\text{ bytes}) \\] <p>For a 7B transformer with gradient checkpointing:</p> <ul> <li>Activations typically consume 5 to 15 GB</li> </ul>"},{"location":"optimization/memory_considerations/#315-total-training-memory","title":"3.1.5 Total Training Memory","text":"\\[ M_{\\text{total}} = M_{\\text{weights}} + M_{\\text{grads}} + M_{\\text{optimizer}} + M_{\\text{act}} \\] \\[ \\approx 90 \\text{ to } 100 \\text{ GB} \\] <p>This explains why full fine-tuning of a 7B model does not fit on a single A100.</p>"},{"location":"optimization/memory_considerations/#32-techniques-that-enable-feasible-training","title":"3.2 Techniques That Enable Feasible Training","text":"<p>Common strategies include:</p> <ul> <li>Parameter-efficient fine-tuning such as LoRA or adapters</li> <li>Mixed precision training using BF16</li> <li>Gradient checkpointing to reduce activation memory</li> <li>Small micro-batches with gradient accumulation</li> <li>Limiting sequence length</li> </ul> <p>These techniques primarily reduce:</p> <ul> <li>Gradient memory</li> <li>Optimizer state memory</li> <li>Activation memory</li> </ul>"},{"location":"optimization/memory_considerations/#33-cpu-offloading","title":"3.3 CPU Offloading","text":"<p>CPU offloading can move:</p> <ul> <li>Optimizer states</li> <li>Parameters</li> </ul> <p>Tradeoffs:</p> <ul> <li>Enables fitting larger models</li> <li>Severely reduces training throughput</li> <li>Often impractical for production training</li> </ul>"},{"location":"optimization/memory_considerations/#4-memory-differences-between-training-and-inference","title":"4. Memory Differences Between Training and Inference","text":""},{"location":"optimization/memory_considerations/#41-training-memory-profile","title":"4.1 Training Memory Profile","text":"<p>Training requires:</p> <ul> <li>Weights</li> <li>Activations from forward pass</li> <li>Gradients from backward pass</li> <li>Optimizer states</li> </ul> <p>Memory usage is dominated by optimizer states and activations.</p>"},{"location":"optimization/memory_considerations/#42-inference-memory-profile","title":"4.2 Inference Memory Profile","text":"<p>Inference requires:</p> <ul> <li>Model weights</li> <li>Activations for current forward pass</li> <li>KV cache for attention</li> </ul> <p>Notably absent:</p> <ul> <li>Gradients</li> <li>Optimizer states</li> </ul> <p>This is why inference fits models that training cannot.</p>"},{"location":"optimization/memory_considerations/#43-quantitative-difference","title":"4.3 Quantitative Difference","text":"<p>For a 7B model:</p> <ul> <li>Training memory: ~90GB or more</li> <li>Inference memory: ~20 to 30GB depending on sequence length and KV cache size</li> </ul>"},{"location":"optimization/memory_considerations/#5-kv-cache-memory-usage","title":"5. KV Cache Memory Usage","text":""},{"location":"optimization/memory_considerations/#51-what-is-the-kv-cache","title":"5.1 What Is the KV Cache","text":"<p>The KV cache stores the key and value tensors from the attention mechanism for previously processed tokens.</p> <p>It avoids recomputing attention over the full context during autoregressive decoding.</p>"},{"location":"optimization/memory_considerations/#52-which-memory-does-kv-cache-use","title":"5.2 Which Memory Does KV Cache Use","text":"<p>KV cache resides in:</p> <ul> <li>GPU memory during standard inference and serving</li> <li>CPU memory only in specialized offloading or paging setups</li> </ul> <p>During high throughput inference:</p> <ul> <li>KV cache must stay in GPU memory</li> <li>Frequent access makes CPU storage impractical</li> </ul>"},{"location":"optimization/memory_considerations/#53-kv-cache-memory-scaling","title":"5.3 KV Cache Memory Scaling","text":"<p>KV cache memory scales with:</p> <ul> <li>Number of layers</li> <li>Number of attention heads</li> <li>Sequence length</li> <li>Batch size</li> </ul> <p>This makes KV cache the dominant memory consumer during long context inference.</p>"},{"location":"optimization/memory_considerations/#6-summary","title":"6. Summary","text":"<p>Key takeaways: - GPU memory is the primary constraint for both training and inference - Bandwidth often limits performance more than compute - Training requires additional memory for gradients and optimizer states - Inference is cheaper because it avoids optimizer and gradient storage - KV cache lives in GPU memory and dominates long-context inference memory</p> <p>Understanding these tradeoffs is essential for designing and debugging large-scale LLM systems.</p>"},{"location":"optimization/paged_adam/","title":"Paged Adam Optimizer","text":""},{"location":"optimization/paged_adam/#1-motivation","title":"1. Motivation","text":"<p>The Adam optimizer maintains additional state for each trainable parameter, which becomes a major memory bottleneck when training or fine tuning large language models.</p> <p>For each parameter, Adam stores:</p> <ul> <li>First moment estimate \\(m\\) in FP32</li> <li>Second moment estimate \\(v\\) in FP32</li> </ul> <p>This requires 8 bytes per parameter, often exceeding available GPU memory even when model weights fit.</p> <p>Paged Adam is designed to address this optimizer state memory bottleneck.</p>"},{"location":"optimization/paged_adam/#2-what-is-paged-adam","title":"2. What Is Paged Adam","text":"<p>Paged Adam is a memory managed variant of the Adam optimizer that stores optimizer states in CPU memory and moves them to GPU memory in small chunks only when needed for the update step.</p> <p>The Adam update rule itself is unchanged. The optimization focuses entirely on where optimizer states live and how they are accessed.</p> <p>Paged Adam is most commonly implemented in the <code>bitsandbytes</code> library and is widely used in large model fine tuning workflows.</p>"},{"location":"optimization/paged_adam/#3-core-idea","title":"3. Core Idea","text":"<p>The key idea behind Paged Adam is inspired by virtual memory systems.</p> <ul> <li>Optimizer states reside primarily in host memory</li> <li>Small pages of optimizer states are transferred to the GPU</li> <li>Updates are applied for that page</li> <li>Updated states are written back to CPU memory</li> <li>The next page is processed</li> </ul> <p>At no point do all optimizer states need to be resident in GPU memory.</p>"},{"location":"optimization/paged_adam/#4-step-by-step-operation","title":"4. Step by Step Operation","text":"<p>During each optimizer step:</p> <ol> <li>Gradients are computed on the GPU.</li> <li>A page of optimizer states \\(m, v\\) is transferred from CPU memory to GPU memory.</li> <li>The Adam update is applied for the corresponding parameters.</li> <li>Updated optimizer states are transferred back to CPU memory.</li> <li>The process repeats for the next page.</li> </ol> <p>Only a small fraction of optimizer state memory is present on the GPU at any time.</p>"},{"location":"optimization/paged_adam/#5-memory-benefits","title":"5. Memory Benefits","text":"<p>Paged Adam dramatically reduces GPU memory usage by optimizer states.</p> <ul> <li>GPU memory usage becomes nearly constant with respect to model size</li> <li>Enables fine-tuning models that exceed GPU memory limits</li> <li>Particularly effective when combined with LoRA or QLoRA</li> </ul> <p>This makes single GPU fine tuning of large models feasible.</p>"},{"location":"optimization/paged_adam/#6-performance-tradeoffs","title":"6. Performance Tradeoffs","text":"<p>The main cost of Paged Adam is performance.</p> <ul> <li>Frequent CPU to GPU memory transfers</li> <li>PCIe or NVLink bandwidth becomes the bottleneck</li> <li>Optimizer step latency increases significantly</li> </ul> <p>As a result:</p> <ul> <li>Training throughput is lower</li> <li>Paged Adam is better suited for fine-tuning than large scale pretraining</li> </ul>"},{"location":"optimization/paged_adam/#7-relationship-to-other-memory-saving-techniques","title":"7. Relationship to Other Memory Saving Techniques","text":""},{"location":"optimization/paged_adam/#71-paged-adam-vs-zero-offload","title":"7.1 Paged Adam vs ZeRO Offload","text":"Aspect Paged Adam ZeRO Offload Scope Optimizer states only Parameters, gradients, optimizer Granularity Page level Tensor or state level Typical use Single GPU setups Multi GPU distributed training Complexity Relatively simple Distributed system complexity"},{"location":"optimization/paged_adam/#72-paged-adam-and-8-bit-optimizers","title":"7.2 Paged Adam and 8-bit Optimizers","text":"<p>Paged Adam is often combined with:</p> <ul> <li>8-bit or 4-bit optimizer states</li> </ul> <p>This reduces:</p> <ul> <li>CPU memory usage</li> <li>Data transfer volume per page</li> </ul> <p>This combination is common in QLoRA training pipelines.</p>"},{"location":"optimization/paged_adam/#8-when-to-use-paged-adam","title":"8. When to Use Paged Adam","text":"<p>Paged Adam is well suited for:</p> <ul> <li>Single GPU fine-tuning</li> <li>Memory constrained environments</li> <li>LoRA or QLoRA based training</li> </ul> <p>Paged Adam is less suitable for:</p> <ul> <li>High throughput multi GPU training</li> <li>Full model pretraining</li> <li>Latency sensitive workloads</li> </ul>"},{"location":"optimization/paged_adam/#9-one-sentence-interview-summary","title":"9. One Sentence Interview Summary","text":"<p>Paged Adam is a memory efficient variant of Adam that keeps optimizer states in CPU memory and pages them into GPU memory only during the update step, trading training throughput for the ability to fine tune models that would not otherwise fit in GPU memory.</p>"},{"location":"optimization/prefix_caching/","title":"Prefix Caching","text":""},{"location":"optimization/prefix_caching/#paged-attention","title":"Paged Attention","text":""},{"location":"optimization/training_optimization_and_stability/","title":"Training Optimization and Stability","text":"<p>Training large neural networks, especially Transformers, requires careful choices of optimizers, learning rate schedules, and numerical techniques to ensure fast convergence, stable training, and good generalization. This section covers the most important concepts commonly discussed in interviews and used in practice.</p>"},{"location":"optimization/training_optimization_and_stability/#1-optimizers-and-schedules","title":"1. Optimizers and Schedules","text":""},{"location":"optimization/training_optimization_and_stability/#11-adam-vs-adamw","title":"1.1 Adam vs AdamW","text":""},{"location":"optimization/training_optimization_and_stability/#adam-optimizer","title":"Adam Optimizer","text":"<p>Adam (Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning.</p> <p>Key ideas:</p> <ul> <li>Maintains an exponential moving average of gradients (first moment)</li> <li>Maintains an exponential moving average of squared gradients (second moment)</li> <li>Uses bias correction for both moments</li> <li>Adapts the learning rate per parameter</li> </ul> <p>Update rule (simplified):</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] <p>Where:</p> <ul> <li>\\(\\theta_t\\): Model parameters at training step \\(t\\).</li> <li>\\(\\theta_{t+1}\\): Updated model parameters after applying one optimization step.</li> <li>\\(\\eta\\) (Learning Rate): Global step size that controls how large the parameter update is.</li> <li>\\(\\hat{m}_t\\) (Bias Corrected First Moment): Exponentially decayed moving average of past gradients, corrected for initialization bias.   Represents the estimated mean of the gradients.   $$   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}   $$</li> <li>\\(\\hat{v}_t\\) (Bias Corrected Second Moment): Exponentially decayed moving average of squared gradients, corrected for initialization bias.   Represents the estimated variance of the gradients.   $$   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}   $$</li> <li>\\(\\beta_1\\): Exponential decay rate for the first moment estimate.   Typical value: 0.9</li> <li>\\(\\beta_2\\): Exponential decay rate for the second moment estimate.   Typical value: 0.999</li> <li>\\(\\epsilon\\): Small constant added for numerical stability to prevent division by zero.   Typical value: \\(10^{-8}\\)</li> </ul> <p>Advantages:</p> <ul> <li>Fast convergence</li> <li>Works well with sparse gradients</li> <li>Requires minimal tuning compared to SGD</li> </ul> <p>Limitations:</p> <ul> <li>Implicitly couples L2 regularization with adaptive learning rates</li> <li>Often leads to worse generalization compared to SGD or AdamW in large models</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#adamw-optimizer","title":"AdamW Optimizer","text":"<p>AdamW decouples weight decay from the gradient-based update.</p> <p>Key difference from Adam:</p> <ul> <li>Weight decay is applied directly to parameters, not via the gradient</li> </ul> <p>Update rule (conceptually):</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\lambda \\theta_t \\] <p>where:</p> <ul> <li>\\(\\lambda\\) (Weight Decay Coefficient): Controls the strength of weight decay regularization.   Larger values enforce stronger penalization of large parameter magnitudes.</li> <li>\\(- \\eta \\lambda \\theta_t\\) (Decoupled Weight Decay Term): Applies weight decay directly to the parameters, independent of the gradient-based update.   This ensures consistent regularization regardless of adaptive learning rates.</li> </ul> <p>Why AdamW matters:</p> <ul> <li>Correctly implements weight decay as regularization</li> <li>Prevents adaptive learning rates from weakening regularization</li> <li>Standard optimizer for modern Transformers (BERT, GPT, ViT)</li> </ul> <p>Note: AdamW is almost always preferred over Adam for large-scale Transformer training.</p>"},{"location":"optimization/training_optimization_and_stability/#12-learning-rate-warmup-and-decay","title":"1.2 Learning Rate Warmup and Decay","text":""},{"location":"optimization/training_optimization_and_stability/#learning-rate-warmup","title":"Learning Rate Warmup","text":"<p>What it is:</p> <ul> <li>Gradually increases the learning rate from a small value to the target value over the first few training steps</li> </ul> <p>Why it is needed:</p> <ul> <li>Large models have unstable gradients at initialization</li> <li>Prevents divergence caused by large updates early in training</li> <li>Especially critical for Transformers and mixed precision training</li> </ul> <p>Common strategies:</p> <ul> <li>Linear warmup</li> <li>Warmup for a fixed number of steps (e.g., 1k to 10k steps)</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#learning-rate-decay","title":"Learning Rate Decay","text":"<p>After warmup, the learning rate is gradually reduced to improve convergence.</p> <p>Common decay schedules:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Step decay</li> <li>Polynomial decay</li> </ul> <p>Cosine decay (widely used):</p> <ul> <li>Smoothly reduces learning rate</li> <li>Avoids sudden drops that can destabilize training</li> </ul> <p>Insight: Warmup handles early instability, decay improves late-stage convergence and generalization.</p>"},{"location":"optimization/training_optimization_and_stability/#13-weight-decay-and-regularization","title":"1.3 Weight Decay and Regularization","text":""},{"location":"optimization/training_optimization_and_stability/#weight-decay","title":"Weight Decay","text":"<ul> <li>Penalizes large weights to prevent overfitting</li> </ul> <p>Important distinction:</p> <ul> <li>L2 regularization modifies the loss</li> <li>Weight decay directly modifies the parameter update</li> </ul> <p>Why decoupling matters:</p> <ul> <li>With adaptive optimizers, L2 regularization is not equivalent to weight decay</li> <li>AdamW fixes this mismatch</li> </ul> Explanation: L2 regularization is not equivalent to weight decay  1. L2 Regularization: It adds a penalty term to the loss function:  $$ \\mathcal{L}' = \\mathcal{L} + \\frac{\\lambda}{2} \\|\\theta\\|^2 $$  Taking the gradient:  $$ \\nabla_\\theta \\mathcal{L}' = \\nabla_\\theta \\mathcal{L} + \\lambda \\theta $$  So the optimizer update becomes:  $$ \\theta_{t+1} = \\theta_t - \\eta \\left( \\nabla_\\theta \\mathcal{L} + \\lambda \\theta_t \\right) $$  This means regularization is applied through the gradient.   2. Why This Works for SGD   For SGD, the update is:  $$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L} - \\eta \\lambda \\theta_t $$  Here, L2 regularization is mathematically equivalent to weight decay because:  - All parameters use the same learning rate  - No per parameter scaling is applied   So both methods shrink weights uniformly.   3. What Breaks with Adam   Adam modifies the update using adaptive, per parameter learning rates:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  When L2 regularization is added, the penalty term $\\lambda \\theta_t$ is also scaled by the adaptive denominator:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t + \\lambda \\theta_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  Consequence   - Parameters with large gradient variance receive less regularization - Parameters with small gradient variance receive more regularization - Regularization strength becomes parameter dependent and inconsistent  This is not true weight decay.  4. What Weight Decay Actually Means  True weight decay directly shrinks parameters independently of gradients:  $$ \\theta_{t+1} = (1 - \\eta \\lambda)\\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  Key properties:  - Uniform shrinkage across parameters - Independent of gradient statistics - Matches the intended regularization behavior  5. How AdamW Fixes the Problem   AdamW explicitly decouples weight decay from the gradient update:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\lambda \\theta_t $$   6. Why this works   - Weight decay is applied directly to parameters - Adaptive learning rates affect only the gradient update - Regularization strength remains consistent"},{"location":"optimization/training_optimization_and_stability/#intuition-summary","title":"Intuition Summary","text":"Method How Regularization Is Applied Problem SGD + L2 Uniform parameter shrinkage None Adam + L2 Scaled by adaptive learning rates Inconsistent regularization AdamW Direct parameter decay Correct behavior"},{"location":"optimization/training_optimization_and_stability/#other-regularization-techniques","title":"Other Regularization Techniques","text":"<ul> <li>Dropout</li> <li>Label smoothing</li> <li>Data augmentation</li> <li>Early stopping</li> </ul> <p>Transformer-specific note:</p> <ul> <li>Biases and LayerNorm parameters often exclude weight decay</li> <li>This is a common best practice in large models</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#2-numerical-stability","title":"2 Numerical Stability","text":""},{"location":"optimization/training_optimization_and_stability/#21-fp16-vs-bf16","title":"2.1 FP16 vs BF16","text":""},{"location":"optimization/training_optimization_and_stability/#fp16-half-precision","title":"FP16 (Half Precision)","text":"<p>Characteristics:</p> <ul> <li>16-bit floating point</li> <li>Limited exponent range</li> <li>Higher risk of overflow and underflow</li> </ul> <p>Challenges:</p> <ul> <li>Gradient underflow for small values</li> <li>Requires loss scaling for stability</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#bf16-brain-floating-point","title":"BF16 (Brain Floating Point)","text":"<p>Characteristics:</p> <ul> <li>16-bit floating point with larger exponent range</li> <li>Same exponent range as FP32</li> <li>Lower mantissa precision than FP16</li> </ul> <p>Advantages:</p> <ul> <li>Much more numerically stable than FP16</li> <li>Usually does not require loss scaling</li> <li>Widely supported on TPUs and newer GPUs</li> </ul> <p>Insight: BF16 is preferred when hardware supports it due to better stability with minimal complexity.</p>"},{"location":"optimization/training_optimization_and_stability/#22-mixed-precision-training","title":"2.2 Mixed Precision Training","text":"<p>This technique uses both FP16/BF16 and FP32 to get the \"best of both worlds\": the speed of 16-bit and the accuracy of 32-bit.</p> <ul> <li>Forward Pass: Done in FP16 for speed.</li> <li>Loss Scaling: Since FP16 has a narrow range, gradients can \"underflow\" (become zero). We multiply the loss by a large scale factor to push gradients into a representable range.</li> <li>Master Weights: A copy of the weights is kept in FP32. The gradients are converted to FP32 to update these master weights, ensuring precision isn't lost over time.</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#benefits","title":"Benefits","text":"<ul> <li>Faster training</li> <li>Lower memory usage</li> <li>Enables larger batch sizes and models</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#loss-scaling","title":"Loss Scaling","text":"<p>Why it is needed (mainly for FP16):</p> <ul> <li>Prevents gradients from underflowing to zero</li> </ul> <p>How it works:</p> <ul> <li>Multiply loss by a scale factor before backprop</li> <li>Divide gradients by the same factor before optimizer step</li> </ul> <p>Dynamic loss scaling:</p> <ul> <li>Automatically adjusts scale based on overflow detection</li> <li>Common in frameworks like PyTorch AMP</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#23-gradient-clipping","title":"2.3 Gradient Clipping","text":""},{"location":"optimization/training_optimization_and_stability/#what-is-gradient-clipping","title":"What is Gradient Clipping?","text":"<p>To prevent \"Exploding Gradients\" (where a large update ruins the model weights), we cap the gradients.</p> <ul> <li>Value Clipping: Caps each element of the gradient at a min/max.</li> <li>Norm Clipping: Scales the entire gradient vector so its \\(L_2\\) norm does not exceed a threshold. This preserves the direction of the gradient while limiting the magnitude.</li> </ul> <p>Norm Clipping is more common.</p> \\[ g \\leftarrow g \\cdot \\min\\left(1, \\frac{\\tau}{\\|g\\|}\\right) \\] <p>where \\(\\tau\\) is the clipping threshold.</p>"},{"location":"optimization/training_optimization_and_stability/#why-it-matters","title":"Why it matters","text":"<ul> <li>Prevents exploding gradients</li> <li>Stabilizes training in deep or recurrent models</li> <li>Especially important for large learning rates or noisy gradients</li> </ul> <p>Typical values:</p> <ul> <li>Global norm between 0.5 and 1.0 for Transformers</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#24-loss-spikes-and-divergence-diagnosis","title":"2.4 Loss Spikes and Divergence Diagnosis","text":""},{"location":"optimization/training_optimization_and_stability/#common-causes-of-loss-spikes","title":"Common Causes of Loss Spikes","text":"<ul> <li>Learning rate too high</li> <li>Insufficient warmup</li> <li>Numerical overflow in FP16</li> <li>Poor initialization</li> <li>Data outliers or corrupted batches</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#diagnosis-checklist","title":"Diagnosis Checklist","text":"<ul> <li>Check learning rate schedule and warmup length</li> <li>Monitor gradient norms</li> <li>Enable gradient clipping</li> <li>Inspect loss scaling behavior</li> <li>Compare FP16 vs BF16 runs</li> <li>Verify data preprocessing and labels</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#practical-debugging-tips","title":"Practical Debugging Tips","text":"<ul> <li>Reduce learning rate and re-run</li> <li>Increase warmup steps</li> <li>Switch from FP16 to BF16 if possible</li> <li>Enable anomaly detection for NaNs and Infs</li> <li>Log per-layer gradient norms</li> </ul>"},{"location":"optimization/training_optimization_and_stability/#summary","title":"Summary","text":"<ul> <li>AdamW is the default optimizer for modern large models</li> <li>Learning rate warmup is critical for early stability</li> <li>Weight decay must be decoupled from adaptive updates</li> <li>BF16 offers better numerical stability than FP16</li> <li>Mixed precision improves efficiency but requires care</li> <li>Gradient clipping and monitoring are essential debugging tools</li> </ul> <p>These concepts form the backbone of stable and efficient training for large-scale neural networks and are frequently tested in machine learning interviews.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/","title":"\ud83e\udde9 LORA: Low-Rank Adaptation","text":""},{"location":"parameter_efficient_fine_tuning/adaptors/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) contain billions of parameters, making full fine-tuning computationally expensive and memory intensive.  </p> <p>Low-Rank Adaptation (LoRA) provides a parameter-efficient way to adapt pretrained models by freezing the original weights and introducing small trainable low-rank update matrices.  </p> <p>LoRA decomposes weight updates into a low-rank factorization, allowing fine-tuning with only a fraction of the original parameters while retaining model quality.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model requires adjusting all parameters, which can be:</p> <ul> <li>Expensive \u2014 requires large GPU memory and long training time.</li> <li>Inefficient \u2014 multiple downstream tasks need separate full fine-tunes.</li> <li>Redundant \u2014 many weight updates lie in a low intrinsic dimension subspace.</li> </ul> <p>LoRA aims to address these issues by restricting weight updates to a low-rank subspace.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/#3-core-idea","title":"3. Core Idea","text":"<p>Let \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) be a pretrained weight matrix of a layer (e.g., in attention or MLP). In full fine-tuning, the model learns a weight update \\(\\Delta W\\), resulting in:</p> \\[ W = W_0 + \\Delta W \\] <p>LoRA assumes \\(\\Delta W\\) is low-rank and can be decomposed as:</p> \\[ \\Delta W = B A \\] <p>where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{r \\times k}\\)</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\)</li> <li>\\(r \\ll \\min(d, k)\\) is the rank hyperparameter.</li> </ul> <p>During fine-tuning:</p> <ul> <li>\\(W_0\\) is frozen (no gradient updates).</li> <li>Only \\(A\\) and \\(B\\) are trainable.</li> </ul> <p>At inference, the effective weight is:</p> \\[ W_{\\text{eff}} = W_0 + \\frac{\\alpha}{r} B A \\] <p>where \\(\\alpha\\) is a scaling factor controlling the magnitude of updates.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/#4-lora-in-attention-layers","title":"4. LoRA in Attention Layers","text":"<p>In Transformer architectures, LoRA is typically applied to query (Q) and value (V) projection matrices within the self-attention module.</p> <p>For example, the modified query projection becomes:</p> \\[ h = (W_Q + \\Delta W_Q) x = W_Q x + B_Q A_Q x \\] <p>This retains the original computation while enabling efficient adaptation with small additional matrices.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/#5-objective-function","title":"5. Objective Function","text":"<p>LoRA uses the same loss function as the base fine-tuning objective (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_{t} \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>The only difference is that only the parameters in \\( A \\) and \\( B \\) are updated:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial W_0} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial A}, \\frac{\\partial \\mathcal{L}}{\\partial B} \\neq 0 \\] <p>This selective gradient flow drastically reduces training cost and memory footprint.</p>"},{"location":"parameter_efficient_fine_tuning/adaptors/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_dim, out_dim, r=8, alpha=16):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n        self.scaling = self.alpha / self.r\n\n        self.weight = nn.Parameter(torch.empty(out_dim, in_dim))\n        self.A = nn.Parameter(torch.empty(r, in_dim))\n        self.B = nn.Parameter(torch.empty(out_dim, r))\n\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n        self.weight.requires_grad = False  # Freeze base weights\n\n    def forward(self, x):\n        return F.linear(x, self.weight + self.scaling * self.B @ self.A)\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/adaptors/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Rank (r) 4 \u2013 64 (sometimes up to 256) Start small (4/8/16) and increase if underfitting Alpha (\u03b1) \u2248 2 \u00d7 r Scaling factor: <code>scaling = \u03b1 / r</code> Learning Rate 1e-4 \u2013 5e-4 Too high \u2192 drift; too low \u2192 slow adaptation Dropout (<code>lora_dropout</code>) 0.0 \u2013 0.1 0.05 often helpful on small datasets Epochs 1 \u2013 few Avoid many epochs on small instruction datasets"},{"location":"parameter_efficient_fine_tuning/adaptors/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision: Use <code>fp16</code> or <code>bf16</code> to reduce memory usage and speed up training.  </li> <li>Gradient accumulation: Emulate large batch sizes using smaller per-device batches.  </li> <li>Gradient checkpointing: Trade compute for reduced activation memory footprint.  </li> <li>CPU offload / <code>device_map</code>: Offload frozen weights using the <code>accelerate</code> or Hugging Face <code>device_map</code> feature.  </li> <li>Optimizer: <code>AdamW</code> is the default; for very large adapter parameter sets, consider memory-efficient optimizers or even <code>SGD</code> if appropriate.  </li> <li>QLoRA: Load the base model in 4-bit precision using <code>bitsandbytes</code>, and train LoRA adapters \u2014 enables single-GPU training for very large models.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"parameter_efficient_fine_tuning/adaptors/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Lower <code>rank (r)</code>.  </li> <li>Use QLoRA (4-bit) or mixed precision.  </li> <li>Reduce batch size and use gradient accumulation.  </li> <li>Enable gradient checkpointing or CPU offload.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#training-instability-divergence","title":"\u26a1 Training Instability / Divergence","text":"<ul> <li>Lower <code>learning rate</code> and/or <code>\u03b1</code>.  </li> <li>Add a small LoRA dropout.  </li> <li>Use warmup and learning rate schedulers (e.g., cosine or linear).  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#underfitting-insufficient-capacity","title":"\ud83e\udeab Underfitting (Insufficient Capacity)","text":"<ul> <li>Gradually increase rank (r).  </li> <li>Add adapters to more modules (e.g., MLP layers).  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs and learning rate.  </li> <li>Add dropout and data augmentation.  </li> <li>Use early stopping and validation checks.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#quantization-compatibility-issues","title":"\u2699\ufe0f Quantization Compatibility Issues","text":"<ul> <li>Prefer tested stacks: <code>bitsandbytes</code> + Hugging Face + <code>peft</code>.  </li> <li>Validate numeric stability on a small subset before full training.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#adapter-conflicts-when-stacking","title":"\ud83d\udd17 Adapter Conflicts When Stacking","text":"<ul> <li>Avoid overlapping target modules unless intentionally merging adapters.  </li> <li>Use explicit adapter fusion tools when combining multiple adapters.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with small rank <code>r = 4\u201316</code> and <code>\u03b1 = 2 \u00d7 r</code>.  </li> <li>Freeze base model weights; train only adapter parameters.  </li> <li>Use mixed precision and gradient checkpointing where appropriate.  </li> <li>Use PEFT / Hugging Face tooling for reliable save/load and metadata management.  </li> <li>Monitor validation metrics and KL-like drift metrics (compare outputs to base).  </li> <li>If memory constrained, use QLoRA + LoRA adapters.  </li> <li>Keep logs, seeds, and repeat runs for reproducibility.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Rank\u2013Capacity Tradeoff: Small <code>r</code> may underfit; large <code>r</code> increases memory use and instability.  </li> <li>Task-Specific Sensitivity: Optimal values for <code>r</code>, <code>\u03b1</code>, and learning rate vary across models and tasks.  </li> <li>Quantization Effects: Combining LoRA with quantization (as in QLoRA) requires additional tuning.  </li> <li>Adapter Management: Multiple adapters need clear naming and metadata to avoid conflicts.  </li> <li>Not a Universal Replacement: For extreme distribution shifts, full fine-tuning may still be necessary.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/adaptors/#12-comparison-lora-vs-other-methods","title":"12. Comparison: LoRA vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prompt vectors LoRA \u2705 Low High Mergeable, simple low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"parameter_efficient_fine_tuning/lora/","title":"\ud83e\udde9 LORA: Low-Rank Adaptation","text":""},{"location":"parameter_efficient_fine_tuning/lora/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) contain billions of parameters, making full fine-tuning computationally expensive and memory intensive.  </p> <p>Low-Rank Adaptation (LoRA) provides a parameter-efficient way to adapt pretrained models by freezing the original weights and introducing small trainable low-rank update matrices.  </p> <p>LoRA decomposes weight updates into a low-rank factorization, allowing fine-tuning with only a fraction of the original parameters while retaining model quality.</p>"},{"location":"parameter_efficient_fine_tuning/lora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model requires adjusting all parameters, which can be:</p> <ul> <li>Expensive \u2014 requires large GPU memory and long training time.</li> <li>Inefficient \u2014 multiple downstream tasks need separate full fine-tunes.</li> <li>Redundant \u2014 many weight updates lie in a low intrinsic dimension subspace.</li> </ul> <p>LoRA aims to address these issues by restricting weight updates to a low-rank subspace.</p>"},{"location":"parameter_efficient_fine_tuning/lora/#3-core-idea","title":"3. Core Idea","text":"<p>Let \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) be a pretrained weight matrix of a layer (e.g., in attention or MLP). In full fine-tuning, the model learns a weight update \\(\\Delta W\\), resulting in:</p> \\[ W = W_0 + \\Delta W \\] <p>LoRA assumes \\(\\Delta W\\) is low-rank and can be decomposed as:</p> \\[ \\Delta W = B A \\] <p>where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{r \\times k}\\)</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\)</li> <li>\\(r \\ll \\min(d, k)\\) is the rank hyperparameter.</li> </ul> <p>During fine-tuning:</p> <ul> <li>\\(W_0\\) is frozen (no gradient updates).</li> <li>Only \\(A\\) and \\(B\\) are trainable.</li> </ul> <p>At inference, the effective weight is:</p> \\[ W_{\\text{eff}} = W_0 + \\frac{\\alpha}{r} B A \\] <p>where \\(\\alpha\\) is a scaling factor controlling the magnitude of updates.</p>"},{"location":"parameter_efficient_fine_tuning/lora/#4-lora-in-attention-layers","title":"4. LoRA in Attention Layers","text":"<p>In Transformer architectures, LoRA is typically applied to query (Q) and value (V) projection matrices within the self-attention module.</p> <p>For example, the modified query projection becomes:</p> \\[ h = (W_Q + \\Delta W_Q) x = W_Q x + B_Q A_Q x \\] <p>This retains the original computation while enabling efficient adaptation with small additional matrices.</p>"},{"location":"parameter_efficient_fine_tuning/lora/#5-objective-function","title":"5. Objective Function","text":"<p>LoRA uses the same loss function as the base fine-tuning objective (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_{t} \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>The only difference is that only the parameters in \\( A \\) and \\( B \\) are updated:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial W_0} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial A}, \\frac{\\partial \\mathcal{L}}{\\partial B} \\neq 0 \\] <p>This selective gradient flow drastically reduces training cost and memory footprint.</p>"},{"location":"parameter_efficient_fine_tuning/lora/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_dim, out_dim, r=8, alpha=16):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n        self.scaling = self.alpha / self.r\n\n        self.weight = nn.Parameter(torch.empty(out_dim, in_dim))\n        self.A = nn.Parameter(torch.empty(r, in_dim))\n        self.B = nn.Parameter(torch.empty(out_dim, r))\n\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n        self.weight.requires_grad = False  # Freeze base weights\n\n    def forward(self, x):\n        return F.linear(x, self.weight + self.scaling * self.B @ self.A)\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/lora/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Rank (r) 4 \u2013 64 (sometimes up to 256) Start small (4/8/16) and increase if underfitting Alpha (\u03b1) \u2248 2 \u00d7 r Scaling factor: <code>scaling = \u03b1 / r</code> Learning Rate 1e-4 \u2013 5e-4 Too high \u2192 drift; too low \u2192 slow adaptation Dropout (<code>lora_dropout</code>) 0.0 \u2013 0.1 0.05 often helpful on small datasets Epochs 1 \u2013 few Avoid many epochs on small instruction datasets"},{"location":"parameter_efficient_fine_tuning/lora/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision: Use <code>fp16</code> or <code>bf16</code> to reduce memory usage and speed up training.  </li> <li>Gradient accumulation: Emulate large batch sizes using smaller per-device batches.  </li> <li>Gradient checkpointing: Trade compute for reduced activation memory footprint.  </li> <li>CPU offload / <code>device_map</code>: Offload frozen weights using the <code>accelerate</code> or Hugging Face <code>device_map</code> feature.  </li> <li>Optimizer: <code>AdamW</code> is the default; for very large adapter parameter sets, consider memory-efficient optimizers or even <code>SGD</code> if appropriate.  </li> <li>QLoRA: Load the base model in 4-bit precision using <code>bitsandbytes</code>, and train LoRA adapters \u2014 enables single-GPU training for very large models.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"parameter_efficient_fine_tuning/lora/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Lower <code>rank (r)</code>.  </li> <li>Use QLoRA (4-bit) or mixed precision.  </li> <li>Reduce batch size and use gradient accumulation.  </li> <li>Enable gradient checkpointing or CPU offload.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#training-instability-divergence","title":"\u26a1 Training Instability / Divergence","text":"<ul> <li>Lower <code>learning rate</code> and/or <code>\u03b1</code>.  </li> <li>Add a small LoRA dropout.  </li> <li>Use warmup and learning rate schedulers (e.g., cosine or linear).  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#underfitting-insufficient-capacity","title":"\ud83e\udeab Underfitting (Insufficient Capacity)","text":"<ul> <li>Gradually increase rank (r).  </li> <li>Add adapters to more modules (e.g., MLP layers).  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs and learning rate.  </li> <li>Add dropout and data augmentation.  </li> <li>Use early stopping and validation checks.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#quantization-compatibility-issues","title":"\u2699\ufe0f Quantization Compatibility Issues","text":"<ul> <li>Prefer tested stacks: <code>bitsandbytes</code> + Hugging Face + <code>peft</code>.  </li> <li>Validate numeric stability on a small subset before full training.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#adapter-conflicts-when-stacking","title":"\ud83d\udd17 Adapter Conflicts When Stacking","text":"<ul> <li>Avoid overlapping target modules unless intentionally merging adapters.  </li> <li>Use explicit adapter fusion tools when combining multiple adapters.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with small rank <code>r = 4\u201316</code> and <code>\u03b1 = 2 \u00d7 r</code>.  </li> <li>Freeze base model weights; train only adapter parameters.  </li> <li>Use mixed precision and gradient checkpointing where appropriate.  </li> <li>Use PEFT / Hugging Face tooling for reliable save/load and metadata management.  </li> <li>Monitor validation metrics and KL-like drift metrics (compare outputs to base).  </li> <li>If memory constrained, use QLoRA + LoRA adapters.  </li> <li>Keep logs, seeds, and repeat runs for reproducibility.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Rank\u2013Capacity Tradeoff: Small <code>r</code> may underfit; large <code>r</code> increases memory use and instability.  </li> <li>Task-Specific Sensitivity: Optimal values for <code>r</code>, <code>\u03b1</code>, and learning rate vary across models and tasks.  </li> <li>Quantization Effects: Combining LoRA with quantization (as in QLoRA) requires additional tuning.  </li> <li>Adapter Management: Multiple adapters need clear naming and metadata to avoid conflicts.  </li> <li>Not a Universal Replacement: For extreme distribution shifts, full fine-tuning may still be necessary.  </li> </ul>"},{"location":"parameter_efficient_fine_tuning/lora/#12-comparison-lora-vs-other-methods","title":"12. Comparison: LoRA vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prompt vectors LoRA \u2705 Low High Mergeable, simple low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/","title":"\ud83e\udde9 Prefix Tuning","text":""},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) have billions of parameters, making full fine-tuning computationally expensive and memory intensive.</p> <p>Prefix Tuning provides a parameter-efficient method to adapt pretrained models by keeping the model weights frozen and prepending trainable continuous vectors (\u201cprefixes\u201d) to the input of each Transformer layer.</p> <p>The key idea is to learn task-specific prompts in the hidden space rather than modifying the model weights directly, allowing small memory footprint fine-tuning.</p>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model fully is often:</p> <ul> <li>Expensive \u2014 requires high GPU memory and long training cycles.</li> <li>Inefficient \u2014 multiple tasks need separate full fine-tunes.</li> <li>Redundant \u2014 only a small portion of the model\u2019s hidden space needs adaptation for many tasks.</li> </ul> <p>Prefix tuning addresses this by modifying the activations at each layer via trainable prefix vectors, keeping all original model parameters frozen.</p>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#3-core-idea","title":"3. Core Idea","text":"<p>Let a Transformer layer have hidden states \\(h \\in \\mathbb{R}^{L \\times d}\\), where:</p> <ul> <li>\\(L\\) is the sequence length</li> <li>\\(d\\) is the hidden dimension</li> </ul> <p>Prefix tuning introduces learnable prefix vectors \\(P \\in \\mathbb{R}^{L_p \\times d}\\) (with \\(L_p \\ll L\\)), prepended to the key and value projections of each attention layer:</p> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <p>where:</p> <ul> <li>\\(P_K, P_V \\in \\mathbb{R}^{L_p \\times d_k}\\) are trainable prefixes</li> <li>\\(K, V\\) are original key and value matrices</li> <li>\\([;]\\) denotes concatenation along the sequence dimension</li> </ul> <p>During training:</p> <ul> <li>Original model weights are frozen.</li> <li>Only the prefix vectors \\(P\\) are updated.</li> </ul> <p>At inference, the model uses:</p> \\[ \\text{Attention}(Q, \\tilde{K}, \\tilde{V}) \\] <p>allowing adaptation without modifying the model weights.</p>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#4-prefix-tuning-in-attention-layers","title":"4. Prefix Tuning in Attention Layers","text":"<p>In self-attention, standard attention is computed as:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V \\] <p>With prefix tuning:</p> <ul> <li>\\(K\\) and \\(V\\) are augmented with trainable prefix vectors:</li> </ul> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <ul> <li>This injects task-specific context without changing \\(Q\\), \\(K\\), \\(V\\) weights.</li> <li>The model effectively learns a small task embedding that guides attention.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#5-objective-function","title":"5. Objective Function","text":"<p>Prefix tuning uses the same loss as standard fine-tuning (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_t \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>Gradients only flow through the prefix vectors:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\text{model weights}} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial P} \\neq 0 \\] <p>This drastically reduces memory usage while maintaining strong task performance.</p>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class PrefixTuning(nn.Module):\n    def __init__(self, hidden_size, prefix_length=10, num_layers=12):\n        super().__init__()\n        self.prefix_length = prefix_length\n        self.num_layers = num_layers\n        # Trainable prefix vectors per layer\n        self.prefix_keys = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n        self.prefix_values = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, layer_idx, K, V):\n        # Prepend trainable prefixes\n        P_K = self.prefix_keys[layer_idx]\n        P_V = self.prefix_values[layer_idx]\n        K_aug = torch.cat([P_K, K], dim=0)\n        V_aug = torch.cat([P_V, V], dim=0)\n        return K_aug, V_aug\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Prefix length (L_p) 5 \u2013 50 Longer prefixes for complex tasks Hidden size Match model hidden size Usually same as Transformer hidden dimension Learning Rate 1e-4 \u2013 5e-4 Small LR helps stable training Dropout 0.0 \u2013 0.1 Helps prevent overfitting Epochs 1 \u2013 few Avoid overfitting on small datasets"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision (<code>fp16</code> / <code>bf16</code>) for memory and speed.</li> <li>Gradient checkpointing to save activation memory.</li> <li>CPU offload for frozen model weights using <code>accelerate</code> or <code>device_map</code>.</li> <li>Optimizer: <code>AdamW</code> or memory-efficient optimizers for prefix parameters.</li> <li>Single-GPU friendly: Prefix tuning only trains a small number of parameters.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Prefix tuning is already lightweight; reduce prefix length if necessary.</li> <li>Use mixed precision and gradient checkpointing.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#training-instability","title":"\u26a1 Training Instability","text":"<ul> <li>Reduce learning rate.</li> <li>Add dropout to prefixes.</li> <li>Use learning rate schedulers or warmup.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#underfitting","title":"\ud83e\udeab Underfitting","text":"<ul> <li>Increase prefix length.</li> <li>Add prefixes to more layers.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs.</li> <li>Add dropout or early stopping.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with prefix length \\(L_p\\) = 10\u201320.</li> <li>Freeze base model weights; train only prefix vectors.</li> <li>Use mixed precision and gradient checkpointing where needed.</li> <li>Log validation metrics and monitor task performance drift.</li> <li>For large models, prefix tuning can be combined with LoRA or QLoRA for stronger adaptation.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Prefix length sensitivity: Too short \u2192 underfitting; too long \u2192 memory overhead.</li> <li>Layer selection: Optimal layers for prefix insertion may vary.</li> <li>Task generalization: Prefixes are task-specific; transferring to new tasks may require re-training.</li> <li>Not suitable for extreme distribution shifts: Full fine-tuning may still be needed.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/prefix_tuning/#12-comparison-prefix-tuning-vs-other-methods","title":"12. Comparison: Prefix Tuning vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prefix vectors prepended to attention LoRA \u2705 Low High Mergeable, low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"parameter_efficient_fine_tuning/qlora/","title":"\ud83e\udde9 QLoRA: Quantized Low-Rank Adaptation","text":""},{"location":"parameter_efficient_fine_tuning/qlora/#1-overview","title":"1. Overview","text":"<p>QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large pre-trained LLMs to downstream tasks without modifying all the model weights.</p> <p>It achieves this by combining 4-bit quantization (using NormalFloat-4, or NF4) with Low-Rank Adaptation (LoRA), enabling fine-tuning of massive models (e.g., 65B parameters) on a single 48 GB GPU - with performance close to full fine-tuning.</p>"},{"location":"parameter_efficient_fine_tuning/qlora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning large LLMs poses major computational and memory challenges. QLoRA addresses these by:</p> <ul> <li>Reducing Memory Footprint - 4-bit quantization shrinks model memory up to 75%, enabling single-GPU fine-tuning.</li> <li>Preserving Accuracy - NF4 quantization minimizes quantization error by modeling real weight distributions.</li> <li>Parameter Efficiency - Only a small number of low-rank matrices (LoRA adapters) are trained.</li> <li>Ease of Integration - Built atop Hugging Face PEFT, it fits easily into existing LLM fine-tuning workflows.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/qlora/#3-core-concepts","title":"3. Core Concepts","text":""},{"location":"parameter_efficient_fine_tuning/qlora/#31-quantization-in-qlora","title":"3.1 Quantization in QLoRA","text":"<p>The base model\u2019s parameters are quantized into 4-bit NormalFloat (NF4) values and kept frozen during fine-tuning. NF4 uses a normal-distribution-aware quantization scheme that minimizes the quantization error between original FP16 weights and 4-bit representations.</p> <p>\ud83d\udd17 Detailed explanation: NF4 Quantization: Principles and Implementation</p> <p>In addition, QLoRA leverages block quantization and double quantization to optimize memory even further:</p> <ul> <li> <p>Block Quantization: Weights are quantized in small blocks (e.g., 64 values per block) with block-specific scaling factors, balancing compression and precision.   This reduces quantization noise compared to uniform quantization.</p> </li> <li> <p>Double Quantization: Instead of storing the full-scale values for each block, these scale values are themselves quantized (typically to 8 bits).   This reduces memory overhead by ~0.37 bits per parameter on average.</p> </li> </ul> <p>\ud83d\udd17 Detailed explanation: Block &amp; Double Quantization in QLoRA</p>"},{"location":"parameter_efficient_fine_tuning/qlora/#32-low-rank-adaptation-lora","title":"3.2 Low-Rank Adaptation (LoRA)","text":"<p>LoRA introduces trainable low-rank matrices (A) and (B) into each transformer layer, approximating weight updates as:</p> \\[ \\Delta W = B A \\] <p>where \\(A \\in \\mathbb{R}^{r \\times d}\\), \\(B \\in \\mathbb{R}^{d \\times r}\\), and \\(r\\) is the rank (e.g., 8\u201316). The base weights \\(W_0\\) are frozen, and only \\(A, B\\) are trained.</p> <p>The adapted output is:</p> \\[ h = W_0 x + \\frac{\\alpha}{r} B (A x) \\] <p>where \\(\\alpha\\) is a scaling factor controlling the LoRA contribution.</p>"},{"location":"parameter_efficient_fine_tuning/qlora/#4-integrating-quantization-and-lora","title":"4. Integrating Quantization and LoRA","text":"<p>QLoRA\u2019s key innovation is the combination of 4-bit quantization with LoRA fine-tuning, enabling efficient adaptation without unfreezing or copying large models.</p> <p>Step-by-Step Process</p>"},{"location":"parameter_efficient_fine_tuning/qlora/#step-1-quantize-base-model","title":"Step-1. Quantize Base Model:","text":"<pre><code>The pretrained model weights $W_0$ are quantized once into NF4 format using `bitsandbytes`:\n$$\nW_0^{(q)} = Q_{\\text{NF4}}(W_0)\n$$\n\n* Quantization uses per-block scaling and optional double quantization.\n* These quantized weights are **frozen** during training.\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/qlora/#step-2-dynamic-dequantization-during-forward-pass","title":"Step-2. Dynamic Dequantization During Forward Pass:","text":"<pre><code>During each forward pass, QLoRA dequantizes small blocks of weights on-the-fly:\n\n* The `bnb.nn.Linear4bit` layer from `bitsandbytes` automatically dequantizes just-in-time for computation.\n* After the matrix multiplication, the dequantized block is discarded to minimize GPU memory usage.\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/qlora/#step-3-trainable-lora-adapters-full-precision","title":"Step-3. Trainable LoRA Adapters (Full Precision):","text":"<pre><code>The LoRA adapter matrices (A) and (B) are added to target modules (e.g., query/key/value projections) and are trained in **FP16 or BF16 precision**:\n$$\n\\Delta W = B A\n$$\nThese are **not quantized**, since:\n\n* They constitute &lt;1% of total parameters.\n* Quantization would harm convergence and stability.\n* Keeping them in higher precision stabilizes gradient updates.\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/qlora/#step-4-combined-forward-pass","title":"Step-4. Combined Forward Pass:","text":"<pre><code>$$\nh = (W_0^{(dq)} + \\frac{\\alpha}{r} B A) x\n$$\n\n* $W_0^{(dq)}$: dynamically dequantized base weights.\n* $\\frac{\\alpha}{r} B A$: LoRA correction term in FP16/BF16.\n* Gradients flow only through LoRA parameters.\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/qlora/#step-5-backward-pass-updates","title":"Step-5. Backward Pass &amp; Updates:","text":"<pre><code>* Only LoRA parameters are updated during training.\n* Quantized base weights remain frozen and untouched.\n* Gradients and optimizer states are maintained in FP16/BF16 for efficiency.\n</code></pre>"},{"location":"parameter_efficient_fine_tuning/qlora/#inference-with-qlora","title":"\ud83e\udde0 Inference with QLoRA","text":"<p>During inference, QLoRA continues to leverage 4-bit quantization to ensure efficiency while maintaining accuracy:</p> <ul> <li>The base model weights remain quantized (NF4), allowing the model to run efficiently on limited GPU memory.</li> <li>The LoRA adapter weights are applied in higher precision (typically fp16 or bf16) to preserve the fine-tuned adaptations.</li> <li>During the forward pass, the quantized base weights are temporarily dequantized for computation and combined with the adapter outputs:</li> </ul> \\[ h = W_{q}x + \\frac{\\alpha}{r}B(Ax) \\] <p>where \\(W_q\\) represents the quantized base model weights.</p> <ul> <li>The majority of computation is performed on the quantized backbone, while the LoRA adapter adds a small high-precision correction.</li> <li>This hybrid setup provides a balance between memory efficiency (from quantization) and model fidelity (from LoRA adapters), enabling low-cost, high-performance inference even on large LLMs.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/qlora/#5-quantization-mechanics-summary","title":"5. Quantization Mechanics Summary","text":"Feature Description Benefit NF4 NormalFloat-4 data type; 4-bit quantization optimized for normally distributed weights Preserves accuracy Block Quantization Quantizes weights in fixed-size blocks with shared scaling Reduces quantization error Double Quantization Second quantization of scale parameters Saves additional memory Mixed Precision Training Adapters in fp16/bf16; base model in NF4 Optimal compute/memory tradeoff"},{"location":"parameter_efficient_fine_tuning/qlora/#6-precision-summary","title":"6. Precision Summary","text":"Component Quantized? Precision Trainable? Notes Base model weights (W_0) \u2705 Yes (NF4 4-bit) Dequantized on-the-fly \u274c No Frozen, quantized by bitsandbytes LoRA adapters (A, B) \u274c No FP16/BF16 \u2705 Yes Trained normally Gradients \u274c No FP16/BF16 \u2705 Yes Only for adapters Optimizer state \u274c No FP16/BF16 \u2705 Yes Small memory footprint"},{"location":"parameter_efficient_fine_tuning/qlora/#7-implementation-details","title":"7. Implementation Details","text":"<ul> <li>QLoRA uses <code>bnb.nn.Linear4bit</code> to wrap quantized linear layers.</li> <li>PEFT integrates LoRA adapters directly on top of quantized layers.</li> <li>Both components are fused during forward passes.</li> <li>During inference, the quantized base and LoRA adapters can be merged for efficient deployment.</li> </ul>"},{"location":"parameter_efficient_fine_tuning/qlora/#8-troubleshooting-guide","title":"8. Troubleshooting Guide","text":"Issue Cause Mitigation OOM / CUDA errors Batch too large / rank too high Lower <code>r</code>, enable offload/checkpointing Training instability LR too high, quant noise Lower LR or \u03b1, use LoRA dropout Underfitting Too low rank Increase <code>r</code> or apply adapters to more layers Overfitting Too high capacity Reduce epochs or use dropout Quantization mismatch NF4 calibration drift Re-quantize base model, validate small batch"},{"location":"parameter_efficient_fine_tuning/qlora/#9-comparison-lora-vs-qlora","title":"9. Comparison: LoRA vs QLoRA","text":"Method Quantized Base Trainable Params Memory Use Performance Full Fine-tuning \u274c No 100% \ud83d\udd34 High \u2705 High LoRA \u274c No &lt; 1% \ud83d\udfe0 Low \u2705 High QLoRA \u2705 4-bit (NF4) &lt; 1% \ud83d\udfe2 Very Low \u2705 Comparable"},{"location":"parameter_efficient_fine_tuning/qlora/#10-limitations-challenges","title":"10. Limitations &amp; Challenges","text":"<ul> <li>Requires accurate NF4 quantization calibration.</li> <li>Sensitive to optimizer precision and scaling.</li> <li>Not ideal for large domain shifts (may need full finetuning).</li> <li>Adapter stacking requires version management.</li> </ul>"},{"location":"phases/mid_training/","title":"Mid-Training","text":""},{"location":"phases/mid_training/#1-overview","title":"1. Overview","text":"<p>In the 2025\u20132026 development cycle, Mid-training (often called Continued Pre-training or the Annealing Phase) has solidified as a critical third pillar in the LLM lifecycle.  It sits between massive-scale general pre-training and task-specific alignment (SFT/RLHF). Its primary purpose is to transform a general-purpose foundation model into a high-reasoning, domain-specialized system while minimizing catastrophic forgetting and preserving global language competence.</p> <p>Mid-training is now viewed not only as a quality enhancer but also as a cost and stability optimization stage that significantly reduces downstream alignment complexity.</p>"},{"location":"phases/mid_training/#2-utility-and-strategic-value","title":"2. Utility and Strategic Value","text":"<p>Mid-training is no longer optional for frontier models. Its utility lies in three main areas:</p> <ul> <li> <p>Domain Deep-Diving: Injection of large-scale, high-signal domain corpora such as law, biomedical literature, mathematics, or enterprise codebases, reaching depth that is infeasible during general pre-training.</p> </li> <li> <p>Cognitive Architecture Scaling: Transitioning from surface-level pattern matching to structured reasoning by exposing the model to dense synthetic reasoning trajectories, proofs, and multi-step problem-solving traces.</p> </li> <li> <p>Architectural Adaptation: Enabling long-context reasoning, tool usage priors, and agentic behaviors that require architectural stress beyond base pre-training distributions.</p> </li> </ul>"},{"location":"phases/mid_training/#3-resource-requirements","title":"3. Resource Requirements","text":"<p>Mid-training is a \"heavyweight\" operation compared to post-training alignment.</p> Resource Requirement Note Compute 5%\u201315% of Pre-training FLOPs Requires high-interconnect GPU clusters (H100/B200). Optimizer States Full Adam or AdamW moments Must resume with original moments to avoid \"loss spikes.\" Data Quality &gt;95% Signal-to-Noise Aggressive filtering, deduplication, and curriculum ordering required. Replay Buffer 10%\u201320% General Data Essential to prevent forgetting of general knowledge."},{"location":"phases/mid_training/#4-training-objectives-and-loss-design","title":"4. Training Objectives and Loss Design","text":""},{"location":"phases/mid_training/#41-objective-continuity","title":"4.1 Objective Continuity","text":"<p>Mid-training typically preserves the original causal language modeling objective used during pre-training. Stability is achieved by changing data distribution and curriculum, not the core loss.</p>"},{"location":"phases/mid_training/#42-auxiliary-objectives","title":"4.2 Auxiliary Objectives","text":"<p>Some advanced pipelines introduce lightly weighted auxiliary losses, including:</p> <ul> <li>Contrastive losses for retrieval-aware representations</li> <li>Outcome-conditioned losses for tool-use traces</li> <li>Self-consistency rewards for reasoning trajectories</li> </ul> <p>These signals are intentionally weak to avoid destabilizing the base representation space.</p>"},{"location":"phases/mid_training/#5-technical-implementation-steps","title":"5. Technical Implementation &amp; Steps","text":""},{"location":"phases/mid_training/#step-1-data-mixture-synthesis","title":"Step 1: Data Mixture Synthesis","text":"<p>The \"recipe\" for a mid-training run is more curated than raw pre-training. A typical 2026 mixture involves: * 40% Specialist Corpora: Textbooks, technical manuals, and white papers. * 30% Synthetic Reasoning: Trajectories generated by larger \"Teacher\" models showing step-by-step logic. * 20% General Web Scrapes: High-quality \"Common Crawl\" subsets (e.g., FineWeb-Edu). * 10% Long-form Documents: Books and full codebases for context stretching.</p>"},{"location":"phases/mid_training/#step-2-lr-annealing-the-spike-and-decay-and-optimization-strategy","title":"Step 2: LR Annealing (The \"Spike and Decay\") and Optimization Strategy","text":"<p>Mid-training utilizes a unique learning rate schedule. Instead of a flat or strictly decaying line, it often employs a Cool-down/Annealing strategy:</p> <ol> <li>Re-warmup: A short period to stabilize the model on the new data distribution.</li> <li>Cosine Decay: A deep decay toward zero to ensure the weights settle into the specialized \"valleys\" of the loss landscape.</li> </ol> <p>Optimizer Sensitivity</p> <ul> <li>Full optimizer state restoration is mandatory</li> <li>Partial state resets commonly cause irrecoverable divergence</li> </ul>"},{"location":"phases/mid_training/#step-3-context-window-extension-rope-scaling","title":"Step 3: Context Window Extension (RoPE Scaling)","text":"<p>To handle long-context, the mid-training phase modifies the Rotary Positional Embeddings (RoPE). The transformation typically follows:</p> \\[f(q, i, \\theta) = R_{\\Theta, i}^d q\\] <p>By increasing the base frequency \\(\\theta\\) (often called \"Base Scaling\"), the model learns to interpret distances between tokens that were previously \"out of range.\"</p>"},{"location":"phases/mid_training/#6-parameter-freezing-and-selective-training","title":"6. Parameter Freezing and Selective Training","text":"<p>Mid-training does not always update all parameters:</p> <ul> <li>Frozen Components:    Token embeddings, early transformer blocks, or normalization statistics</li> <li>Progressive Unfreezing:    Higher layers are unfrozen as specialization increases</li> <li>Adapter-Based Mid-Training:    Low-rank or gated adapters may be trained and later merged, reducing compute cost</li> </ul> <p>Selective training reduces catastrophic forgetting and improves stability.</p>"},{"location":"phases/mid_training/#7-failure-modes-and-diagnostics","title":"7. Failure Modes and Diagnostics","text":"<p>Common Failure Modes</p> <ul> <li>Loss Spikes After Resume due to optimizer mismatch</li> <li>Reasoning Overfitting leading to brittle, verbose outputs</li> <li>Semantic Drift despite replay buffers</li> <li>Context Length Illusions masking real long-range reasoning deficits</li> </ul> <p>Diagnostics</p> <ul> <li>Perplexity on held-out general corpora</li> <li>Loss by token position bucket</li> <li>Reasoning consistency metrics</li> </ul>"},{"location":"phases/mid_training/#8-evaluation-during-mid-training","title":"8. Evaluation During Mid-Training","text":"<p>Mid-training requires continuous evaluation beyond downstream benchmarks.</p> <p>Online Metrics</p> <ul> <li>Replay buffer perplexity</li> <li>Long-context loss curves</li> <li>Reasoning trace self-consistency</li> </ul> <p>Offline Probes</p> <ul> <li>Needle-in-a-haystack retrieval tasks</li> <li>Synthetic math and code reasoning suites</li> <li>Tool-use simulation accuracy</li> </ul> <p>Early detection of regressions is critical due to high rollback cost.</p>"},{"location":"phases/mid_training/#9-impact-on-post-training-and-alignment","title":"9. Impact on Post-Training and Alignment","text":"<p>A strong mid-training phase:</p> <ul> <li>Reduces SFT data needs by 2\u20135\u00d7</li> <li>Improves RLHF stability by narrowing policy search</li> <li>Lowers reward hacking risk by pre-internalizing reasoning norms</li> </ul> <p>This has made mid-training a core efficiency lever in modern LLM pipelines.</p>"},{"location":"phases/mid_training/#10-recent-work-and-2026-breakthroughs","title":"10. Recent Work and 2026 Breakthroughs","text":"<ul> <li> <p>Agentic Synthesis (Late 2025): Including \"Action Trajectories\" (logs of AI models using tools) during mid-training improves agentic performance more than SFT alone.</p> </li> <li> <p>Internalized Reinforcement Learning (IRL): Applying RL during the mid-training phase to reward \"correct reasoning paths\" in math and code.</p> </li> <li> <p>Curriculum Mixing: Using a small \"proctor\" model to dynamically change the data mixture based on the primary model's real-time loss.</p> </li> </ul>"},{"location":"phases/mid_training/#11-comparison-of-training-phases","title":"11. Comparison of Training Phases","text":"Feature Pre-training Mid-training Post-training (SFT) Token Count 5T - 15T 100B - 500B 10M - 50M Focus Breadth Depth/Logic Behavior/Safety Data Source Raw Web Curated/Synthetic Human-labeled"},{"location":"phases/pre_training/","title":"Pre-Training","text":""},{"location":"phases/pre_training/#1-overview","title":"1. Overview","text":"<p>Pre-training defines the raw capability ceiling of a Large Language Model. Architectural choices, data quality, and scaling decisions made at this stage dominate downstream performance far more than post-training alignment or prompting tricks. Most limitations observed later are traceable to decisions made here.</p>"},{"location":"phases/pre_training/#2-objectives-and-scaling","title":"2. Objectives and Scaling","text":""},{"location":"phases/pre_training/#21-self-supervised-learning","title":"2.1 Self-Supervised Learning","text":"<p>LLMs are trained using self-supervision, where labels are derived directly from the data itself. Given a sequence of tokens:</p> \\[ x = (x_1, x_2, \\dots, x_T) \\] <p>the model learns to predict the next token:</p> \\[ P(x_t \\mid x_1, \\dots, x_{t-1}) \\] <p>This formulation:</p> <ul> <li>Requires no human annotation</li> <li>Scales naturally with data size</li> <li>Supports emergent behaviors such as reasoning and in-context learning</li> </ul> <p>Despite its simplicity, this objective implicitly captures syntax, semantics, world knowledge, and procedural patterns.</p>"},{"location":"phases/pre_training/#22-negative-log-likelihood-objective","title":"2.2 Negative Log Likelihood Objective","text":"<p>Training minimizes the Negative Log Likelihood (NLL):</p> \\[ \\mathcal{L} = - \\mathbb{E}_{x \\sim \\mathcal{D}} \\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{&lt;t}) \\] <p>Key properties:</p> <ul> <li>Equivalent to minimizing cross-entropy</li> <li>Strongly penalizes confident incorrect predictions</li> <li>Encourages calibration under idealized assumptions</li> </ul> <p>Important limitation:</p> <ul> <li>NLL optimizes average token prediction, not task success, reasoning correctness, or truthfulness.</li> </ul>"},{"location":"phases/pre_training/#23-chinchilla-scaling-laws-training-optimality","title":"2.3 Chinchilla Scaling Laws (Training Optimality)","text":"<p>The Chinchilla results (Hoffmann et al.) showed that many prior models were over-parameterized and under-trained.</p> <p>Core insight:</p> <p>For a fixed compute budget, model performance is maximized when model size and training tokens are scaled proportionally (roughly 20 tokens per parameter).</p> <p>This shifted industry practice from \"bigger models\" to \"compute-optimal training\" with massive, high-quality corpora.</p>"},{"location":"phases/pre_training/#24-inference-optimal-scaling-the-llama-paradigm","title":"2.4 Inference-Optimal Scaling (The LLaMA Paradigm)","text":""},{"location":"phases/pre_training/#241-background-why-chinchilla-is-not-the-full-story","title":"2.4.1 Background: Why Chinchilla Is Not the Full Story","text":"<p>The Chinchilla scaling laws optimize for training compute efficiency. They answer the question:</p> <p>Given a fixed training compute budget, how should we allocate it between model size and training tokens to minimize loss?</p> <p>This framing is correct for research experiments and one-off model training runs. However, it ignores a critical real-world constraint:</p> <p>Most of the cost of an LLM is paid after training, during inference.</p> <p>In production systems, a model may be:</p> <ul> <li>Trained once</li> <li>Served millions or billions of times</li> </ul> <p>This changes the optimization target entirely.</p>"},{"location":"phases/pre_training/#242-training-cost-vs-inference-cost","title":"2.4.2 Training Cost vs Inference Cost","text":"<p>It is essential to distinguish the two:</p> <p>Training cost</p> <ul> <li>Scales with: parameters \u00d7 tokens \u00d7 optimization overhead</li> <li>Paid once</li> </ul> <p>Inference cost</p> <ul> <li>Scales primarily with: number of active parameters</li> <li>Paid per request</li> <li>Dominates total cost in deployed systems</li> </ul> <p>Key insight:</p> <p>Inference cost depends on model size, not on how many tokens the model saw during training.</p> <p>This means that Chinchilla-optimal models may be cheap to train but expensive to serve.</p>"},{"location":"phases/pre_training/#243-the-shift-in-optimization-objective","title":"2.4.3 The Shift in Optimization Objective","text":"<p>This leads to a new question:</p> <p>Given a fixed inference budget or deployment footprint, how do we maximize model quality?</p> <p>This is where inference-optimal scaling emerges.</p>"},{"location":"phases/pre_training/#244-chinchilla-optimal-vs-inference-optimal","title":"2.4.4 Chinchilla-Optimal vs Inference-Optimal","text":""},{"location":"phases/pre_training/#chinchilla-optimal-scaling","title":"Chinchilla-Optimal Scaling","text":"<ul> <li>Train very large models</li> <li>Use relatively fewer training tokens</li> <li>Minimizes training loss per unit of training compute</li> </ul> <p>This results in:</p> <ul> <li>Large parameter counts</li> <li>High inference latency and cost</li> <li>Practical difficulty deploying at scale</li> </ul> <p>Modern open-weights models (e.g., LLaMA 3) are \"over-trained\" by Chinchilla standards (e.g., &gt;100x tokens per parameter) to maximize performance for a specific deployment footprint.</p>"},{"location":"phases/pre_training/#inference-optimal-scaling","title":"Inference-Optimal Scaling","text":"<ul> <li>Train smaller models</li> <li>Train them on far more tokens than Chinchilla recommends</li> <li>Accept higher training cost to reduce inference cost</li> </ul> <p>This results in:</p> <ul> <li>Fewer parameters</li> <li>Lower latency and memory usage</li> <li>Better cost-performance trade-off in production</li> </ul>"},{"location":"phases/pre_training/#245-why-over-training-smaller-models-works","title":"2.4.5 Why Over-Training Smaller Models Works","text":"<p>Smaller models are capacity-limited, not data-limited.</p> <p>By exposing them to vastly more data:</p> <ul> <li>Representations become more robust</li> <li>Rare patterns are reinforced</li> <li>Generalization improves significantly</li> </ul> <p>Even though Chinchilla would label this as \"over-training\", the additional data:</p> <ul> <li>Continues to reduce downstream error</li> <li>Improves reasoning and instruction-following</li> <li>Makes the model competitive with much larger alternatives</li> </ul>"},{"location":"phases/pre_training/#246-the-llama-paradigm","title":"2.4.6 The LLaMA Paradigm","text":"<p>Modern open-weight models such as LLaMA 2 and LLaMA 3 follow this strategy.</p> <p>Characteristics:</p> <ul> <li>Moderate parameter count</li> <li>Extremely high tokens-per-parameter ratio</li> <li>Often tens to hundreds of times more tokens per parameter than Chinchilla-optimal</li> </ul> <p>This design choice:</p> <ul> <li>Maximizes quality for a fixed inference budget</li> <li>Enables efficient deployment on limited hardware</li> <li>Makes open models more practical for real-world use</li> </ul>"},{"location":"phases/pre_training/#247-learnings","title":"2.4.7 Learnings","text":""},{"location":"phases/pre_training/#1-scaling-laws-are-objective-dependent","title":"1. Scaling Laws Are Objective-Dependent","text":"<p>There is no single \"optimal\" scaling rule. - Chinchilla optimizes training compute - Inference-optimal scaling optimizes deployment cost</p> <p>Always ask: What is the objective being optimized?</p>"},{"location":"phases/pre_training/#2-training-longer-can-be-better-than-training-bigger","title":"2. Training Longer Can Be Better Than Training Bigger","text":"<p>For production systems: - Smaller, heavily trained models often outperform - Larger, lightly trained models are harder to serve</p> <p>This reverses earlier intuitions from the pre-Chinchilla era.</p>"},{"location":"phases/pre_training/#3-tokens-are-cheaper-than-parameters-at-inference-time","title":"3. Tokens Are Cheaper Than Parameters at Inference Time","text":"<ul> <li>Extra training tokens increase one-time cost</li> <li>Extra parameters increase recurring cost</li> </ul> <p>In large-scale deployments, recurring costs dominate.</p>"},{"location":"phases/pre_training/#4-model-design-is-a-systems-problem","title":"4. Model Design Is a Systems Problem","text":"<p>Model size, data scale, and deployment constraints are tightly coupled. Good LLM design requires: - ML theory - Systems thinking - Cost-aware decision making</p>"},{"location":"phases/pre_training/#248-compute-vs-data-vs-parameters","title":"2.4.8 Compute vs Data vs Parameters","text":"<p>Pre-training is a three-way trade-off:</p> Resource Bottleneck Effect Parameters Capacity ceiling Data Generalization and robustness Compute Training duration and batch size <p>Common failure modes:</p> <ul> <li>Too many parameters leads to memorization</li> <li>Too little data leads to brittle generalization</li> <li>Insufficient compute leads to undertrained representations</li> </ul> <p>Modern frontier models are primarily data-limited, not architecture-limited.</p>"},{"location":"phases/pre_training/#3-data-pipeline-and-quality","title":"3. Data Pipeline and Quality","text":"<p>Data quality is often more important than model size. \"Garbage in, garbage out\" applies strictly to LLMs.</p>"},{"location":"phases/pre_training/#31-raw-data-sources","title":"3.1 Raw Data Sources","text":"<p>Typical pre-training corpora include:</p> <ul> <li>Web crawl data (CommonCrawl)</li> <li>Books and long-form text</li> <li>Code repositories (GitHub)</li> <li>Mathematical and scientific text (arXiv)</li> <li>Structured and semi-structured documents</li> </ul> <p>Each domain contributes different inductive biases:</p> <ul> <li>Code improves logical consistency and state tracking</li> <li>Math improves symbolic manipulation</li> <li>Long-form text improves discourse modeling</li> </ul>"},{"location":"phases/pre_training/#32-data-cleaning-and-pii-redaction","title":"3.2 Data Cleaning and PII Redaction","text":"<p>Cleaning steps typically include:</p> <ul> <li>HTML boilerplate removal: Strips navigation bars, scripts, ads, and markup artifacts so the model learns from meaningful textual content rather than page structure noise.</li> <li>Unicode normalization: Converts visually or semantically equivalent characters into a canonical form to reduce vocabulary fragmentation and stabilize token statistics.</li> <li>Language-specific token normalization: Applies rules tailored to each language, such as lowercasing, diacritic handling, or script normalization, to improve consistency and learning efficiency.</li> <li>Removal of corrupted or truncated documents</li> </ul> <p>PII Redaction (Privacy):</p> <p>Strictly required for enterprise models to prevent regurgitating private data.</p> <ul> <li>Regex-based: Removing emails, SSNs, phone numbers.</li> <li>Entity Recognition: Replacing proper names with placeholders (e.g., <code>&lt;PERSON&gt;</code>).</li> <li>Memorization Audits: Checking if the model can generate unique sequences from the training set.</li> </ul>"},{"location":"phases/pre_training/#33-exact-and-near-duplicate-removal","title":"3.3 Exact and Near-Duplicate Removal","text":"<p>Duplicates distort the training distribution and waste compute.</p> <p>Techniques:</p> <ul> <li>Exact hashing: SHA-256 matching.</li> <li>MinHash / LSH: Locality-sensitive hashing for near-duplicates.</li> <li>Embedding-based similarity: For semantic duplicates.</li> </ul> <p>Why this matters:</p> <ul> <li>Inflated frequency biases</li> <li>Artificially low validation loss</li> <li>Memorization instead of abstraction</li> </ul>"},{"location":"phases/pre_training/#34-leakage-and-benchmark-contamination","title":"3.4 Leakage and Benchmark Contamination","text":"<p>Leakage sources:</p> <ul> <li>Public benchmark solutions in web data</li> <li>GitHub repositories with answers</li> <li>Fine-tuning data overlapping with evaluation sets</li> </ul> <p>Consequences:</p> <ul> <li>Inflated benchmark scores</li> <li>Misleading claims of reasoning ability</li> <li>Poor real-world generalization</li> </ul> <p>Mitigation requires proactive filtering (n-gram matching against test sets) and post-hoc auditing.</p>"},{"location":"phases/pre_training/#35-data-mixing-and-annealing","title":"3.5 Data Mixing and Annealing","text":"<p>How different data sources are combined determines the model's flavor.</p> <ul> <li>Static Mixing: Pre-assigned weights (e.g., 60% Web, 20% Code, 10% Math).</li> <li>Dynamic Selection (DoReMi): Continuously adjusts the sampling weights of different data sources during training using feedback from a smaller proxy model, prioritizing data that most improves validation loss and downweighting less useful or noisy sources.</li> <li>Annealing: A form of curriculum learning where high-quality data (synthetic, textbooks, math) is upsampled heavily in the final 5-10% of training to \"polish\" the model's skills.</li> </ul>"},{"location":"phases/pre_training/#36-synthetic-data-generation-and-risks","title":"3.6 Synthetic Data Generation and Risks","text":"<p>Synthetic data is increasingly used to:</p> <ul> <li>Fill data gaps</li> <li>Emphasize rare skills</li> <li>Bootstrap reasoning behaviors</li> </ul> <p>However, risks include:</p> <ul> <li>Model Collapse: Reinforcement of model errors leading to distribution narrowing.</li> <li>Reduced diversity and creativity.</li> </ul> <p>Uncontrolled synthetic feedback loops can permanently damage model quality.</p>"},{"location":"phases/pre_training/#4-architecture-choices","title":"4. Architecture Choices","text":""},{"location":"phases/pre_training/#41-decoder-only-transformers","title":"4.1 Decoder-Only Transformers","text":"<p>Most LLMs use a decoder-only Transformer due to:</p> <ul> <li>Causal attention alignment with autoregressive training</li> <li>Simpler deployment and KV caching</li> <li>Strong scaling behavior</li> </ul>"},{"location":"phases/pre_training/#42-mixture-of-experts-moe","title":"4.2 Mixture of Experts (MoE)","text":"<p>MoE is the standard for scaling model capacity while keeping inference cheap.</p> <ul> <li>Concept: Feed-Forward Network (FFN) layers are split into multiple \"experts.\"</li> <li>Routing: A learnable gate selects only top-\\(k\\) experts (usually \\(k=2\\)) for each token.</li> <li>Sparse Activation: A model might have huge total parameters (e.g., 8x7B) but low active parameters per token.</li> <li>Trade-offs: High capacity and low latency, but training can be unstable (load balancing) and memory bandwidth heavy.</li> </ul>"},{"location":"phases/pre_training/#43-modern-component-standards-the-no-vanilla-transformer","title":"4.3 Modern Component Standards (The \"No-Vanilla\" Transformer)","text":"<p>Standard Transformers (2017) are rarely used. Modern defaults include:</p> <ul> <li>RMSNorm: Replaces LayerNorm. It is computationally simpler (no mean centering) and numerically stable.</li> <li>Pre-Norm: Normalization is applied before attention/FFN layers (improves gradient flow) rather than after.</li> <li>SwiGLU: An activation function that replaces ReLU/GeLU. It adds a gating mechanism, increasing parameters slightly but improving convergence significantly.</li> <li>Bias-Free Layers: Removing bias terms from Linear layers to improve stability.</li> </ul>"},{"location":"phases/pre_training/#44-tokenization","title":"4.4 Tokenization","text":"<p>Common approaches:</p> <ul> <li>BPE (Byte Pair Encoding)</li> <li>SentencePiece Unigram</li> </ul> <p>Modern Nuances:</p> <ul> <li>Byte-Fallback: Falls back to raw bytes for unknown characters to ensure no <code>&lt;UNK&gt;</code> tokens exist (crucial for code).</li> <li>Digit Splitting: Splitting numbers (e.g., \"2025\" \\(\\to\\) \"2\", \"0\", \"2\", \"5\") rather than grouping them improves arithmetic reasoning.</li> <li>Trade-offs: Larger vocabularies compress text better (faster inference) but increase the embedding layer size and training difficulty.</li> </ul>"},{"location":"phases/pre_training/#45-positional-embeddings","title":"4.5 Positional Embeddings","text":"<p>Modern models use Rotary Positional Embeddings (RoPE).</p> <p>Advantages:</p> <ul> <li>Implicit relative positioning</li> <li>Better extrapolation to longer contexts</li> <li>Compatible with attention optimizations (FlashAttention)</li> </ul>"},{"location":"phases/pre_training/#46-attention-variants","title":"4.6 Attention Variants","text":"<p>To reduce memory and compute:</p> <ul> <li>Multi-Query Attention (MQA): All heads share one KV head.</li> <li>Grouped-Query Attention (GQA): Compromise where groups of heads share a KV head (Standard in LLaMA).</li> </ul> <p>Benefits:</p> <ul> <li>Drastically lower KV cache memory usage</li> <li>Faster inference decoding</li> </ul>"},{"location":"phases/pre_training/#47-context-length-scaling","title":"4.7 Context Length Scaling","text":"<p>Increasing context length impacts:</p> <ul> <li>Memory quadratically (without FlashAttention/Ring Attention)</li> <li>Training stability (loss spikes)</li> </ul> <p>Common strategies:</p> <ul> <li>Long-context fine-tuning: Pre-train on short context (e.g., 4k), then anneal on long context (e.g., 128k).</li> <li>Ring Attention: For training on sequences longer than single-GPU memory.</li> </ul>"},{"location":"phases/sft/","title":"Post-Training","text":"<p>Supervised Fine-Tuning (SFT) is the stage where a pre-trained base model is transformed into a useful assistant that follows instructions, respects formats, and exhibits desired interaction behavior. While pre-training builds a broad world model, SFT shapes how that knowledge is expressed.</p> <p>From an interview perspective, SFT is best understood as behavioral alignment via supervised learning.</p>"},{"location":"phases/sft/#1-what-sft-optimizes","title":"1. What SFT Optimizes","text":"<p>Map broad knowledge into a consistent, controllable interface.</p> <p>Conceptual shift</p> <ul> <li>Pre-training: \u201cContinue the text\u201d</li> <li>SFT: \u201cRespond appropriately to a user instruction\u201d</li> </ul> <p>This is achieved without reinforcement learning. The training signal is fully supervised.</p> <p>Within SFT:</p> <ul> <li>Instruction Tuning defines what behavior you are teaching</li> <li>Task Formatting defines how that behavior is presented to the model</li> </ul>"},{"location":"phases/sft/#2-instruction-tuning-and-task-formatting","title":"2. Instruction Tuning and Task Formatting","text":""},{"location":"phases/sft/#21-instruction-tuning","title":"2.1 Instruction Tuning","text":"<p>Instruction tuning teaches the model to condition its output on an explicit instruction rather than implicit continuation.</p> <p>Training objective</p> <p>Standard cross-entropy loss with selective loss masking.</p> <p>If:</p> <ul> <li>\\(x\\) = prompt tokens (system + user)</li> <li>\\(y\\) = assistant response tokens</li> </ul> <p>Then SFT minimizes:</p> \\[ \\mathcal{L}_{\\text{SFT}} = -\\sum_{t=1}^{|y|} \\log P(y_t \\mid x, y_{&lt;t}) \\] <p>Tokens belonging to \\(x\\) are excluded from the loss.</p> <p>Why masking matters</p> <ul> <li>Prevents memorization of prompts</li> <li>Ensures gradients only optimize response generation</li> <li>Stabilizes alignment behavior</li> </ul>"},{"location":"phases/sft/#22-task-and-prompt-formatting","title":"2.2 Task and Prompt Formatting","text":"<p>Modern SFT relies heavily on structured role-based templates, such as ChatML or LLaMA-style formats.</p> <p>Example <pre><code>&lt;|system|&gt; You are a helpful assistant. &lt;|end_of_text|&gt;\n&lt;|user|&gt; Summarize this article. &lt;|end_of_text|&gt;\n&lt;|assistant|&gt; The article discusses...\n</code></pre></p> <p>Why Formatting Matters</p> <ul> <li>Separates intent, context, and response  </li> <li>Enables multi-turn dialogue modeling  </li> <li>Reduces ambiguity during inference  </li> </ul> <p>Formatting does more than separate roles.</p> <ul> <li> <p>Implicit policy learning   Role tokens and system messages act as soft constraints that the model internalizes as behavioral priors.</p> </li> <li> <p>Gradient routing   Loss masking combined with role tokens ensures gradients primarily shape response behavior rather than prompt reconstruction.</p> </li> <li> <p>Inference controllability   Well-designed templates allow downstream systems to inject safety, tools, or routing instructions without retraining.</p> </li> <li> <p>Multi-turn state compression   Structured formatting helps the model compress dialogue history into latent state representations instead of treating each turn independently.</p> </li> </ul>"},{"location":"phases/sft/#3-prompt-diversity-as-regularization","title":"3. Prompt Diversity as Regularization","text":"<p>Prompt diversity is best understood as a regularization strategy, not just data augmentation.</p>"},{"location":"phases/sft/#31-semantic-diversity","title":"3.1 Semantic Diversity","text":"<p>Maintains coverage of distinct internal circuits:</p> <ul> <li>Symbolic reasoning and math</li> <li>Program synthesis and execution-style reasoning</li> <li>Creative and stylistic generation</li> <li>Factual recall under instruction pressure</li> <li>Conversational grounding</li> </ul>"},{"location":"phases/sft/#32-structural-diversity","title":"3.2 Structural Diversity","text":"<p>Reduces shortcut learning:</p> <ul> <li>Paraphrased intents prevent lexical memorization</li> <li>Variable verbosity avoids length priors</li> <li>Explicit vs implicit constraints force instruction parsing</li> </ul> <p>Key insight Insufficient structural diversity causes the model to learn response templates instead of instruction semantics.</p>"},{"location":"phases/sft/#4-human-vs-synthetic-supervision","title":"4. Human vs Synthetic Supervision","text":""},{"location":"phases/sft/#41-human-labeled-data","title":"4.1 Human-Labeled Data","text":"<p>Strengths</p> <ul> <li>High factual accuracy  </li> <li>Strong alignment with human preferences  </li> <li>Better safety and tone control  </li> </ul> <p>Limitations</p> <ul> <li>Expensive  </li> <li>Slow to scale  </li> <li>Limited coverage of edge cases  </li> </ul>"},{"location":"phases/sft/#42-synthetic-supervision","title":"4.2 Synthetic Supervision","text":"<p>Modern post-training pipelines rely heavily on synthetic data generation.</p>"},{"location":"phases/sft/#self-instruct","title":"Self-Instruct","text":"<ul> <li>Start with a small human-curated seed set  </li> <li>Use a strong model to generate new instructions and responses  </li> <li>Filter for quality  </li> </ul>"},{"location":"phases/sft/#evol-instruct-as-curriculum-learning","title":"Evol-Instruct as Curriculum Learning","text":"<p>Evol-Instruct implicitly creates a difficulty curriculum:</p> <ul> <li>Base instruction</li> <li>Added constraints</li> <li>Multi-hop or multi-objective reasoning</li> <li>Strict formatting or safety requirements</li> </ul> <p>This improves:</p> <ul> <li>Instruction decomposition</li> <li>Constraint satisfaction</li> <li>Planning depth</li> </ul>"},{"location":"phases/sft/#43-rejection-sampling-best-of-n","title":"4.3 Rejection Sampling (Best-of-N)","text":"<p>Process</p> <ol> <li>Generate \\(K\\) responses per prompt  </li> <li>Score them using:<ul> <li>A reward model, or  </li> <li>A stronger reference model  </li> </ul> </li> <li>Select the best response  </li> <li>Fine-tune on the selected outputs  </li> </ol> <p>Key properties</p> <ul> <li>Sharpens instruction adherence without policy gradients</li> <li>Reduces variance compared to RLHF</li> <li>Biases the model toward high-reward modes</li> </ul> <p>Advanced risk</p> <ul> <li>Over-optimization toward the reward model</li> <li>Reduced output diversity</li> <li>Reward hacking if the scorer is weak</li> </ul>"},{"location":"phases/sft/#5-data-quality-over-quantity","title":"5. Data Quality over Quantity","text":""},{"location":"phases/sft/#51-the-lima-hypothesis","title":"5.1 The LIMA Hypothesis","text":"<p>\u201cLess Is More for Alignment\u201d</p> <p>Key insight: </p> <p>~1,000 extremely high-quality examples can outperform tens of thousands of noisy ones  </p> <p>Implications: </p> <ul> <li>Careful curation matters more than scale  </li> <li>Labeler expertise is critical  </li> <li>Reduces overfitting and style bias  </li> </ul>"},{"location":"phases/sft/#52-typical-sft-data-mix","title":"5.2 Typical SFT Data Mix","text":"<p>A strong SFT dataset often includes:</p> <ul> <li>Reasoning and step-by-step explanations  </li> <li>Creative and stylistic writing  </li> <li>Coding and math problems  </li> <li>Safety and refusal examples  </li> <li>Multi-turn conversations  </li> </ul>"},{"location":"phases/sft/#6-training-optimizations-and-stability","title":"6. Training Optimizations and Stability","text":"Technique Purpose Explanation Packing Throughput Concatenates multiple short samples into a single context window to avoid padding waste Loss Masking Correct gradients Computes loss only on assistant tokens NEFTune Generalization Adds noise to embeddings during SFT to prevent token-level overfitting Low learning rate Stability Typical values are \\(1e^{-6}\\) to \\(5e^{-6}\\) Dropout Regularization Reduces stylistic memorization"},{"location":"phases/sft/#61-packing-vs-padding","title":"6.1 Packing vs Padding","text":"<p>In Supervised Fine-Tuning, training samples vary widely in length. How these samples are batched has a direct impact on compute efficiency, gradient quality, and training stability.</p>"},{"location":"phases/sft/#padding","title":"Padding","text":"<p>All sequences in a batch are padded to the length of the longest sequence using <code>[PAD]</code> tokens.</p> <p>Example</p> <ul> <li><code>Seq 1: [x x x x x x]</code></li> <li><code>Seq 2: [x x x PAD PAD PAD]</code></li> <li><code>Seq 3: [x x x x PAD PAD]</code></li> </ul> <p>Why it is inefficient</p> <ul> <li>Attention, feedforward layers, and layer norms still execute on padded tokens</li> <li>Memory bandwidth and FLOPs are wasted on tokens that contribute no gradient</li> <li>Effective tokens per batch can drop sharply when length variance is high</li> </ul> <p>Impact at scale</p> <ul> <li>Lowers tokens processed per second</li> <li>Increases training cost</li> <li>Reduces gradient signal density</li> </ul>"},{"location":"phases/sft/#packing","title":"Packing","text":"<p>Multiple short samples are concatenated into a single long sequence up to the model\u2019s maximum context length. Each sample is separated by an EOS or special boundary token, and loss masking prevents cross-sample leakage.</p> <p>Example</p> <p><code>[Prompt\u2081 \u2192 Response\u2081 &lt;EOS&gt; Prompt\u2082 \u2192 Response\u2082 &lt;EOS&gt; Prompt\u2083 \u2192 Response\u2083]</code></p> <p>Why it is efficient</p> <ul> <li>Nearly every token contributes to loss</li> <li>Attention computation is fully utilized</li> <li>Higher effective batch token count without increasing memory</li> </ul> <p>Why Packing Improves GPU Utilization</p> <p>Packing increases:</p> <ul> <li>Arithmetic intensity by reducing idle FLOPs</li> <li>Token density per batch, improving gradient signal-to-noise ratio</li> <li>Throughput, often by 2x to 3x in instruction-tuning workloads where samples are short</li> </ul> <p>This is especially impactful for:</p> <ul> <li>Instruction datasets with short prompts</li> <li>Chat-style SFT data</li> <li>Small to medium batch sizes constrained by memory</li> </ul> <p>Important Implementation Details</p> <ul> <li>Loss masking must reset at each sample boundary</li> <li>Attention masking must prevent tokens from attending across examples</li> <li>EOS handling is critical to avoid information leakage</li> <li>Position indices may need resetting depending on the architecture</li> </ul> <p>Incorrect packing can cause:</p> <ul> <li>Cross-example contamination</li> <li>Training instability</li> <li>Spurious memorization</li> </ul> <p>Padding vs Packing Summary</p> Aspect Padding Packing Compute efficiency Low High Token utilization Sparse Dense Training speed Slow Fast Implementation complexity Simple Moderate Risk of leakage None Requires care <p>Takeaway: Packing is not just an optimization. It changes the effective learning dynamics by increasing gradient density and stabilizing updates, which is why it is now standard practice in large-scale SFT pipelines.</p>"},{"location":"phases/sft/#62-neftune-concept-and-recent-insights","title":"6.2 NEFTune: Concept and Recent Insights","text":"<p>NEFTune (Noisy Embeddings Fine-Tuning) is a targeted regularization method used during supervised fine-tuning to improve generalization and stability.</p>"},{"location":"phases/sft/#core-idea","title":"Core Idea","text":"<p>During SFT, small controlled noise is injected into the embedding layer (or early representation layers). The noise acts as a soft regularizer that prevents the model from over-specializing on specific training tokens or patterns.</p> <p>Instead of purely minimizing loss on the fine-tuning dataset, NEFTune encourages the model to learn representations that are robust to small perturbations, resulting in better performance on unseen prompts and fewer hallucinations.</p>"},{"location":"phases/sft/#why-it-works","title":"Why It Works","text":"<ul> <li> <p>Prevents token memorization   Noise makes exact token sequences less predictable, forcing the model to rely on deeper semantic features instead of surface patterns.</p> </li> <li> <p>Improves out-of-distribution (OOD) robustness   Tuning with noise helps the model resist over-confidence on narrow fine-tuning distributions.</p> </li> <li> <p>Smooths loss landscape   By blurring precise embedding positions, NEFTune reduces sharp local minima that often cause overfitting.</p> </li> </ul>"},{"location":"phases/sft/#how-it-is-applied","title":"How It Is Applied","text":"<p>A typical NEFTune variant:</p> <ul> <li>Add Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) to embeddings each forward pass</li> <li>The noise scale \\(\\sigma\\) is kept small so that semantics are preserved but spurious correlations are suppressed</li> </ul> <p>During backpropagation the noise is not removed; it shapes gradient updates continuously.</p>"},{"location":"phases/sft/#recent-trends-20252026","title":"Recent Trends (2025\u20132026)","text":"<ul> <li>NEFTune has emerged as a standard trick in instruction-tuning pipelines for models like LLaMA-derivatives and open-weight assistants.</li> <li>Research shows it consistently boosts generalization metrics (e.g., AlpacaEval, Ultrachat) without increasing dataset size.</li> <li>Some systems use layer-wise noise schedules, adding noise in early layers and reducing it in later layers to balance representation robustness with output precision.</li> <li>Combined with packing and rejection sampling, NEFTune significantly improves instruction fidelity on long-context dialogues.</li> </ul>"},{"location":"phases/sft/#practical-tips","title":"Practical Tips","text":"<ul> <li>Use a conservative noise magnitude to avoid destabilizing training</li> <li>Pair NEFTune with low learning rates and dropout for maximum regularization</li> <li>Monitor validation generalization rather than training loss to tune noise hyperparameters</li> </ul>"},{"location":"phases/sft/#7-overfitting-and-catastrophic-forgetting","title":"7. Overfitting and Catastrophic Forgetting","text":""},{"location":"phases/sft/#catastrophic-forgetting","title":"Catastrophic Forgetting","text":"<p>The model loses general reasoning or factual knowledge learned during pre-training.</p> <p>Causes</p> <ul> <li>Narrow SFT domain  </li> <li>High learning rates  </li> <li>Full fine-tuning on small datasets  </li> </ul> <p>Mitigations</p> <ul> <li>Mix 5\u201310% pre-training style data  </li> <li>Use PEFT methods such as LoRA  </li> <li>Lower learning rates  </li> <li>Shorter training schedules  </li> </ul>"},{"location":"phases/sft/#overfitting","title":"Overfitting","text":"<p>The model learns labeler-specific style rather than task intent.</p> <p>Symptoms</p> <ul> <li>Over-politeness  </li> <li>Repetitive phrasing  </li> <li>Template-like answers  </li> </ul> <p>Mitigations</p> <ul> <li>Prompt diversity  </li> <li>Early stopping  </li> <li>Noise injection such as NEFTune  </li> </ul>"},{"location":"phases/sft/#8-lora-vs-full-fine-tuning","title":"8. LoRA vs Full Fine-Tuning","text":""},{"location":"phases/sft/#lora-and-peft","title":"LoRA and PEFT","text":"<ul> <li>Low compute cost  </li> <li>Preserves base model knowledge  </li> <li>Lower risk of catastrophic forgetting  </li> <li>Preferred for alignment and style shifts  </li> </ul>"},{"location":"phases/sft/#full-fine-tuning","title":"Full Fine-Tuning","text":"<ul> <li>Needed for large domain shifts  </li> <li>Higher risk of forgetting  </li> <li>Requires careful regularization  </li> </ul> <p>Rule of thumb: </p> <ul> <li>Behavior change \u2192 LoRA  </li> <li>Knowledge change \u2192 Full fine-tuning  </li> </ul>"},{"location":"phases/sft/#9-common-failure-modes-after-sft","title":"9. Common Failure Modes After SFT","text":""},{"location":"phases/sft/#increased-hallucinations","title":"Increased Hallucinations","text":"<p>Often caused by knowledge contradiction.</p> <p>If SFT data conflicts with pre-training facts, the model may prioritize format compliance over correctness.</p> <p>Mitigations</p> <ul> <li>Fact-consistent SFT data  </li> <li>Retrieval-augmented generation  </li> <li>Post-SFT preference optimization  </li> </ul>"},{"location":"phases/sft/#10-sft-vs-pre-training-summary","title":"10. SFT vs Pre-training Summary","text":"Aspect Pre-training Supervised Fine-Tuning Objective World modeling Behavior alignment Data scale Trillions of tokens 10k to 100k samples Loss Full sequence NTP Masked response NTP Compute Massive Moderate Primary risk Under-training Overfitting and forgetting"},{"location":"phases/system2/","title":"System2","text":"<p>This documentation tracks the transition from \"System 1\" (probabilistic next-token prediction) to \"System 2\" (deliberate, slow thinking) in Large Language Models.</p>"},{"location":"phases/system2/#the-three-pillars-of-ai-reasoning","title":"\ud83c\udfd7\ufe0f The Three Pillars of AI Reasoning","text":""},{"location":"phases/system2/#1-the-training-objective-learning-to-think","title":"1. The Training Objective: Learning to Think","text":"<p>Reasoning is not just a prompting trick; it is a learned capability baked into model weights during post-training.</p> <ul> <li>CoT Training (STaR): Moving beyond simple \"think step-by-step\" prompts. We now fine-tune models on massive datasets of reasoning traces where the model has successfully \"self-corrected\" to reach a solution.</li> <li>RLVR (Reinforcement Learning from Verifiable Rewards): Unlike RLHF, which is subjective, RLVR uses objective ground truths.<ul> <li>Math &amp; Code: These serve as \"Gold Standard\" domains because the reward signal is binary (the code compiles or it doesn't; the equation balances or it doesn't).</li> </ul> </li> <li>PRM vs. ORM:<ul> <li>ORM (Outcome Reward Model): Rewards only the final answer. High risk of \"reward hacking\" (getting the right answer for the wrong reason).</li> <li>PRM (Process Reward Model): Rewards the model for every intermediate logical step. This is the primary driver of performance in models like OpenAI o1.</li> </ul> </li> </ul>"},{"location":"phases/system2/#2-the-inference-architecture-scaling-test-time-compute-ttc","title":"2. The Inference Architecture: Scaling Test-Time Compute (TTC)","text":"<p>The \"Inference Scaling Law\" states that model performance scales with the amount of compute dedicated to \"thinking\" during the generation phase.</p> <ul> <li> <p>Search Strategies:</p> <ul> <li>Best-of-N: Sampling \\(N\\) different completions and using a verifier to pick the best one.</li> <li>MCTS (Monte Carlo Tree Search): Navigating a tree of possible reasoning steps, simulating outcomes, and backtracking when a path fails.</li> </ul> </li> <li> <p>Compute-Optimal Allocation: Modern systems use a \"router\" to decide how much TTC to spend.</p> <ul> <li>Low Difficulty: 1 forward pass (System 1).</li> <li>High Difficulty: Thousands of search iterations (System 2).</li> </ul> </li> <li> <p>Speculative Reasoning: Using a small, fast model to draft reasoning steps and a large model to verify them in batches, reducing latency.</p> </li> </ul>"},{"location":"phases/system2/#3-behavioral-dynamics-self-correction-traces","title":"3. Behavioral Dynamics: Self-Correction &amp; Traces","text":"<p>How a model manages its internal logic and interacts with the user.</p> <ul> <li> <p>Hidden vs. Visible Traces:</p> <ul> <li>Visible: High interpretability, but prone to user-influence and \"sycophancy.\"</li> <li>Hidden: Used by o1/R1 to prevent \"Chain-of-Thought hacking.\" It allows for Neuralese\u2014an internal, highly efficient symbolic language the model develops to reason faster than human language allows.</li> </ul> </li> <li> <p>Reflection Loops: The model is trained to recognize \"dead ends.\" If a logic path contradicts a prior step, the model is penalized and forced to backtrack.</p> </li> <li> <p>Steganography Risk: A safety concern where models might hide \"unsafe\" reasoning or planning within hidden thought traces to bypass monitoring.</p> </li> </ul>"},{"location":"phases/system2/#interview-prep-high-signal-questions","title":"\ud83d\ude80 Interview Prep: High-Signal Questions","text":"Question Key Insight Why use RLVR over RLHF for reasoning? RLHF is limited by human checking speed and bias; RLVR uses automated verifiers (compilers/solvers) to scale exponentially. What is the \"Pause\" token? A latent mechanism (or specific token) that allows a model to perform more internal computation before committing to a visible output. How does DeepSeek's GRPO change scaling? It removes the need for a separate \"Critic\" model during RL, significantly reducing the VRAM required to train reasoning capabilities."},{"location":"phases/system2/#project-ideas-for-this-repo","title":"\ud83d\udee0\ufe0f Project Ideas for this Repo","text":"<ol> <li>Verifier Implementation: Build a Python script that uses a Process Reward Model to score a math reasoning trace.</li> <li>TTC Benchmark: Compare the accuracy of a 7B model using \"Best-of-10\" search against a 70B model using a single pass.</li> <li>Trace Visualization: Create a tool to visualize MCTS tree exploration in a logical puzzle.</li> </ol>"},{"location":"training_techniques/foundation/","title":"Foundation","text":""},{"location":"training_techniques/foundation/#1-what-is-an-llm-optimizing","title":"1 What is an LLM Optimizing?","text":"<p>At its core, a Large Language Model (LLM) is a probabilistic system designed to model the distribution of natural language. Despite emergent reasoning and planning behaviors, the training objective itself is simple: reduce uncertainty about the next token given prior context.</p>"},{"location":"training_techniques/foundation/#11-the-autoregressive-objective","title":"1.1 The Autoregressive Objective","text":"<p>Most LLMs are trained using an Autoregressive (AR) or Causal Language Modeling (CLM) objective. The joint probability of a token sequence is factorized as a product of conditional probabilities:</p> \\[ P(x_1, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{&lt;t}) \\] <p>This formulation assumes:</p> <ul> <li>Tokens are conditionally independent given their history</li> <li>Knowledge is captured implicitly through long context dependencies</li> <li>Tokenization defines the atomic units of prediction</li> </ul>"},{"location":"training_techniques/foundation/#12-the-loss-function-negative-log-likelihood","title":"1.2 The Loss Function: Negative Log-Likelihood","text":"<p>Training minimizes the Negative Log-Likelihood (NLL) of the observed data, which is equivalent to Cross-Entropy Loss:</p> \\[ \\mathcal{L}(\\theta) = - \\mathbb{E}_{(x_1,\\dots,x_T) \\sim D} \\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{&lt;t}) \\] <p>Where: - \\(\\theta\\) are the model parameters - \\(x_{&lt;t}\\) is the context window - \\(P_\\theta\\) is the predicted probability distribution produced by a Softmax layer</p>"},{"location":"training_techniques/foundation/#13-cross-entropy-kl-divergence-and-learning","title":"1.3 Cross-Entropy, KL Divergence, and Learning","text":"<p>Minimizing cross-entropy implicitly minimizes the KL divergence between the true data distribution \\(P_{data}\\) and the model distribution \\(P_\\theta\\):</p> \\[ H(P_{data}, P_\\theta) = H(P_{data}) + D_{KL}(P_{data} \\| P_\\theta) \\] <p>Since \\(H(P_{data})\\) is fixed, training pushes the model distribution closer to the real language distribution. This connects LLM training directly to information theory and compression.</p>"},{"location":"training_techniques/foundation/#14-why-next-token-prediction-works-the-compression-hypothesis","title":"1.4 Why Next-Token Prediction Works (The Compression Hypothesis)","text":"<p>Predicting the next token forces the model to internalize structure across many domains.</p> <ul> <li>Compression implies abstraction: To compress diverse text, the model must learn syntax, semantics, facts, and procedures.</li> <li>Softmax competition: Increasing probability mass for the correct token necessarily decreases mass for alternatives, encouraging fine-grained representations.</li> <li>Generalization pressure: Predicting well across domains requires reusable internal features, which later appear as reasoning, translation, or coding skills.</li> </ul>"},{"location":"training_techniques/foundation/#2-language-modeling-and-likelihood-minimization","title":"2 Language Modeling and Likelihood Minimization","text":""},{"location":"training_techniques/foundation/#21-maximum-likelihood-estimation-mle","title":"2.1 Maximum Likelihood Estimation (MLE)","text":"<p>LLM training is an instance of Maximum Likelihood Estimation, where we seek parameters \\(\\theta^*\\) that maximize the likelihood of observed text:</p> \\[ \\theta^* = \\arg\\max_\\theta \\mathbb{E}_{x \\sim D} [\\log P_\\theta(x)] \\] <p>MLE provides a stable and scalable objective but does not encode preferences such as helpfulness, safety, or instruction following.</p>"},{"location":"training_techniques/foundation/#22-teacher-forcing","title":"2.2 Teacher Forcing","text":"<p>Teacher forcing is a training strategy where the model is conditioned on the ground-truth previous token rather than its own prediction when computing the next-token loss. For a sequence \\(x_1, \\dots, x_T\\), the model always receives \\(x_{t-1}\\) as input when predicting \\(x_t\\), even if it would have predicted a different token at step \\(t-1\\).</p> <p>Small Practical Example</p> <p>Task: Predict the sentence  </p> <p>\u201cThe cat sat on the mat\u201d</p>"},{"location":"training_techniques/foundation/#step-1-training-data","title":"Step 1: Training data","text":"<p>Tokens: <code>[The, cat, sat, on, the, mat]</code></p>"},{"location":"training_techniques/foundation/#step-2-training-with-teacher-forcing","title":"Step 2: Training with teacher forcing","text":"<p>At each step, the model is conditioned on the ground-truth previous token.</p> <ol> <li> <p>Input: <code>The</code> Target: <code>cat</code></p> </li> <li> <p>Input: <code>The cat</code> Target: <code>sat</code></p> </li> <li> <p>Input: <code>The cat sat</code> Target: <code>on</code></p> </li> <li> <p>Input: <code>The cat sat on</code> Target: <code>the</code></p> </li> <li> <p>Input: <code>The cat sat on the</code> Target: <code>mat</code></p> </li> </ol> <p>Even if the model predicts an incorrect token at any step, the next input still uses the true token from the dataset.</p>"},{"location":"training_techniques/foundation/#step-3-inference-time-behavior","title":"Step 3: Inference time behavior","text":"<p>At inference, the model must condition on its own predictions.</p> <ol> <li> <p>Input: <code>The</code> Prediction: <code>dog</code></p> </li> <li> <p>Next input: <code>The dog</code>    Errors now propagate forward</p> </li> </ol> <p>This train\u2013test mismatch is known as exposure bias and is a key limitation of teacher forcing.</p> <p>Advantages:</p> <ul> <li>Full parallelization of loss computation across all positions in a Transformer</li> <li>Stable gradients, since errors do not compound during training</li> </ul> <p>Limitation: Exposure Bias</p> <ul> <li>Introduces exposure bias: during inference, the model must condition on its own past predictions, a distribution shift that can lead to error accumulation. This gap motivates post-training techniques such as supervised fine-tuning and reinforcement learning based alignment.</li> </ul>"},{"location":"training_techniques/foundation/#23-perplexity-as-an-evaluation-metric","title":"2.3 Perplexity as an Evaluation Metric","text":"<p>Perplexity measures how uncertain a language model is, on average, when predicting the next token in a sequence.</p> <p>Formally, if a model is trained using cross-entropy loss \\(\\mathcal{L}\\), then perplexity is defined as:</p> \\[ \\text{PPL} = \\exp(\\mathcal{L}) \\] <p>where \\(\\mathcal{L}\\) is the average negative log-probability assigned to the correct next token.</p>"},{"location":"training_techniques/foundation/#intuition","title":"Intuition","text":"<p>A language model repeatedly answers the question:</p> <p>How many plausible choices do I believe the next token could be?</p> <p>Perplexity converts log probabilities back into an interpretable scale representing the effective number of choices.</p> <ul> <li> <p>PPL = 1   The model is fully confident and assigns probability 1 to the correct token.</p> </li> <li> <p>PPL = 10   The model behaves as if it is choosing uniformly among about 10 tokens.</p> </li> <li> <p>PPL = 100   The model is highly uncertain, with roughly 100 plausible next tokens.</p> </li> </ul> <p>This is why perplexity is often described as the model\u2019s average branching factor.</p>"},{"location":"training_techniques/foundation/#why-lower-perplexity-is-better","title":"Why Lower Perplexity Is Better","text":"<ul> <li>Lower PPL means the model assigns higher probability to the correct next token.</li> <li>This corresponds to lower uncertainty and better next-token prediction.</li> <li>Minimizing cross-entropy during training directly minimizes perplexity.</li> </ul>"},{"location":"training_techniques/foundation/#important-clarification","title":"Important Clarification","text":"<p>Perplexity is not a direct measure of: - Reasoning ability - Factual correctness - Alignment or instruction following</p> <p>It only measures how well the model predicts the data distribution. While lower perplexity often correlates with better text quality, it does not guarantee better downstream performance.</p>"},{"location":"training_techniques/foundation/#3-why-scaling-works-and-where-it-breaks","title":"3. Why Scaling Works and Where it Breaks","text":""},{"location":"training_techniques/foundation/#31-empirical-scaling-laws","title":"3.1 Empirical Scaling Laws","text":"<p>Performance improves predictably as a power-law function of:</p> <ul> <li>Model parameters</li> <li>Training tokens</li> <li>Compute budget</li> </ul> <p>This empirical behavior explains the rapid gains from larger models.</p>"},{"location":"training_techniques/foundation/#kaplan-scaling-laws-2020","title":"Kaplan Scaling Laws (2020)","text":"<p>Early results suggested scaling model size was the dominant factor.</p> <ul> <li>Loss scales roughly as a power-law in parameter count</li> <li>Data was treated as effectively unlimited</li> </ul>"},{"location":"training_techniques/foundation/#chinchilla-scaling-laws-2022","title":"Chinchilla Scaling Laws (2022)","text":"<p>Later work showed most large models were undertrained.</p> <p>Key findings:</p> <ul> <li>Optimal performance requires balancing parameters and tokens</li> <li>Roughly 20 training tokens per parameter is compute optimal</li> <li>Smaller models trained on more data can outperform larger undertrained models</li> </ul> <p>This led to data-centric model design such as LLaMA and Mistral.</p>"},{"location":"training_techniques/foundation/#32-where-scaling-breaks","title":"3.2 Where Scaling Breaks","text":""},{"location":"training_techniques/foundation/#1-data-scarcity-and-synthetic-feedback-loops","title":"1. Data Scarcity and Synthetic Feedback Loops","text":"<ul> <li>High-quality human text is limited</li> <li>Synthetic data risks reducing diversity</li> <li>Repeated self-training can lead to model collapse</li> </ul>"},{"location":"training_techniques/foundation/#2-capability-saturation","title":"2. Capability Saturation","text":"<ul> <li>Loss improves smoothly, but abilities emerge discontinuously</li> <li>Reasoning, planning, and tool use do not scale linearly with perplexity</li> <li>Small loss gains can hide large behavioral differences</li> </ul>"},{"location":"training_techniques/foundation/#3-inference-cost-and-latency","title":"3. Inference Cost and Latency","text":"<ul> <li>Larger models increase memory, latency, and cost</li> <li>This motivates inference-efficient designs</li> </ul>"},{"location":"training_techniques/foundation/#4-test-time-scaling","title":"4. Test-Time Scaling","text":"<ul> <li>Recent systems scale inference compute rather than parameters</li> <li>Models generate longer internal reasoning traces</li> <li>This shifts scaling from training time to inference time</li> </ul> <p>Examples include OpenAI o1 and DeepSeek-R1.</p>"},{"location":"training_techniques/thinking_llms/","title":"LLM Reasoning & Thinking Models","text":""},{"location":"training_techniques/thinking_llms/#1-overview-the-system-1-vs-system-2-shift","title":"1. Overview: The System 1 vs. System 2 Shift","text":"<p>In 2026, the AI landscape is defined by the transition from \"Next-Token Predictors\" to \"Reasoning Agents.\" This is often explained using Daniel Kahneman's framework:</p> <ul> <li>Generic LLMs (System 1): Fast, intuitive, and associative. They respond instantly based on high-probability patterns.</li> <li>Thinking LLMs (System 2): Slow, deliberate, and logical. They use \"Inference-Time Compute\" to verify their own work before displaying it.</li> </ul>"},{"location":"training_techniques/thinking_llms/#2-technical-comparison-table","title":"2. Technical Comparison Table","text":"Feature Generic LLMs (GPT-4o, Llama 3) Thinking LLMs (o1, R1, Claude 3.7) Primary Mechanism Pattern Recognition Reinforcement Learning (RL) + Search Computation Constant (Fixed cost per token) Variable (More time = More intelligence) Internal Process Direct Output Hidden \"Chain of Thought\" (CoT) Self-Correction Rare (usually hallucinations) Native (backtracks on errors) Best For Chat, Summaries, Creative Writing Coding, Math, Strategic Planning"},{"location":"training_techniques/thinking_llms/#3-core-mechanism-test-time-compute-scaling","title":"3. Core Mechanism: Test-Time Compute Scaling","text":""},{"location":"training_techniques/thinking_llms/#31-inference-time-scaling-laws","title":"3.1 Inference-Time Scaling Laws","text":"<p>Reasoning performance scales with compute allocated at inference, not just with parameter count. This manifests as:</p> <ul> <li>More internal tokens devoted to reasoning</li> <li>Sampling and evaluating multiple candidate solutions</li> <li>Backtracking and correction loops</li> </ul> <p>Empirically, reasoning benchmarks show near-log-linear gains with additional inference steps until saturation.</p> <p>3.2 Compute as a Control Knob</p> <p>Thinking models expose inference compute as a tunable parameter:</p> <ul> <li>Low compute approximates System 1 behavior</li> <li>High compute activates deeper reasoning loops</li> <li>This decouples model intelligence from latency constraints, enabling dynamic deployment strategies.</li> </ul>"},{"location":"training_techniques/thinking_llms/#4-internal-reasoning-representations","title":"4. Internal Reasoning Representations","text":""},{"location":"training_techniques/thinking_llms/#41-hidden-chain-of-thought","title":"4.1 Hidden Chain-of-Thought","text":"<p>Thinking LLMs generate intermediate reasoning traces that are:</p> <ul> <li>Hidden from the user</li> <li>Used for internal verification and search</li> <li>Discarded or summarized before final output</li> </ul> <p>This avoids leaking brittle or unsafe reasoning while retaining cognitive benefits.</p>"},{"location":"training_techniques/thinking_llms/#42-search-over-reasoning-paths","title":"4.2 Search Over Reasoning Paths","text":"<p>Rather than committing to a single chain, System 2 models often:</p> <ul> <li>Generate multiple reasoning candidates</li> <li>Score them using reward models</li> <li>Select or refine the best trajectory</li> </ul> <p>This reframes inference as a planning problem rather than a single forward pass.</p>"},{"location":"training_techniques/thinking_llms/#5-reinforcement-learning-for-reasoning","title":"5. Reinforcement Learning for Reasoning","text":""},{"location":"training_techniques/thinking_llms/#51-outcome-reward-models-vs-process-reward-models","title":"5.1 Outcome Reward Models vs Process Reward Models","text":"Reward Type What It Rewards Limitation Outcome Reward Model (ORM) Final answer correctness No signal for reasoning quality Process Reward Model (PRM) Each intermediate reasoning step Higher training complexity <p>PRMs dramatically reduce multi-step hallucinations by aligning rewards with valid intermediate logic.</p>"},{"location":"training_techniques/thinking_llms/#52-grpo-group-relative-policy-optimization","title":"5.2 GRPO: Group Relative Policy Optimization","text":"<p>Introduced prominently in DeepSeek-R1, GRPO:</p> <ul> <li>Samples multiple reasoning trajectories from the same model</li> <li>Ranks them relative to each other</li> <li>Reinforces the most efficient and correct paths</li> </ul> <p>This removes reliance on large teacher models and enables reasoning capability to emerge from self-comparison.</p>"},{"location":"training_techniques/thinking_llms/#6-distillation-of-reasoning","title":"6. Distillation of Reasoning","text":"<p>Reasoning capabilities are distillable, even when learned via reinforcement learning.</p>"},{"location":"training_techniques/thinking_llms/#distillation-pipeline","title":"Distillation Pipeline","text":"<ol> <li>Run a large System 2 model with high inference compute</li> <li>Collect validated reasoning traces</li> <li>Use them as supervised fine-tuning data for smaller models</li> </ol> <p>This has enabled 7B\u201314B models to exhibit structured reasoning previously limited to frontier-scale systems.</p>"},{"location":"training_techniques/thinking_llms/#7-failure-modes-and-trade-offs","title":"7. Failure Modes and Trade-offs","text":""},{"location":"training_techniques/thinking_llms/#71-reward-hacking","title":"7.1 Reward Hacking","text":"<p>Models may learn that: - Longer reasoning chains yield higher rewards - Redundancy masquerades as correctness</p> <p>Mitigations include length penalties, diversity rewards, and consistency checks.</p>"},{"location":"training_techniques/thinking_llms/#72-overthinking","title":"7.2 Overthinking","text":"<ul> <li>Excessive inference compute yields diminishing returns</li> <li>Latency increases without accuracy gains</li> </ul> <p>Adaptive early stopping is commonly used to terminate reasoning once confidence thresholds are met.</p>"},{"location":"training_techniques/thinking_llms/#8-evaluation-and-benchmarks","title":"8. Evaluation and Benchmarks","text":"<p>Reasoning models require benchmarks that stress multi-step cognition.</p> Benchmark Capability Tested AIME Olympiad-level mathematics GPQA Graduate-level scientific reasoning HumanEval Algorithmic and coding logic Codeforces Long-horizon program synthesis <p>Evaluation must consider accuracy versus compute trade-offs, not accuracy alone.</p>"},{"location":"training_techniques/thinking_llms/#9-deployment-decision-framework","title":"9. Deployment Decision Framework","text":"<p>Choosing between System 1 and System 2 depends on: - Latency tolerance - Cost constraints - Error sensitivity</p> <p>System 1 models are preferred for high-throughput, low-risk tasks.</p> <p>System 2 models are preferred when correctness dominates, such as legal analysis, scientific reasoning, and complex debugging.</p> <p>Hybrid systems increasingly route queries dynamically based on estimated difficulty.</p>"},{"location":"training_techniques/thinking_llms/#10-notable-models-of-20252026","title":"10. Notable Models of 2025\u20132026","text":"<ul> <li>OpenAI o1 &amp; o3: The first to commercialize \"Reasoning Tokens.\" o3 is currently the gold standard for competitive programming (Codeforces) and high-level mathematics.</li> <li>DeepSeek-R1: A massive breakthrough for open-source AI. It proved that RL can \"distill\" reasoning capabilities into smaller models (like 7B or 14B parameters).</li> <li>Claude 3.7 Sonnet: Introduced \"Hybrid Reasoning,\" allowing the user to toggle \"Extended Thinking.\" This solves the latency issue by letting users choose when they need the model to \"think.\"</li> <li>Gemini 2.5 Pro (Reasoning): Uses Google\u2019s massive context window to reason across hours of video or thousands of lines of code simultaneously.</li> </ul>"},{"location":"training_techniques/thinking_llms/#11-relationship-to-training-phases","title":"11. Relationship to Training Phases","text":"<p>Thinking behavior emerges from the interaction of all training stages:</p> <ul> <li>Pre-training provides linguistic and world knowledge</li> <li>Mid-training injects structured reasoning and long-context priors</li> <li>RL-based post-training activates inference-time computation strategies</li> </ul> <p>System 2 capability is therefore not purely a post-training artifact.</p>"},{"location":"training_techniques/thinking_llms/#12-technical-qa","title":"12. Technical Q&amp;A","text":"<p>Q: What is \"Reward Hacking\" in reasoning models?</p> <p>A: It\u2019s a failure mode where a model learns that writing longer reasoning chains results in higher rewards from the PRM, even if the logic is circular or redundant. Engineers combat this by penalizing length or using more diverse reward signals.</p> <p>Q: How do you decide when to deploy a Thinking LLM vs. a Generic LLM?</p> <p>A: It's a trade-off between Latency, Cost, and Accuracy.  * Use Generic for high-throughput, low-latency tasks (customer service bots, simple translations). * Use Thinking for high-stakes accuracy (legal analysis, complex debugging, scientific research).</p> <p>Q: Can you distill reasoning?</p> <p>A: Yes. You can take the \"Thinking Traces\" from a large model (like o1) and use them as fine-tuning data for a smaller model (like Llama 8B). This teaches the smaller model to \"mimic\" the logical structure of the larger one.</p>"}]}