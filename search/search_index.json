{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"references/","title":"References","text":"<ul> <li>Hu, E. J., et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 </li> <li>Dettmers, T., et al. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 </li> <li>Hugging Face PEFT documentation: https://huggingface.co/docs/peft </li> <li>Community writeups and practical tips (e.g., blog posts and notebooks on LoRA/QLoRA setups).</li> <li>\"4-bit NormalFloat (NF4) Quantization\" \u2013 EmergentMind. Link </li> <li>Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs (2023)</li> <li>Tim Dettmers, bitsandbytes GitHub: https://github.com/TimDettmers/bitsandbytes</li> <li>Hugging Face PEFT Documentation: https://huggingface.co/docs/peft</li> <li>Manal Elaidouni, 4-Bit Quantization in QLoRA Explained</li> <li> <p>D. Prasad, QLoRA Explained \u2013 A Deep Dive into Parameter Efficient Fine-Tuning</p> </li> <li> <p>Paper: QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</p> </li> <li>Hugging Face PEFT: https://huggingface.co/docs/peft</li> <li>Bitsandbytes Library: https://github.com/TimDettmers/bitsandbytes</li> </ul>"},{"location":"advanced_topics/4bit_normal_float/","title":"\ud83d\udcd0 4-bit NormalFloat (NF4) Quantization","text":""},{"location":"advanced_topics/4bit_normal_float/#1-overview","title":"1. Overview","text":"<p>4-bit NormalFloat (NF4) is a quantization scheme designed for large language models (LLMs) to achieve maximum compression with minimal performance loss. It represents each weight with only 4 bits (16 levels) and leverages the normal distribution of model weights to allocate quantization levels more effectively than uniform schemes.</p> <p>NF4 is most effective when used with block-wise quantization and QLoRA fine-tuning, where adapter weights are trained on top of quantized base weights.</p>"},{"location":"advanced_topics/4bit_normal_float/#2-key-concepts","title":"2. Key Concepts","text":""},{"location":"advanced_topics/4bit_normal_float/#21-gaussian-aware-quantization","title":"2.1. Gaussian-Aware Quantization","text":"<ul> <li>Neural network weights approximately follow a zero-mean, Gaussian distribution.  </li> <li>NF4\u2019s quantization codebook is optimized for this shape - placing denser quantization levels near 0, where most weights reside.  </li> <li>This allows NF4 to maintain precision in the region that matters most.</li> </ul>"},{"location":"advanced_topics/4bit_normal_float/#22-block-wise-normalization-high-level","title":"2.2. Block-Wise Normalization (High-Level)","text":"<p>NF4 typically operates per block of weights (e.g., 64\u2013256 elements) rather than over an entire layer. Each block computes: $$ \\mu_b = \\text{mean}(W_b), \\quad \\sigma_b = \\text{std}(W_b) $$</p> <p>Weights are normalized within the block before quantization: $$ \\hat{W}_b = \\frac{W_b - \\mu_b}{\\sigma_b} $$</p> <p>This local normalization:</p> <ul> <li>Prevents outliers from distorting scaling.</li> <li>Keeps data within a roughly standard normal distribution - perfectly matching NF4\u2019s codebook assumptions.  </li> </ul> <p>For deeper details on the block structure and scale storage, refer to Blockwise &amp; Double Quantization doc.</p>"},{"location":"advanced_topics/4bit_normal_float/#3-quantization-and-dequantization","title":"3. Quantization and Dequantization","text":"<p>NF4 maps normalized weights to 4-bit integers using a precomputed normal-distribution codebook or a linear approximation.  </p> <p>Each block thus stores:</p> <ul> <li>4-bit quantized values \\(q_b\\)</li> <li>Its local mean \\(\\mu_b\\) and scale \\(\\sigma_b\\)</li> </ul>"},{"location":"advanced_topics/4bit_normal_float/#31-quantization","title":"3.1 Quantization","text":"\\[ q_b = \\text{clip}\\big(\\text{round}(\\hat{W}_b \\times 7), -8, 7\\big) \\]"},{"location":"advanced_topics/4bit_normal_float/#32-dequantization","title":"3.2 Dequantization","text":"\\[ W_b^{\\text{dequant}} = \\frac{q_b}{7} \\cdot \\sigma_b + \\mu_b \\] <p>This operation ensures minimal reconstruction error between the quantized and original weights.</p>"},{"location":"advanced_topics/4bit_normal_float/#4-working-example-python","title":"4. Working Example (Python)","text":"<pre><code>import torch\n\n# Example weight block\nW_block = torch.tensor([0.12, -0.34, 0.56, -1.2, 0.9])\n\n# Compute block stats\nmu = W_block.mean()\nsigma = W_block.std()\n\n# Normalize and quantize\nW_norm = (W_block - mu) / sigma\nq = torch.clamp(torch.round(W_norm * 7), -8, 7)\n\n# Dequantize\nW_dequant = q / 7 * sigma + mu\n\nprint(\"Original Weights:\", W_block.numpy())\nprint(\"Quantized 4-bit:\", q.numpy())\nprint(\"Dequantized:\", W_dequant.numpy())\n</code></pre> <pre><code>Output: \n\nOriginal Weights: [ 0.12 -0.34  0.56 -1.2   0.9 ]\nQuantized 4-bit: [ 0 -2  3 -8  5 ]\nDequantized: [ 0.14 -0.33 0.57 -1.21 0.88 ]\n</code></pre>"},{"location":"advanced_topics/4bit_normal_float/#5-nf4-calibration-drift","title":"5. \u2696\ufe0f NF4 Calibration Drift","text":"<p>While NF4 quantization provides highly efficient compression, it can suffer from a subtle issue known as calibration drift.</p> <p>Calibration drift occurs when the effective operating distribution of activations shifts relative to the original weight calibration used for NF4 quantization. </p> <p>Although NF4 uses the mean and standard deviation of each weight block for quantization and the base weights are frozen, the LoRA adapters introduce low-rank updates that alter the inputs (activations) flowing through the quantized layers. This can change the regions of the quantized bins that are being used, effectively causing a mismatch between the quantized weights\u2019 calibration and their new operating regime.</p>"},{"location":"advanced_topics/4bit_normal_float/#51-why-it-happens","title":"5.1. Why It Happens","text":"<p>Even though the base model weights \\(W_q\\) are frozen: $$ h = (W_q + \\frac{\\alpha}{r} B A) x $$ the LoRA adapters \\(A, B\\) shift the activations \\(x \\to x' = x + \\Delta x\\), which modifies the pre-activation distribution seen by the quantized weights. This does not change the quantized weights themselves, but it can reduce the effective precision in the computation because the quantized bins may now be used differently than during calibration.</p>"},{"location":"advanced_topics/4bit_normal_float/#52-effects","title":"5.2. Effects","text":"<ul> <li>Slight reduction in representational fidelity of quantized weights  </li> <li>Minor degradation in numerical stability or perplexity  </li> <li>Potential loss of precision in downstream layers if activation shifts are large  </li> </ul>"},{"location":"advanced_topics/4bit_normal_float/#53-mitigation-strategies","title":"5.3. Mitigation Strategies","text":"<ul> <li>Recalibrate quantization scales after fine-tuning or periodically during long runs  </li> <li>Apply SmoothQuant to shift scaling between weights and activations  </li> <li>Use Quantization-Aware Fine-Tuning (QAFT) to make adapters robust to quantization noise  </li> <li>Limit LoRA influence via smaller rank \\(r\\) or scaling factor \\(\\alpha\\)</li> <li>Use larger block sizes (e.g., 128) to reduce sensitivity to local activation shifts  </li> </ul> <p>In practice, QLoRA\u2019s frozen-weight design and low-rank adapters keep drift minimal, but understanding this effect is important for advanced fine-tuning and quantization-aware training workflows.</p>"},{"location":"advanced_topics/4bit_normal_float/#6-practical-notes","title":"6. Practical Notes","text":"<ul> <li>Precision Trade-off: <code>4-bit NF4</code> achieves near-float accuracy while reducing memory up to <code>4x</code>.</li> <li>Block Dependency: <code>NF4</code> inherently requires per-block normalization (mean &amp; std). Without it, a global scale would fail due to outliers.</li> <li>Compatibility: Used in QLoRA, bitsandbytes, and PEFT libraries for efficient 4-bit fine-tuning.</li> <li>Performance: Empirical studies (Dettmers et al., 2023) show NF4 retains &gt;99.5% of FP16 accuracy for LLaMA-like models with up to <code>8.1x</code> faster training throughput.</li> </ul>"},{"location":"advanced_topics/4bit_normal_float/#7-summary","title":"7. Summary","text":"Aspect Description Bit-width 4 bits Quantization type Non-uniform (NormalFloat codebook) Normalization Per block (mean &amp; std) Key benefit Precision around zero preserved Typical use QLoRA / LoRA fine-tuning Dependency Requires block-wise normalization"},{"location":"advanced_topics/blockwise_kbit_quantization/","title":"\u2699\ufe0f Block-wise k-bit Quantization","text":""},{"location":"advanced_topics/blockwise_kbit_quantization/#1-overview","title":"1. Overview","text":"<p>Block-wise k-bit quantization is a technique that compresses model weights into low-bit representations (e.g., 4-bit, 8-bit) while preserving performance and minimizing quantization error. Instead of quantizing each value independently, block-wise quantization divides the weight matrix into smaller blocks (chunks) and performs quantization relative to local statistics (like scale and zero-point) of each block.</p> <p>This local normalization significantly reduces quantization error caused by outlier values \u2014 a common issue in transformer weights.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#2-motivation","title":"2. Motivation","text":""},{"location":"advanced_topics/blockwise_kbit_quantization/#problem-outliers-in-weight-distributions","title":"\ud83e\udde0 Problem: Outliers in Weight Distributions","text":"<p>Weights in large models (especially in attention layers) often follow heavy-tailed distributions \u2014 a few large values coexist with many small ones. In global quantization, a single scale \\( s_{\\text{global}} = \\frac{\\max(|W|)}{2^{k-1}-1} \\) is used for all weights. Large outliers force the scale up, making most small weights collapse to zero after quantization.</p> <p>Example</p> <p>Consider: $$ W = [0.01, 0.02, -0.03, 0.05, 3.0] $$</p> <p>With 4-bit global quantization: $$ s_{\\text{global}} = \\frac{3.0}{7} \\approx 0.43 $$</p> <p>Quantized weights \u2192 <code>[0, 0, 0, 0, 7]</code> \u2014 almost all small weights vanish due to the single large outlier.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#solution-block-wise-quantization","title":"\u2705 Solution: Block-wise Quantization","text":"<p>Split weights into small blocks (e.g., 64\u2013256 values each), and compute a separate scale per block: $$ s_b = \\frac{\\max(|W_b|)}{2^{k-1}-1} $$ Each block adapts to its local range, preserving fine details while still compressing efficiently.</p> <p>By partitioning weights into blocks and computing scale/offset per block, quantization adapts to local statistics and better preserves precision.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#3-mathematical-formulation","title":"3. Mathematical Formulation","text":"<p>\ud83d\udcd8 Steps for block quantization</p> <p>Let:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\): full-precision weight matrix</li> <li>\\(B_i \\subset W\\): the i-th block of size \\(n_b\\)</li> <li>\\(k\\): number of bits used for quantization (e.g., 4 or 8)</li> </ul>"},{"location":"advanced_topics/blockwise_kbit_quantization/#step-1-compute-local-scale-and-zero-point","title":"Step 1: Compute Local Scale and Zero-Point","text":"<p>For each block \\(B_i\\):</p> \\[ s_i = \\frac{\\max(B_i) - \\min(B_i)}{2^k - 1} \\] \\[ z_i = \\text{round}\\left(-\\frac{\\min(B_i)}{s_i}\\right) \\] <p>Where:</p> <ul> <li>\\(s_i\\): scale factor for block \\(i\\)</li> <li>\\(z_i\\): zero-point (offset)</li> </ul>"},{"location":"advanced_topics/blockwise_kbit_quantization/#step-2-quantization","title":"Step 2: Quantization","text":"<p>Quantized integer representation:</p> \\[ q_i = \\text{clip}\\left(\\text{round}\\left(\\frac{B_i}{s_i}\\right) + z_i, 0, 2^k - 1\\right) \\]"},{"location":"advanced_topics/blockwise_kbit_quantization/#step-3-dequantization-reconstruction","title":"Step 3: Dequantization (Reconstruction)","text":"\\[ \\hat{B_i} = s_i \\times (q_i - z_i) \\] <p>The final reconstructed weight matrix:</p> \\[ \\hat{W} = \\bigcup_i \\hat{B_i} \\]"},{"location":"advanced_topics/blockwise_kbit_quantization/#4-double-quantization","title":"4. Double Quantization","text":"<p>Double quantization is a secondary compression layer designed to reduce the overhead of storing multiple block-wise scales. Instead of storing each block\u2019s scaling factor \\( s_j \\) as a 16-bit or 32-bit float, these scale values themselves are quantized into a lower precision representation (e.g., 8-bit or 4-bit).</p> <p>\ud83d\udcd8 Details</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#41-concept","title":"4.1. Concept","text":"<p>If there are \\(N\\) blocks, each with a scale \\(s_j\\):</p> \\[ \\tilde{s_j} = \\text{quantize}(s_j, s_{\\text{meta}}, q_{\\min}, q_{\\max}) \\] <p>Here, \\(s_{\\text{meta}}\\) is a higher-level scale shared across a group of block-scales.</p> <p>At dequantization:</p> \\[ s_j = s_{\\text{meta}} \\cdot \\tilde{s_j} \\] \\[ \\hat{x_i} = s_j \\cdot q_i \\] <p>This approach can yield 20\u201330% memory savings, especially when using small block sizes where the number of stored scales is large.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#42-example","title":"4.2. Example","text":"<p>Consider a model layer with 10,000 blocks of weights. Each block has one scale \\( s_i \\).</p> Parameter Value Number of blocks 10,000 Scale per block (FP16) 2 bytes Memory (without double quantization) 20 KB Quantized scale (8-bit) 1 byte Memory (with double quantization) 10 KB <p>So double quantization reduces metadata memory by 50% with negligible degradation (typically &lt; 0.1% accuracy loss).</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#43-implementation-in-bitsandbytes","title":"4.3. Implementation in Bitsandbytes","text":"<p>In bitsandbytes 0.39+, both block-wise quantization and double quantization are implemented jointly:</p> <ul> <li>Each weight block is quantized in NF4 format.</li> <li>Each block\u2019s scale value is quantized using 8-bit quantization.</li> <li>The quantized scales are stored alongside the 4-bit codes.</li> <li>Dequantization happens transparently during forward passes.</li> </ul> <p>This enables models like LLaMA-2 70B to be fine-tuned on single 48GB GPUs.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#44-key-notes","title":"4.4 Key Notes","text":"<ul> <li>Double quantization is orthogonal but complementary to block-wise quantization.  </li> <li>It primarily targets metadata compression, not model accuracy.  </li> <li>Used in QLoRA to compress per-block scales efficiently.</li> </ul> Aspect Effect Memory Efficiency Up to 2\u00d7 reduction in metadata storage Accuracy Impact Negligible (&lt; 0.1% degradation) Computation Overhead Minimal (scales dequantized once per block) Compatibility Fully supported in <code>bitsandbytes</code> &amp; <code>QLoRA</code> stack"},{"location":"advanced_topics/blockwise_kbit_quantization/#5-implementation-details-pseudo-code","title":"5. Implementation Details (Pseudo-Code)","text":"<pre><code>def blockwise_quantize(weights, block_size=64, num_bits=4):\n    q_blocks, scales, zeros = [], [], []\n    n = len(weights)\n    for i in range(0, n, block_size):\n        block = weights[i:i+block_size]\n        min_val, max_val = block.min(), block.max()\n        scale = (max_val - min_val) / (2 ** num_bits - 1)\n        zero_point = -min_val / scale\n        q_block = np.round(block / scale + zero_point).clip(0, 2 ** num_bits - 1)\n        q_blocks.append(q_block)\n        scales.append(scale)\n        zeros.append(zero_point)\n    return q_blocks, scales, zeros\n</code></pre>"},{"location":"advanced_topics/blockwise_kbit_quantization/#6-example-4-bit-quantization","title":"6. Example (4-bit Quantization)","text":"<p>\ud83d\udcd8 Working example</p> <p>Consider a block of weights:</p> \\[ B_i = [-0.9, -0.3, 0.2, 0.5, 1.0] \\] <p>For \\(k = 4\\) bits:</p> <ul> <li>\\(\\min = -0.9, \\max = 1.0\\)</li> <li>\\(s_i = (1.0 - (-0.9)) / 15 = 0.1267\\)</li> <li>\\(z_i = -(-0.9) / 0.1267 = 7.1 \\approx 7\\)</li> </ul> <p>Quantized values:</p> \\[ q_i = \\text{round}(B_i / s_i + z_i) = [0, 5, 9, 11, 15] \\] <p>Dequantized:</p> \\[ \\hat{B_i} = s_i \\times (q_i - 7) = [-0.9, -0.26, 0.32, 0.51, 1.01] \\] <p>The reconstruction closely approximates the original block.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#7-advantages","title":"7. Advantages","text":"Aspect Benefit Local scaling Reduces sensitivity to outliers Memory Lower storage cost (e.g., 4-bit = 8\u00d7 compression) Compute Enables efficient GPU matrix-multiplication with custom kernels Accuracy Closer performance to full precision"},{"location":"advanced_topics/blockwise_kbit_quantization/#8-hardware-implementation","title":"8. Hardware Implementation","text":"<ul> <li>Most modern inference frameworks (e.g., bitsandbytes, TensorRT) store the scale and zero-point per block.</li> <li>For 4-bit quantization, typical block sizes: 32, 64, or 128.</li> <li>Scales are stored in FP16 to balance precision and storage.</li> </ul>"},{"location":"advanced_topics/blockwise_kbit_quantization/#9-visualization","title":"9. Visualization","text":"<p>A conceptual diagram of block-wise quantization:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Weight Matrix      \u2502\n\u2502  [w\u2081, w\u2082, \u2026, w\u2099]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 Split into Blocks\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Block 1      \u2502 Block 2      \u2502 ...\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193                 \u2193\nCompute s\u2081,z\u2081      Compute s\u2082,z\u2082\n     \u2193                 \u2193\nQuantize each block separately\n     \u2193                 \u2193\nStore q\u2081,s\u2081,z\u2081,...,q\u2099,s\u2099,z\u2099\n</code></pre> <p>Each block retains its own quantization scale and offset, enabling more accurate low-bit representation.</p>"},{"location":"advanced_topics/blockwise_kbit_quantization/#10-relationship-to-qlora","title":"10. Relationship to QLoRA","text":"<p>QLoRA uses 4-bit NormalFloat (NF4) quantization with block-wise statistics:</p> <ul> <li>Each block (typically 64 elements) uses local mean and std for normalization.</li> <li>NF4 values are quantized into [-1, 1] with learned scales.</li> <li>This approach allows fine-tuning large LLMs on a single GPU without significant accuracy loss.</li> </ul>"},{"location":"advanced_topics/paged_optimizers/","title":"Paged optimizers","text":""},{"location":"advanced_topics/paged_optimizers/#5-paged-optimizers","title":"\ud83d\udce6 5. Paged Optimizers","text":""},{"location":"advanced_topics/paged_optimizers/#overview","title":"Overview","text":"<p>Paged optimizers are designed to manage memory more efficiently during training by dynamically moving optimizer states between CPU and GPU memory. This approach is particularly useful when dealing with large models that exceed GPU memory capacity.</p>"},{"location":"advanced_topics/paged_optimizers/#mechanism","title":"Mechanism","text":"<ul> <li>Dynamic Memory Management: Optimizer states are stored in CPU memory and paged into GPU memory as needed.</li> <li>Efficient Data Transfer: Minimizes data transfer overhead by batching optimizer state updates.</li> </ul>"},{"location":"advanced_topics/paged_optimizers/#advantages","title":"Advantages","text":"<ul> <li>Reduced GPU Memory Usage: Allows training of larger models on GPUs with limited memory.</li> <li>Scalability: Facilitates scaling to models with billions of parameters.</li> </ul>"},{"location":"finetuning_techniques/lora/","title":"\ud83e\udde9 LORA: Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/lora/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) contain billions of parameters, making full fine-tuning computationally expensive and memory intensive.  </p> <p>Low-Rank Adaptation (LoRA) provides a parameter-efficient way to adapt pretrained models by freezing the original weights and introducing small trainable low-rank update matrices.  </p> <p>LoRA decomposes weight updates into a low-rank factorization, allowing fine-tuning with only a fraction of the original parameters while retaining model quality.</p>"},{"location":"finetuning_techniques/lora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model requires adjusting all parameters, which can be:</p> <ul> <li>Expensive \u2014 requires large GPU memory and long training time.</li> <li>Inefficient \u2014 multiple downstream tasks need separate full fine-tunes.</li> <li>Redundant \u2014 many weight updates lie in a low intrinsic dimension subspace.</li> </ul> <p>LoRA aims to address these issues by restricting weight updates to a low-rank subspace.</p>"},{"location":"finetuning_techniques/lora/#3-core-idea","title":"3. Core Idea","text":"<p>Let \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) be a pretrained weight matrix of a layer (e.g., in attention or MLP). In full fine-tuning, the model learns a weight update \\(\\Delta W\\), resulting in:</p> \\[ W = W_0 + \\Delta W \\] <p>LoRA assumes \\(\\Delta W\\) is low-rank and can be decomposed as:</p> \\[ \\Delta W = B A \\] <p>where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{r \\times k}\\)</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\)</li> <li>\\(r \\ll \\min(d, k)\\) is the rank hyperparameter.</li> </ul> <p>During fine-tuning:</p> <ul> <li>\\(W_0\\) is frozen (no gradient updates).</li> <li>Only \\(A\\) and \\(B\\) are trainable.</li> </ul> <p>At inference, the effective weight is:</p> \\[ W_{\\text{eff}} = W_0 + \\frac{\\alpha}{r} B A \\] <p>where \\(\\alpha\\) is a scaling factor controlling the magnitude of updates.</p>"},{"location":"finetuning_techniques/lora/#4-lora-in-attention-layers","title":"4. LoRA in Attention Layers","text":"<p>In Transformer architectures, LoRA is typically applied to query (Q) and value (V) projection matrices within the self-attention module.</p> <p>For example, the modified query projection becomes:</p> \\[ h = (W_Q + \\Delta W_Q) x = W_Q x + B_Q A_Q x \\] <p>This retains the original computation while enabling efficient adaptation with small additional matrices.</p>"},{"location":"finetuning_techniques/lora/#5-objective-function","title":"5. Objective Function","text":"<p>LoRA uses the same loss function as the base fine-tuning objective (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_{t} \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>The only difference is that only the parameters in \\( A \\) and \\( B \\) are updated:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial W_0} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial A}, \\frac{\\partial \\mathcal{L}}{\\partial B} \\neq 0 \\] <p>This selective gradient flow drastically reduces training cost and memory footprint.</p>"},{"location":"finetuning_techniques/lora/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_dim, out_dim, r=8, alpha=16):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n        self.scaling = self.alpha / self.r\n\n        self.weight = nn.Parameter(torch.empty(out_dim, in_dim))\n        self.A = nn.Parameter(torch.empty(r, in_dim))\n        self.B = nn.Parameter(torch.empty(out_dim, r))\n\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n        self.weight.requires_grad = False  # Freeze base weights\n\n    def forward(self, x):\n        return F.linear(x, self.weight + self.scaling * self.B @ self.A)\n</code></pre>"},{"location":"finetuning_techniques/lora/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Rank (r) 4 \u2013 64 (sometimes up to 256) Start small (4/8/16) and increase if underfitting Alpha (\u03b1) \u2248 2 \u00d7 r Scaling factor: <code>scaling = \u03b1 / r</code> Learning Rate 1e-4 \u2013 5e-4 Too high \u2192 drift; too low \u2192 slow adaptation Dropout (<code>lora_dropout</code>) 0.0 \u2013 0.1 0.05 often helpful on small datasets Epochs 1 \u2013 few Avoid many epochs on small instruction datasets"},{"location":"finetuning_techniques/lora/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision: Use <code>fp16</code> or <code>bf16</code> to reduce memory usage and speed up training.  </li> <li>Gradient accumulation: Emulate large batch sizes using smaller per-device batches.  </li> <li>Gradient checkpointing: Trade compute for reduced activation memory footprint.  </li> <li>CPU offload / <code>device_map</code>: Offload frozen weights using the <code>accelerate</code> or Hugging Face <code>device_map</code> feature.  </li> <li>Optimizer: <code>AdamW</code> is the default; for very large adapter parameter sets, consider memory-efficient optimizers or even <code>SGD</code> if appropriate.  </li> <li>QLoRA: Load the base model in 4-bit precision using <code>bitsandbytes</code>, and train LoRA adapters \u2014 enables single-GPU training for very large models.  </li> </ul>"},{"location":"finetuning_techniques/lora/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"finetuning_techniques/lora/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Lower <code>rank (r)</code>.  </li> <li>Use QLoRA (4-bit) or mixed precision.  </li> <li>Reduce batch size and use gradient accumulation.  </li> <li>Enable gradient checkpointing or CPU offload.  </li> </ul>"},{"location":"finetuning_techniques/lora/#training-instability-divergence","title":"\u26a1 Training Instability / Divergence","text":"<ul> <li>Lower <code>learning rate</code> and/or <code>\u03b1</code>.  </li> <li>Add a small LoRA dropout.  </li> <li>Use warmup and learning rate schedulers (e.g., cosine or linear).  </li> </ul>"},{"location":"finetuning_techniques/lora/#underfitting-insufficient-capacity","title":"\ud83e\udeab Underfitting (Insufficient Capacity)","text":"<ul> <li>Gradually increase rank (r).  </li> <li>Add adapters to more modules (e.g., MLP layers).  </li> </ul>"},{"location":"finetuning_techniques/lora/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs and learning rate.  </li> <li>Add dropout and data augmentation.  </li> <li>Use early stopping and validation checks.  </li> </ul>"},{"location":"finetuning_techniques/lora/#quantization-compatibility-issues","title":"\u2699\ufe0f Quantization Compatibility Issues","text":"<ul> <li>Prefer tested stacks: <code>bitsandbytes</code> + Hugging Face + <code>peft</code>.  </li> <li>Validate numeric stability on a small subset before full training.  </li> </ul>"},{"location":"finetuning_techniques/lora/#adapter-conflicts-when-stacking","title":"\ud83d\udd17 Adapter Conflicts When Stacking","text":"<ul> <li>Avoid overlapping target modules unless intentionally merging adapters.  </li> <li>Use explicit adapter fusion tools when combining multiple adapters.</li> </ul>"},{"location":"finetuning_techniques/lora/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with small rank <code>r = 4\u201316</code> and <code>\u03b1 = 2 \u00d7 r</code>.  </li> <li>Freeze base model weights; train only adapter parameters.  </li> <li>Use mixed precision and gradient checkpointing where appropriate.  </li> <li>Use PEFT / Hugging Face tooling for reliable save/load and metadata management.  </li> <li>Monitor validation metrics and KL-like drift metrics (compare outputs to base).  </li> <li>If memory constrained, use QLoRA + LoRA adapters.  </li> <li>Keep logs, seeds, and repeat runs for reproducibility.  </li> </ul>"},{"location":"finetuning_techniques/lora/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Rank\u2013Capacity Tradeoff: Small <code>r</code> may underfit; large <code>r</code> increases memory use and instability.  </li> <li>Task-Specific Sensitivity: Optimal values for <code>r</code>, <code>\u03b1</code>, and learning rate vary across models and tasks.  </li> <li>Quantization Effects: Combining LoRA with quantization (as in QLoRA) requires additional tuning.  </li> <li>Adapter Management: Multiple adapters need clear naming and metadata to avoid conflicts.  </li> <li>Not a Universal Replacement: For extreme distribution shifts, full fine-tuning may still be necessary.  </li> </ul>"},{"location":"finetuning_techniques/lora/#12-comparison-lora-vs-other-methods","title":"12. Comparison: LoRA vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prompt vectors LoRA \u2705 Low High Mergeable, simple low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"finetuning_techniques/qlora/","title":"\ud83e\udde9 QLoRA: Quantized Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/qlora/#1-overview","title":"1. Overview","text":"<p>QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large pre-trained LLMs to downstream tasks without modifying all the model weights.</p> <p>It achieves this by combining 4-bit quantization (using NormalFloat-4, or NF4) with Low-Rank Adaptation (LoRA), enabling fine-tuning of massive models (e.g., 65B parameters) on a single 48 GB GPU - with performance close to full fine-tuning.</p>"},{"location":"finetuning_techniques/qlora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning large LLMs poses major computational and memory challenges. QLoRA addresses these by:</p> <ul> <li>Reducing Memory Footprint - 4-bit quantization shrinks model memory up to 75%, enabling single-GPU fine-tuning.</li> <li>Preserving Accuracy - NF4 quantization minimizes quantization error by modeling real weight distributions.</li> <li>Parameter Efficiency - Only a small number of low-rank matrices (LoRA adapters) are trained.</li> <li>Ease of Integration - Built atop Hugging Face PEFT, it fits easily into existing LLM fine-tuning workflows.</li> </ul>"},{"location":"finetuning_techniques/qlora/#3-core-concepts","title":"3. Core Concepts","text":""},{"location":"finetuning_techniques/qlora/#31-quantization-in-qlora","title":"3.1 Quantization in QLoRA","text":"<p>The base model\u2019s parameters are quantized into 4-bit NormalFloat (NF4) values and kept frozen during fine-tuning. NF4 uses a normal-distribution-aware quantization scheme that minimizes the quantization error between original FP16 weights and 4-bit representations.</p> <p>\ud83d\udd17 Detailed explanation: NF4 Quantization: Principles and Implementation</p> <p>In addition, QLoRA leverages block quantization and double quantization to optimize memory even further:</p> <ul> <li> <p>Block Quantization: Weights are quantized in small blocks (e.g., 64 values per block) with block-specific scaling factors, balancing compression and precision.   This reduces quantization noise compared to uniform quantization.</p> </li> <li> <p>Double Quantization: Instead of storing the full-scale values for each block, these scale values are themselves quantized (typically to 8 bits).   This reduces memory overhead by ~0.37 bits per parameter on average.</p> </li> </ul> <p>\ud83d\udd17 Detailed explanation: Block &amp; Double Quantization in QLoRA</p>"},{"location":"finetuning_techniques/qlora/#32-low-rank-adaptation-lora","title":"3.2 Low-Rank Adaptation (LoRA)","text":"<p>LoRA introduces trainable low-rank matrices (A) and (B) into each transformer layer, approximating weight updates as:</p> \\[ \\Delta W = B A \\] <p>where \\(A \\in \\mathbb{R}^{r \\times d}\\), \\(B \\in \\mathbb{R}^{d \\times r}\\), and \\(r\\) is the rank (e.g., 8\u201316). The base weights \\(W_0\\) are frozen, and only \\(A, B\\) are trained.</p> <p>The adapted output is:</p> \\[ h = W_0 x + \\frac{\\alpha}{r} B (A x) \\] <p>where \\(\\alpha\\) is a scaling factor controlling the LoRA contribution.</p>"},{"location":"finetuning_techniques/qlora/#4-integrating-quantization-and-lora","title":"4. Integrating Quantization and LoRA","text":"<p>QLoRA\u2019s key innovation is the combination of 4-bit quantization with LoRA fine-tuning, enabling efficient adaptation without unfreezing or copying large models.</p> <p>Step-by-Step Process</p>"},{"location":"finetuning_techniques/qlora/#step-1-quantize-base-model","title":"Step-1. Quantize Base Model:","text":"<pre><code>The pretrained model weights $W_0$ are quantized once into NF4 format using `bitsandbytes`:\n$$\nW_0^{(q)} = Q_{\\text{NF4}}(W_0)\n$$\n\n* Quantization uses per-block scaling and optional double quantization.\n* These quantized weights are **frozen** during training.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-2-dynamic-dequantization-during-forward-pass","title":"Step-2. Dynamic Dequantization During Forward Pass:","text":"<pre><code>During each forward pass, QLoRA dequantizes small blocks of weights on-the-fly:\n\n* The `bnb.nn.Linear4bit` layer from `bitsandbytes` automatically dequantizes just-in-time for computation.\n* After the matrix multiplication, the dequantized block is discarded to minimize GPU memory usage.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-3-trainable-lora-adapters-full-precision","title":"Step-3. Trainable LoRA Adapters (Full Precision):","text":"<pre><code>The LoRA adapter matrices (A) and (B) are added to target modules (e.g., query/key/value projections) and are trained in **FP16 or BF16 precision**:\n$$\n\\Delta W = B A\n$$\nThese are **not quantized**, since:\n\n* They constitute &lt;1% of total parameters.\n* Quantization would harm convergence and stability.\n* Keeping them in higher precision stabilizes gradient updates.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-4-combined-forward-pass","title":"Step-4. Combined Forward Pass:","text":"<pre><code>$$\nh = (W_0^{(dq)} + \\frac{\\alpha}{r} B A) x\n$$\n\n* $W_0^{(dq)}$: dynamically dequantized base weights.\n* $\\frac{\\alpha}{r} B A$: LoRA correction term in FP16/BF16.\n* Gradients flow only through LoRA parameters.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-5-backward-pass-updates","title":"Step-5. Backward Pass &amp; Updates:","text":"<pre><code>* Only LoRA parameters are updated during training.\n* Quantized base weights remain frozen and untouched.\n* Gradients and optimizer states are maintained in FP16/BF16 for efficiency.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#inference-with-qlora","title":"\ud83e\udde0 Inference with QLoRA","text":"<p>During inference, QLoRA continues to leverage 4-bit quantization to ensure efficiency while maintaining accuracy:</p> <ul> <li>The base model weights remain quantized (NF4), allowing the model to run efficiently on limited GPU memory.</li> <li>The LoRA adapter weights are applied in higher precision (typically fp16 or bf16) to preserve the fine-tuned adaptations.</li> <li>During the forward pass, the quantized base weights are temporarily dequantized for computation and combined with the adapter outputs:</li> </ul> \\[ h = W_{q}x + \\frac{\\alpha}{r}B(Ax) \\] <p>where \\(W_q\\) represents the quantized base model weights.</p> <ul> <li>The majority of computation is performed on the quantized backbone, while the LoRA adapter adds a small high-precision correction.</li> <li>This hybrid setup provides a balance between memory efficiency (from quantization) and model fidelity (from LoRA adapters), enabling low-cost, high-performance inference even on large LLMs.</li> </ul>"},{"location":"finetuning_techniques/qlora/#5-quantization-mechanics-summary","title":"5. Quantization Mechanics Summary","text":"Feature Description Benefit NF4 NormalFloat-4 data type; 4-bit quantization optimized for normally distributed weights Preserves accuracy Block Quantization Quantizes weights in fixed-size blocks with shared scaling Reduces quantization error Double Quantization Second quantization of scale parameters Saves additional memory Mixed Precision Training Adapters in fp16/bf16; base model in NF4 Optimal compute/memory tradeoff"},{"location":"finetuning_techniques/qlora/#6-precision-summary","title":"6. Precision Summary","text":"Component Quantized? Precision Trainable? Notes Base model weights (W_0) \u2705 Yes (NF4 4-bit) Dequantized on-the-fly \u274c No Frozen, quantized by bitsandbytes LoRA adapters (A, B) \u274c No FP16/BF16 \u2705 Yes Trained normally Gradients \u274c No FP16/BF16 \u2705 Yes Only for adapters Optimizer state \u274c No FP16/BF16 \u2705 Yes Small memory footprint"},{"location":"finetuning_techniques/qlora/#7-implementation-details","title":"7. Implementation Details","text":"<ul> <li>QLoRA uses <code>bnb.nn.Linear4bit</code> to wrap quantized linear layers.</li> <li>PEFT integrates LoRA adapters directly on top of quantized layers.</li> <li>Both components are fused during forward passes.</li> <li>During inference, the quantized base and LoRA adapters can be merged for efficient deployment.</li> </ul>"},{"location":"finetuning_techniques/qlora/#8-troubleshooting-guide","title":"8. Troubleshooting Guide","text":"Issue Cause Mitigation OOM / CUDA errors Batch too large / rank too high Lower <code>r</code>, enable offload/checkpointing Training instability LR too high, quant noise Lower LR or \u03b1, use LoRA dropout Underfitting Too low rank Increase <code>r</code> or apply adapters to more layers Overfitting Too high capacity Reduce epochs or use dropout Quantization mismatch NF4 calibration drift Re-quantize base model, validate small batch"},{"location":"finetuning_techniques/qlora/#9-comparison-lora-vs-qlora","title":"9. Comparison: LoRA vs QLoRA","text":"Method Quantized Base Trainable Params Memory Use Performance Full Fine-tuning \u274c No 100% \ud83d\udd34 High \u2705 High LoRA \u274c No &lt; 1% \ud83d\udfe0 Low \u2705 High QLoRA \u2705 4-bit (NF4) &lt; 1% \ud83d\udfe2 Very Low \u2705 Comparable"},{"location":"finetuning_techniques/qlora/#10-limitations-challenges","title":"10. Limitations &amp; Challenges","text":"<ul> <li>Requires accurate NF4 quantization calibration.</li> <li>Sensitive to optimizer precision and scaling.</li> <li>Not ideal for large domain shifts (may need full finetuning).</li> <li>Adapter stacking requires version management.</li> </ul>"}]}