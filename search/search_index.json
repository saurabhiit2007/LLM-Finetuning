{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"lora/","title":"LoRA (Low-Rank Adaptation) \u2014 A Practical Guide for Fine-tuning LLMs","text":"<p>A practical, hands-on guide for using LoRA adapters to fine-tune large language models (LLMs). This document compiles conceptual background, implementation recipes, hyperparameter heuristics, common pitfalls and solutions, and answers to frequently asked questions.</p>"},{"location":"lora/#table-of-contents","title":"Table of contents","text":"<ol> <li>Introduction</li> <li>Quick summary / TL;DR</li> <li>LoRA: the idea and math</li> <li>Where to apply LoRA in a Transformer</li> <li> <p>Implementation recipes</p> </li> <li> <p>Minimal PyTorch pseudocode</p> </li> <li>Hugging Face + PEFT example</li> <li>Working with quantized models (QLoRA)</li> <li>Hyperparameters &amp; heuristics (rank <code>r</code>, alpha, learning rate)</li> <li>Practical training configurations (memory, precision, optimizers)</li> <li>Common issues and concrete solutions</li> <li>Best practices &amp; checklist</li> <li>FAQ (ten targeted questions)</li> <li>Example <code>README.md</code> / GitHub Pages setup notes</li> <li>References &amp; further reading</li> </ol>"},{"location":"lora/#1-introduction","title":"1. Introduction","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient way to fine-tune large pretrained models by injecting small low-rank learnable matrices into existing weight projections instead of updating the entire model. LoRA reduces trainable parameters dramatically while preserving the pretrained weights \u2014 making it ideal when GPU memory or compute is constrained.</p> <p>This guide combines broadly useful LoRA knowledge with practical insights and caveats distilled from hundreds of experiments (notably, the \"Practical Tips for Finetuning LLMs Using LoRA\" writeup by Sebastian Raschka).</p>"},{"location":"lora/#2-quick-summary-tldr","title":"2. Quick summary / TL;DR","text":"<ul> <li>LoRA replaces full-weight updates \u0394W by a low-rank decomposition: \u0394W = A \u00b7 B, where <code>A</code> and <code>B</code> are small trainable matrices and the base weight <code>W</code> remains frozen.</li> <li>Typical ranks <code>r</code> are small (e.g., 4\u201364) for most use cases; increasing <code>r</code> increases capacity and memory usage.</li> <li>A commonly used heuristic is <code>alpha \u2248 2 \u00d7 r</code> (scaling factor), but it's worth tuning for large <code>r</code> values.</li> <li>QLoRA (quantized LoRA) reduces memory usage further (e.g., ~33%) at the cost of additional runtime (~39% slower in reported experiments) and more complex setup.</li> <li>Avoid na\u00efve multi-epoch training on small static instruction datasets \u2014 this often hurts performance (likely overfitting).</li> </ul>"},{"location":"lora/#3-lora-the-idea-and-math","title":"3. LoRA: the idea and math","text":"<p>Suppose a pretrained linear layer has weights <code>W \u2208 R^{d_out\u00d7d_in}</code>. Standard fine-tuning updates <code>W</code> directly (\u0394W). LoRA parametrizes the update as:</p> <pre><code>\u0394W = A @ B\n</code></pre> <p>where <code>A \u2208 R^{d_out\u00d7r}</code>, <code>B \u2208 R^{r\u00d7d_in}</code>, and <code>r &lt;&lt; min(d_out, d_in)</code>.</p> <p>During forward pass we compute:</p> <pre><code>y = W x + scaling * (A (B x))\n</code></pre> <p><code>scaling</code> is commonly <code>alpha / r</code>.</p> <p>This reduces stored trainable parameters from <code>d_out\u00d7d_in</code> to <code>(d_out + d_in)\u00d7r</code> and avoids storing large optimizer states for the frozen base model.</p>"},{"location":"lora/#4-where-to-apply-lora-in-a-transformer","title":"4. Where to apply LoRA in a Transformer","text":"<p>Most practitioners apply LoRA to the attention projection matrices because adapting attention often yields high returns:</p> <ul> <li>Query (<code>W_q</code>)</li> <li>Key (<code>W_k</code>)</li> <li>Value (<code>W_v</code>)</li> <li>Output projection (<code>W_o</code>)</li> </ul> <p>You can also enable LoRA on intermediate feed-forward (MLP) projections or projection layers between attention blocks. Enabling LoRA for more layers increases capacity (and memory) and frequently improves performance \u2014 e.g., enabling LoRA across all transformer layers may multiply the number of trainable parameters by ~5\u00d7 compared to only Q &amp; V.</p>"},{"location":"lora/#5-implementation-recipes","title":"5. Implementation recipes","text":""},{"location":"lora/#51-minimal-pytorch-style-lora-layer-pseudocode","title":"5.1 Minimal PyTorch-style LoRA layer (pseudocode)","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass LoRA(nn.Module):\n    def __init__(self, orig_linear: nn.Linear, r: int = 8, alpha: int = 16):\n        super().__init__()\n        self.orig = orig_linear\n        self.r = r\n        self.alpha = alpha\n        self.scaling = alpha / r\n        # LoRA: A and B (we follow convention B then A in many libs)\n        self.lora_B = nn.Parameter(torch.zeros(r, orig_linear.in_features))\n        self.lora_A = nn.Parameter(torch.zeros(orig_linear.out_features, r))\n        # init: small values\n        nn.init.kaiming_uniform_(self.lora_B, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_A)\n\n    def forward(self, x):\n        base = self.orig(x)\n        lora_update = (x @ self.lora_B.T) @ self.lora_A.T\n        return base + self.scaling * lora_update\n</code></pre> <p>Notes: this is illustrative. For batching and shapes ensure correct broadcasting. Many libraries (PEFT, LoRA implementations) implement optimizations to avoid extra copies and to fuse operations.</p>"},{"location":"lora/#52-hugging-face-peft-recommended","title":"5.2 Hugging Face + PEFT (recommended)","text":"<p><code>PEFT</code> (Parameter-Efficient Fine-Tuning) by Hugging Face is the de-facto standard for LoRA integration with Transformers. It abstracts away wiring adapters and saving/loading.</p> <p>Example outline (pseudo-commands):</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b\", torch_dtype=torch.float16)\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"],  # example names depending on model\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, config)\n\n# Then use Trainer or custom loop with model.parameters() (only LoRA params are trainable)\n</code></pre> <p>PEFT handles saving and loading adapters via <code>model.save_pretrained(path)</code> and <code>from_pretrained</code> utilities.</p>"},{"location":"lora/#6-hyperparameters-heuristics","title":"6. Hyperparameters &amp; heuristics","text":"<p>Rank <code>r</code></p> <ul> <li>Typical starting values: <code>r = 4, 8, 16</code> for small/medium tasks.</li> <li>For more complex adaptation or domain shift, try <code>r</code> up to <code>64</code> or <code>256</code> (if memory allows). In experiments, <code>r=256</code> sometimes outperforms smaller <code>r</code> for certain tasks, but be mindful of memory.</li> </ul> <p>Alpha (<code>\u03b1</code>) / scaling</p> <ul> <li>Heuristic: <code>alpha = 2 \u00d7 r</code> is a stable starting point.</li> <li>Compute <code>scaling = alpha / r</code>. If <code>alpha</code> is large relative to <code>r</code>, LoRA updates have a larger influence on outputs.</li> <li>Empirical note: <code>alpha = 2\u00d7r</code> is a good default, but at large <code>r</code> it may be worth trying smaller relative alpha (e.g., in experiments <code>r=256</code> with <code>alpha=128</code> gave strong results).</li> </ul> <p>Learning rate</p> <ul> <li>LoRA often works well with modest learning rates: <code>1e-4</code> to <code>5e-4</code> for many setups, but tune per-task.</li> <li>Smaller lr helps avoid overfitting / catastrophic drift from pretrained behavior.</li> </ul> <p>Dropout (lora_dropout)</p> <ul> <li>Small dropout (0.05) can help regularize LoRA weights on smaller datasets.</li> </ul>"},{"location":"lora/#7-practical-training-configurations","title":"7. Practical training configurations","text":"<p>Memory &amp; precision</p> <ul> <li>Use mixed precision (<code>fp16</code> or <code>bf16</code>) to reduce memory and speed up training. Most GPUs and Transformers support <code>torch_dtype=torch.float16</code> or <code>accelerate</code> flags.</li> <li>Consider gradient checkpointing to reduce activation memory at the cost of extra compute.</li> </ul> <p>Batch size &amp; gradient accumulation</p> <ul> <li>Small GPUs may need tiny per-device batch sizes; compensate with gradient accumulation to simulate larger batches.</li> </ul> <p>Optimizers</p> <ul> <li>AdamW is the default and works well for most <code>r</code> values.</li> <li>If <code>r</code> is small, switching from AdamW to SGD yields minimal memory savings. For large <code>r</code> (e.g., 256), SGD can reduce optimizer memory because Adam stores additional moment estimates.</li> </ul> <p>Epochs &amp; dataset iterations</p> <ul> <li>For static/small instruction datasets, multiple epochs can harm performance through overfitting. Consider single-epoch passes with thorough data curation, or heavy data augmentation / mixing diverse sources.</li> </ul>"},{"location":"lora/#8-common-issues-and-concrete-solutions","title":"8. Common issues and concrete solutions","text":"<p>This section enumerates common pitfalls and what to do about them.</p>"},{"location":"lora/#issue-gpu-oom-during-training","title":"Issue: GPU OOM during training","text":"<p>Solutions:</p> <ul> <li>Lower <code>r</code> (rank) for LoRA adapters.</li> <li>Use QLoRA (4-bit quantization) to reduce model memory footprint.</li> <li>Enable <code>torch.cuda.amp</code> mixed precision (<code>fp16</code>/<code>bf16</code>).</li> <li>Use gradient checkpointing.</li> <li>Reduce batch size and use gradient accumulation.</li> <li>Offload frozen model weights to CPU (Hugging Face <code>accelerate</code> device_map and <code>offload_folder</code> options).</li> </ul>"},{"location":"lora/#issue-choosing-r-incorrectly-underoverfitting","title":"Issue: Choosing <code>r</code> incorrectly (under/overfitting)","text":"<p>Solutions:</p> <ul> <li>Start small (e.g., 8) and increase if validation performance plateaus/underfits.</li> <li>If model capacity seems insufficient for the task, increase <code>r</code> incrementally. Monitor memory use.</li> </ul>"},{"location":"lora/#issue-divergence-instability-in-training","title":"Issue: Divergence / instability in training","text":"<p>Solutions:</p> <ul> <li>Lower learning rate.</li> <li>Reduce alpha or use <code>alpha = 2*r</code> heuristic.</li> <li>Add small dropout to LoRA layers.</li> <li>Use learning rate schedulers (cosine annealing / half-cycle often helps SGD more than Adam variants).</li> </ul>"},{"location":"lora/#issue-quantization-compatibility-lora-with-8-bit-4-bit","title":"Issue: Quantization compatibility (LoRA with 8-bit / 4-bit)","text":"<p>Solutions:</p> <ul> <li>Use libs designed for quantization + adapters (e.g., <code>bitsandbytes</code> + HuggingFace + PEFT). Many community recipes load base model in 4-bit, add LoRA using PEFT, and train with paged optimizers.</li> <li>Validate that numeric stability is acceptable; QLoRA tends to slightly increase runtime and may need hyperparameter tweaks.</li> </ul>"},{"location":"lora/#issue-overfitting-on-small-static-datasets","title":"Issue: Overfitting on small static datasets","text":"<p>Solutions:</p> <ul> <li>Use early stopping and validation monitoring.</li> <li>Mix diverse datasets or use data augmentation.</li> <li>Use smaller <code>r</code> and smaller lr; apply regularization.</li> <li>Avoid unnecessary multiple epochs on small instruction-only sets.</li> </ul>"},{"location":"lora/#issue-adapter-conflicts-when-stacking-multiple-lora-modules","title":"Issue: Adapter conflicts when stacking multiple LoRA modules","text":"<p>Solutions:</p> <ul> <li>Avoid enabling LoRA on the exact same submodule in conflicting adapters unless you plan adapter fusion.</li> <li>Use sequential or non-overlapping layer adaptation.</li> <li>Consider adapter fusion techniques if combining multiple task adapters.</li> </ul>"},{"location":"lora/#issue-savingloading-mismatches","title":"Issue: Saving/loading mismatches","text":"<p>Solutions:</p> <ul> <li>Prefer PEFT's save/load utilities (<code>model.save_pretrained(...)</code> and <code>PeftModel.from_pretrained(...)</code>) to manage adapter metadata and ensure correct wiring on load.</li> </ul>"},{"location":"lora/#9-best-practices-checklist","title":"9. Best practices &amp; checklist","text":"<ul> <li> Start with a small rank (<code>r = 4\u201316</code>) and <code>alpha = 2*r</code> as a baseline.</li> <li> Freeze base model weights; only LoRA params should be trainable.</li> <li> Use mixed precision and gradient checkpointing where possible.</li> <li> Use PEFT/Hugging Face tooling to manage adapters reliably.</li> <li> Monitor validation performance closely \u2014 be conservative with epoch count on small datasets.</li> <li> If memory-limited, try QLoRA and bitsandbytes-backed 4-bit loading.</li> <li> Keep training logs and seed multiple runs if you require reproducibility \u2014 results are generally consistent but still benefit from plotting.</li> </ul>"},{"location":"lora/#10-faq-answers-summarised","title":"10. FAQ (answers summarised)","text":"<p>Q1: How important is the dataset?</p> <ul> <li>Extremely. LoRA adapts model behavior based on the finetuning data. For instruction tuning and domain adaptation, richness and diversity of examples matter more than repeated passes over a small set.</li> </ul> <p>Q2: Does LoRA work for domain adaptation?</p> <ul> <li>Yes \u2014 LoRA is well-suited for domain adaptation because it allows task- or domain-specific lightweight adjustments without altering base knowledge.</li> </ul> <p>Q3: How do you select the best rank <code>r</code>?</p> <ul> <li>Start small and increase until validation performance and resource budget trade-offs are acceptable. Try <code>r</code> values like 8, 32, 64, 256 (if memory allows) to explore the curve.</li> </ul> <p>Q4: Does LoRA need to be enabled for all layers?</p> <ul> <li>Not strictly. Enabling LoRA on Q &amp; V is common and effective. Enabling it across more layers (all attention + projections) typically increases performance at the cost of memory.</li> </ul> <p>Q5: How to avoid overfitting?</p> <ul> <li>Use conservative epochs, regularization, smaller ranks/lr, mixed datasets, and early stopping. For instruction finetuning, multiple epochs on small datasets often degrade performance.</li> </ul> <p>Q6: What about optimizers?</p> <ul> <li>AdamW is a good default. Switch to SGD only if trying to save memory at large <code>r</code> values or if experimentation suggests benefits.</li> </ul> <p>Q7: What other factors influence memory usage?</p> <ul> <li>Model size, <code>r</code>, precision (<code>fp16</code>/<code>bf16</code> vs <code>fp32</code>), optimizer states, batch size, sequence length, gradient checkpointing, and whether you offload weights.</li> </ul> <p>Q8: How does LoRA compare to full finetuning and RLHF?</p> <ul> <li>LoRA is a parameter-efficient alternative to full finetuning (less memory, storage and faster). RLHF is a separate training paradigm (reward modeling and policy optimization) used to align models; LoRA is a mechanism to adapt parameters, not a replacement for RLHF.</li> </ul> <p>Q9: Can LoRA weights be combined?</p> <ul> <li>Yes \u2014 you can merge or sequentially apply adapters, and adapter fusion methods exist. Be careful about overlapping modifications to the same submodules.</li> </ul> <p>Q10: What about layer-wise optimal rank adaptation?</p> <ul> <li>It's plausible that different layers need different <code>r</code> values. Automated layer-wise rank selection is an active topic. A practical approach is to tune <code>r</code> for groups of layers or use heuristics (more adaptation for later layers may help in some tasks).</li> </ul>"},{"location":"lora/#11-example-readmemd-github-pages-setup-notes","title":"11. Example <code>README.md</code> / GitHub Pages setup notes","text":"<p>To turn this into a simple GitHub Page:</p> <ol> <li>Create a repository (public or private with Pages enabled).</li> <li>Add this file as <code>README.md</code> (root) or create <code>docs/index.md</code> to serve using GitHub Pages.</li> <li>In repository Settings -&gt; Pages, set the source branch (<code>main</code>/<code>gh-pages</code>) and folder (<code>/ (root)</code> or <code>/docs</code>).</li> <li>Commit and push. GitHub will serve the markdown as a static page.</li> </ol> <p>Use headings and links in this markdown to create a friendly navigable documentation site.</p>"},{"location":"lora/#12-references-further-reading","title":"12. References &amp; further reading","text":"<ul> <li>Practical Tips for Finetuning LLMs Using LoRA \u2014 Sebastian Raschka (Ahead of AI / Magazine): https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</li> <li>LoRA paper and original references (Hu et al.) \u2014 search <code>LoRA arXiv Hu et al.</code></li> <li>QLoRA (Dettmers et al.) \u2014 for 4-bit quantized LoRA approaches</li> <li>Hugging Face PEFT documentation \u2014 parameter-efficient fine-tuning utilities</li> </ul> <p>End of document.</p>"}]}