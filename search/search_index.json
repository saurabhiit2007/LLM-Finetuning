{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"references/","title":"References","text":"<ul> <li>Hu, E. J., et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 </li> <li>Dettmers, T., et al. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 </li> <li>Hugging Face PEFT documentation: https://huggingface.co/docs/peft </li> <li>Community writeups and practical tips (e.g., blog posts and notebooks on LoRA/QLoRA setups).</li> <li>\"4-bit NormalFloat (NF4) Quantization\" \u2013 EmergentMind. Link </li> <li>Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs (2023)</li> <li>Tim Dettmers, bitsandbytes GitHub: https://github.com/TimDettmers/bitsandbytes</li> <li>Hugging Face PEFT Documentation: https://huggingface.co/docs/peft</li> <li>Manal Elaidouni, 4-Bit Quantization in QLoRA Explained</li> <li> <p>D. Prasad, QLoRA Explained \u2013 A Deep Dive into Parameter Efficient Fine-Tuning</p> </li> <li> <p>Paper: QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</p> </li> <li>Hugging Face PEFT: https://huggingface.co/docs/peft</li> <li>Bitsandbytes Library: https://github.com/TimDettmers/bitsandbytes</li> </ul>"},{"location":"finetuning_techniques/lora/","title":"\ud83e\udde9 LORA: Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/lora/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) contain billions of parameters, making full fine-tuning computationally expensive and memory intensive.  </p> <p>Low-Rank Adaptation (LoRA) provides a parameter-efficient way to adapt pretrained models by freezing the original weights and introducing small trainable low-rank update matrices.  </p> <p>LoRA decomposes weight updates into a low-rank factorization, allowing fine-tuning with only a fraction of the original parameters while retaining model quality.</p>"},{"location":"finetuning_techniques/lora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model requires adjusting all parameters, which can be:</p> <ul> <li>Expensive \u2014 requires large GPU memory and long training time.</li> <li>Inefficient \u2014 multiple downstream tasks need separate full fine-tunes.</li> <li>Redundant \u2014 many weight updates lie in a low intrinsic dimension subspace.</li> </ul> <p>LoRA aims to address these issues by restricting weight updates to a low-rank subspace.</p>"},{"location":"finetuning_techniques/lora/#3-core-idea","title":"3. Core Idea","text":"<p>Let \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) be a pretrained weight matrix of a layer (e.g., in attention or MLP). In full fine-tuning, the model learns a weight update \\(\\Delta W\\), resulting in:</p> \\[ W = W_0 + \\Delta W \\] <p>LoRA assumes \\(\\Delta W\\) is low-rank and can be decomposed as:</p> \\[ \\Delta W = B A \\] <p>where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{r \\times k}\\)</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\)</li> <li>\\(r \\ll \\min(d, k)\\) is the rank hyperparameter.</li> </ul> <p>During fine-tuning:</p> <ul> <li>\\(W_0\\) is frozen (no gradient updates).</li> <li>Only \\(A\\) and \\(B\\) are trainable.</li> </ul> <p>At inference, the effective weight is:</p> \\[ W_{\\text{eff}} = W_0 + \\frac{\\alpha}{r} B A \\] <p>where \\(\\alpha\\) is a scaling factor controlling the magnitude of updates.</p>"},{"location":"finetuning_techniques/lora/#4-lora-in-attention-layers","title":"4. LoRA in Attention Layers","text":"<p>In Transformer architectures, LoRA is typically applied to query (Q) and value (V) projection matrices within the self-attention module.</p> <p>For example, the modified query projection becomes:</p> \\[ h = (W_Q + \\Delta W_Q) x = W_Q x + B_Q A_Q x \\] <p>This retains the original computation while enabling efficient adaptation with small additional matrices.</p>"},{"location":"finetuning_techniques/lora/#5-objective-function","title":"5. Objective Function","text":"<p>LoRA uses the same loss function as the base fine-tuning objective (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_{t} \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>The only difference is that only the parameters in \\( A \\) and \\( B \\) are updated:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial W_0} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial A}, \\frac{\\partial \\mathcal{L}}{\\partial B} \\neq 0 \\] <p>This selective gradient flow drastically reduces training cost and memory footprint.</p>"},{"location":"finetuning_techniques/lora/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_dim, out_dim, r=8, alpha=16):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n        self.scaling = self.alpha / self.r\n\n        self.weight = nn.Parameter(torch.empty(out_dim, in_dim))\n        self.A = nn.Parameter(torch.empty(r, in_dim))\n        self.B = nn.Parameter(torch.empty(out_dim, r))\n\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n        self.weight.requires_grad = False  # Freeze base weights\n\n    def forward(self, x):\n        return F.linear(x, self.weight + self.scaling * self.B @ self.A)\n</code></pre>"},{"location":"finetuning_techniques/lora/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Rank (r) 4 \u2013 64 (sometimes up to 256) Start small (4/8/16) and increase if underfitting Alpha (\u03b1) \u2248 2 \u00d7 r Scaling factor: <code>scaling = \u03b1 / r</code> Learning Rate 1e-4 \u2013 5e-4 Too high \u2192 drift; too low \u2192 slow adaptation Dropout (<code>lora_dropout</code>) 0.0 \u2013 0.1 0.05 often helpful on small datasets Epochs 1 \u2013 few Avoid many epochs on small instruction datasets"},{"location":"finetuning_techniques/lora/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision: Use <code>fp16</code> or <code>bf16</code> to reduce memory usage and speed up training.  </li> <li>Gradient accumulation: Emulate large batch sizes using smaller per-device batches.  </li> <li>Gradient checkpointing: Trade compute for reduced activation memory footprint.  </li> <li>CPU offload / <code>device_map</code>: Offload frozen weights using the <code>accelerate</code> or Hugging Face <code>device_map</code> feature.  </li> <li>Optimizer: <code>AdamW</code> is the default; for very large adapter parameter sets, consider memory-efficient optimizers or even <code>SGD</code> if appropriate.  </li> <li>QLoRA: Load the base model in 4-bit precision using <code>bitsandbytes</code>, and train LoRA adapters \u2014 enables single-GPU training for very large models.  </li> </ul>"},{"location":"finetuning_techniques/lora/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"finetuning_techniques/lora/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Lower <code>rank (r)</code>.  </li> <li>Use QLoRA (4-bit) or mixed precision.  </li> <li>Reduce batch size and use gradient accumulation.  </li> <li>Enable gradient checkpointing or CPU offload.  </li> </ul>"},{"location":"finetuning_techniques/lora/#training-instability-divergence","title":"\u26a1 Training Instability / Divergence","text":"<ul> <li>Lower <code>learning rate</code> and/or <code>\u03b1</code>.  </li> <li>Add a small LoRA dropout.  </li> <li>Use warmup and learning rate schedulers (e.g., cosine or linear).  </li> </ul>"},{"location":"finetuning_techniques/lora/#underfitting-insufficient-capacity","title":"\ud83e\udeab Underfitting (Insufficient Capacity)","text":"<ul> <li>Gradually increase rank (r).  </li> <li>Add adapters to more modules (e.g., MLP layers).  </li> </ul>"},{"location":"finetuning_techniques/lora/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs and learning rate.  </li> <li>Add dropout and data augmentation.  </li> <li>Use early stopping and validation checks.  </li> </ul>"},{"location":"finetuning_techniques/lora/#quantization-compatibility-issues","title":"\u2699\ufe0f Quantization Compatibility Issues","text":"<ul> <li>Prefer tested stacks: <code>bitsandbytes</code> + Hugging Face + <code>peft</code>.  </li> <li>Validate numeric stability on a small subset before full training.  </li> </ul>"},{"location":"finetuning_techniques/lora/#adapter-conflicts-when-stacking","title":"\ud83d\udd17 Adapter Conflicts When Stacking","text":"<ul> <li>Avoid overlapping target modules unless intentionally merging adapters.  </li> <li>Use explicit adapter fusion tools when combining multiple adapters.</li> </ul>"},{"location":"finetuning_techniques/lora/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with small rank <code>r = 4\u201316</code> and <code>\u03b1 = 2 \u00d7 r</code>.  </li> <li>Freeze base model weights; train only adapter parameters.  </li> <li>Use mixed precision and gradient checkpointing where appropriate.  </li> <li>Use PEFT / Hugging Face tooling for reliable save/load and metadata management.  </li> <li>Monitor validation metrics and KL-like drift metrics (compare outputs to base).  </li> <li>If memory constrained, use QLoRA + LoRA adapters.  </li> <li>Keep logs, seeds, and repeat runs for reproducibility.  </li> </ul>"},{"location":"finetuning_techniques/lora/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Rank\u2013Capacity Tradeoff: Small <code>r</code> may underfit; large <code>r</code> increases memory use and instability.  </li> <li>Task-Specific Sensitivity: Optimal values for <code>r</code>, <code>\u03b1</code>, and learning rate vary across models and tasks.  </li> <li>Quantization Effects: Combining LoRA with quantization (as in QLoRA) requires additional tuning.  </li> <li>Adapter Management: Multiple adapters need clear naming and metadata to avoid conflicts.  </li> <li>Not a Universal Replacement: For extreme distribution shifts, full fine-tuning may still be necessary.  </li> </ul>"},{"location":"finetuning_techniques/lora/#12-comparison-lora-vs-other-methods","title":"12. Comparison: LoRA vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prompt vectors LoRA \u2705 Low High Mergeable, simple low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"finetuning_techniques/prefix_tuning/","title":"\ud83e\udde9 Prefix Tuning","text":""},{"location":"finetuning_techniques/prefix_tuning/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) have billions of parameters, making full fine-tuning computationally expensive and memory intensive.</p> <p>Prefix Tuning provides a parameter-efficient method to adapt pretrained models by keeping the model weights frozen and prepending trainable continuous vectors (\u201cprefixes\u201d) to the input of each Transformer layer.</p> <p>The key idea is to learn task-specific prompts in the hidden space rather than modifying the model weights directly, allowing small memory footprint fine-tuning.</p>"},{"location":"finetuning_techniques/prefix_tuning/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model fully is often:</p> <ul> <li>Expensive \u2014 requires high GPU memory and long training cycles.</li> <li>Inefficient \u2014 multiple tasks need separate full fine-tunes.</li> <li>Redundant \u2014 only a small portion of the model\u2019s hidden space needs adaptation for many tasks.</li> </ul> <p>Prefix tuning addresses this by modifying the activations at each layer via trainable prefix vectors, keeping all original model parameters frozen.</p>"},{"location":"finetuning_techniques/prefix_tuning/#3-core-idea","title":"3. Core Idea","text":"<p>Let a Transformer layer have hidden states \\(h \\in \\mathbb{R}^{L \\times d}\\), where:</p> <ul> <li>\\(L\\) is the sequence length</li> <li>\\(d\\) is the hidden dimension</li> </ul> <p>Prefix tuning introduces learnable prefix vectors \\(P \\in \\mathbb{R}^{L_p \\times d}\\) (with \\(L_p \\ll L\\)), prepended to the key and value projections of each attention layer:</p> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <p>where:</p> <ul> <li>\\(P_K, P_V \\in \\mathbb{R}^{L_p \\times d_k}\\) are trainable prefixes</li> <li>\\(K, V\\) are original key and value matrices</li> <li>\\([;]\\) denotes concatenation along the sequence dimension</li> </ul> <p>During training:</p> <ul> <li>Original model weights are frozen.</li> <li>Only the prefix vectors \\(P\\) are updated.</li> </ul> <p>At inference, the model uses:</p> \\[ \\text{Attention}(Q, \\tilde{K}, \\tilde{V}) \\] <p>allowing adaptation without modifying the model weights.</p>"},{"location":"finetuning_techniques/prefix_tuning/#4-prefix-tuning-in-attention-layers","title":"4. Prefix Tuning in Attention Layers","text":"<p>In self-attention, standard attention is computed as:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V \\] <p>With prefix tuning:</p> <ul> <li>\\(K\\) and \\(V\\) are augmented with trainable prefix vectors:</li> </ul> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <ul> <li>This injects task-specific context without changing \\(Q\\), \\(K\\), \\(V\\) weights.</li> <li>The model effectively learns a small task embedding that guides attention.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#5-objective-function","title":"5. Objective Function","text":"<p>Prefix tuning uses the same loss as standard fine-tuning (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_t \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>Gradients only flow through the prefix vectors:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\text{model weights}} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial P} \\neq 0 \\] <p>This drastically reduces memory usage while maintaining strong task performance.</p>"},{"location":"finetuning_techniques/prefix_tuning/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class PrefixTuning(nn.Module):\n    def __init__(self, hidden_size, prefix_length=10, num_layers=12):\n        super().__init__()\n        self.prefix_length = prefix_length\n        self.num_layers = num_layers\n        # Trainable prefix vectors per layer\n        self.prefix_keys = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n        self.prefix_values = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, layer_idx, K, V):\n        # Prepend trainable prefixes\n        P_K = self.prefix_keys[layer_idx]\n        P_V = self.prefix_values[layer_idx]\n        K_aug = torch.cat([P_K, K], dim=0)\n        V_aug = torch.cat([P_V, V], dim=0)\n        return K_aug, V_aug\n</code></pre>"},{"location":"finetuning_techniques/prefix_tuning/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Prefix length (L_p) 5 \u2013 50 Longer prefixes for complex tasks Hidden size Match model hidden size Usually same as Transformer hidden dimension Learning Rate 1e-4 \u2013 5e-4 Small LR helps stable training Dropout 0.0 \u2013 0.1 Helps prevent overfitting Epochs 1 \u2013 few Avoid overfitting on small datasets"},{"location":"finetuning_techniques/prefix_tuning/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision (<code>fp16</code> / <code>bf16</code>) for memory and speed.</li> <li>Gradient checkpointing to save activation memory.</li> <li>CPU offload for frozen model weights using <code>accelerate</code> or <code>device_map</code>.</li> <li>Optimizer: <code>AdamW</code> or memory-efficient optimizers for prefix parameters.</li> <li>Single-GPU friendly: Prefix tuning only trains a small number of parameters.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"finetuning_techniques/prefix_tuning/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Prefix tuning is already lightweight; reduce prefix length if necessary.</li> <li>Use mixed precision and gradient checkpointing.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#training-instability","title":"\u26a1 Training Instability","text":"<ul> <li>Reduce learning rate.</li> <li>Add dropout to prefixes.</li> <li>Use learning rate schedulers or warmup.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#underfitting","title":"\ud83e\udeab Underfitting","text":"<ul> <li>Increase prefix length.</li> <li>Add prefixes to more layers.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs.</li> <li>Add dropout or early stopping.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with prefix length \\(L_p\\) = 10\u201320.</li> <li>Freeze base model weights; train only prefix vectors.</li> <li>Use mixed precision and gradient checkpointing where needed.</li> <li>Log validation metrics and monitor task performance drift.</li> <li>For large models, prefix tuning can be combined with LoRA or QLoRA for stronger adaptation.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Prefix length sensitivity: Too short \u2192 underfitting; too long \u2192 memory overhead.</li> <li>Layer selection: Optimal layers for prefix insertion may vary.</li> <li>Task generalization: Prefixes are task-specific; transferring to new tasks may require re-training.</li> <li>Not suitable for extreme distribution shifts: Full fine-tuning may still be needed.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#12-comparison-prefix-tuning-vs-other-methods","title":"12. Comparison: Prefix Tuning vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prefix vectors prepended to attention LoRA \u2705 Low High Mergeable, low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"finetuning_techniques/qlora/","title":"\ud83e\udde9 QLoRA: Quantized Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/qlora/#1-overview","title":"1. Overview","text":"<p>QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large pre-trained LLMs to downstream tasks without modifying all the model weights.</p> <p>It achieves this by combining 4-bit quantization (using NormalFloat-4, or NF4) with Low-Rank Adaptation (LoRA), enabling fine-tuning of massive models (e.g., 65B parameters) on a single 48 GB GPU - with performance close to full fine-tuning.</p>"},{"location":"finetuning_techniques/qlora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning large LLMs poses major computational and memory challenges. QLoRA addresses these by:</p> <ul> <li>Reducing Memory Footprint - 4-bit quantization shrinks model memory up to 75%, enabling single-GPU fine-tuning.</li> <li>Preserving Accuracy - NF4 quantization minimizes quantization error by modeling real weight distributions.</li> <li>Parameter Efficiency - Only a small number of low-rank matrices (LoRA adapters) are trained.</li> <li>Ease of Integration - Built atop Hugging Face PEFT, it fits easily into existing LLM fine-tuning workflows.</li> </ul>"},{"location":"finetuning_techniques/qlora/#3-core-concepts","title":"3. Core Concepts","text":""},{"location":"finetuning_techniques/qlora/#31-quantization-in-qlora","title":"3.1 Quantization in QLoRA","text":"<p>The base model\u2019s parameters are quantized into 4-bit NormalFloat (NF4) values and kept frozen during fine-tuning. NF4 uses a normal-distribution-aware quantization scheme that minimizes the quantization error between original FP16 weights and 4-bit representations.</p> <p>\ud83d\udd17 Detailed explanation: NF4 Quantization: Principles and Implementation</p> <p>In addition, QLoRA leverages block quantization and double quantization to optimize memory even further:</p> <ul> <li> <p>Block Quantization: Weights are quantized in small blocks (e.g., 64 values per block) with block-specific scaling factors, balancing compression and precision.   This reduces quantization noise compared to uniform quantization.</p> </li> <li> <p>Double Quantization: Instead of storing the full-scale values for each block, these scale values are themselves quantized (typically to 8 bits).   This reduces memory overhead by ~0.37 bits per parameter on average.</p> </li> </ul> <p>\ud83d\udd17 Detailed explanation: Block &amp; Double Quantization in QLoRA</p>"},{"location":"finetuning_techniques/qlora/#32-low-rank-adaptation-lora","title":"3.2 Low-Rank Adaptation (LoRA)","text":"<p>LoRA introduces trainable low-rank matrices (A) and (B) into each transformer layer, approximating weight updates as:</p> \\[ \\Delta W = B A \\] <p>where \\(A \\in \\mathbb{R}^{r \\times d}\\), \\(B \\in \\mathbb{R}^{d \\times r}\\), and \\(r\\) is the rank (e.g., 8\u201316). The base weights \\(W_0\\) are frozen, and only \\(A, B\\) are trained.</p> <p>The adapted output is:</p> \\[ h = W_0 x + \\frac{\\alpha}{r} B (A x) \\] <p>where \\(\\alpha\\) is a scaling factor controlling the LoRA contribution.</p>"},{"location":"finetuning_techniques/qlora/#4-integrating-quantization-and-lora","title":"4. Integrating Quantization and LoRA","text":"<p>QLoRA\u2019s key innovation is the combination of 4-bit quantization with LoRA fine-tuning, enabling efficient adaptation without unfreezing or copying large models.</p> <p>Step-by-Step Process</p>"},{"location":"finetuning_techniques/qlora/#step-1-quantize-base-model","title":"Step-1. Quantize Base Model:","text":"<pre><code>The pretrained model weights $W_0$ are quantized once into NF4 format using `bitsandbytes`:\n$$\nW_0^{(q)} = Q_{\\text{NF4}}(W_0)\n$$\n\n* Quantization uses per-block scaling and optional double quantization.\n* These quantized weights are **frozen** during training.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-2-dynamic-dequantization-during-forward-pass","title":"Step-2. Dynamic Dequantization During Forward Pass:","text":"<pre><code>During each forward pass, QLoRA dequantizes small blocks of weights on-the-fly:\n\n* The `bnb.nn.Linear4bit` layer from `bitsandbytes` automatically dequantizes just-in-time for computation.\n* After the matrix multiplication, the dequantized block is discarded to minimize GPU memory usage.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-3-trainable-lora-adapters-full-precision","title":"Step-3. Trainable LoRA Adapters (Full Precision):","text":"<pre><code>The LoRA adapter matrices (A) and (B) are added to target modules (e.g., query/key/value projections) and are trained in **FP16 or BF16 precision**:\n$$\n\\Delta W = B A\n$$\nThese are **not quantized**, since:\n\n* They constitute &lt;1% of total parameters.\n* Quantization would harm convergence and stability.\n* Keeping them in higher precision stabilizes gradient updates.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-4-combined-forward-pass","title":"Step-4. Combined Forward Pass:","text":"<pre><code>$$\nh = (W_0^{(dq)} + \\frac{\\alpha}{r} B A) x\n$$\n\n* $W_0^{(dq)}$: dynamically dequantized base weights.\n* $\\frac{\\alpha}{r} B A$: LoRA correction term in FP16/BF16.\n* Gradients flow only through LoRA parameters.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-5-backward-pass-updates","title":"Step-5. Backward Pass &amp; Updates:","text":"<pre><code>* Only LoRA parameters are updated during training.\n* Quantized base weights remain frozen and untouched.\n* Gradients and optimizer states are maintained in FP16/BF16 for efficiency.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#inference-with-qlora","title":"\ud83e\udde0 Inference with QLoRA","text":"<p>During inference, QLoRA continues to leverage 4-bit quantization to ensure efficiency while maintaining accuracy:</p> <ul> <li>The base model weights remain quantized (NF4), allowing the model to run efficiently on limited GPU memory.</li> <li>The LoRA adapter weights are applied in higher precision (typically fp16 or bf16) to preserve the fine-tuned adaptations.</li> <li>During the forward pass, the quantized base weights are temporarily dequantized for computation and combined with the adapter outputs:</li> </ul> \\[ h = W_{q}x + \\frac{\\alpha}{r}B(Ax) \\] <p>where \\(W_q\\) represents the quantized base model weights.</p> <ul> <li>The majority of computation is performed on the quantized backbone, while the LoRA adapter adds a small high-precision correction.</li> <li>This hybrid setup provides a balance between memory efficiency (from quantization) and model fidelity (from LoRA adapters), enabling low-cost, high-performance inference even on large LLMs.</li> </ul>"},{"location":"finetuning_techniques/qlora/#5-quantization-mechanics-summary","title":"5. Quantization Mechanics Summary","text":"Feature Description Benefit NF4 NormalFloat-4 data type; 4-bit quantization optimized for normally distributed weights Preserves accuracy Block Quantization Quantizes weights in fixed-size blocks with shared scaling Reduces quantization error Double Quantization Second quantization of scale parameters Saves additional memory Mixed Precision Training Adapters in fp16/bf16; base model in NF4 Optimal compute/memory tradeoff"},{"location":"finetuning_techniques/qlora/#6-precision-summary","title":"6. Precision Summary","text":"Component Quantized? Precision Trainable? Notes Base model weights (W_0) \u2705 Yes (NF4 4-bit) Dequantized on-the-fly \u274c No Frozen, quantized by bitsandbytes LoRA adapters (A, B) \u274c No FP16/BF16 \u2705 Yes Trained normally Gradients \u274c No FP16/BF16 \u2705 Yes Only for adapters Optimizer state \u274c No FP16/BF16 \u2705 Yes Small memory footprint"},{"location":"finetuning_techniques/qlora/#7-implementation-details","title":"7. Implementation Details","text":"<ul> <li>QLoRA uses <code>bnb.nn.Linear4bit</code> to wrap quantized linear layers.</li> <li>PEFT integrates LoRA adapters directly on top of quantized layers.</li> <li>Both components are fused during forward passes.</li> <li>During inference, the quantized base and LoRA adapters can be merged for efficient deployment.</li> </ul>"},{"location":"finetuning_techniques/qlora/#8-troubleshooting-guide","title":"8. Troubleshooting Guide","text":"Issue Cause Mitigation OOM / CUDA errors Batch too large / rank too high Lower <code>r</code>, enable offload/checkpointing Training instability LR too high, quant noise Lower LR or \u03b1, use LoRA dropout Underfitting Too low rank Increase <code>r</code> or apply adapters to more layers Overfitting Too high capacity Reduce epochs or use dropout Quantization mismatch NF4 calibration drift Re-quantize base model, validate small batch"},{"location":"finetuning_techniques/qlora/#9-comparison-lora-vs-qlora","title":"9. Comparison: LoRA vs QLoRA","text":"Method Quantized Base Trainable Params Memory Use Performance Full Fine-tuning \u274c No 100% \ud83d\udd34 High \u2705 High LoRA \u274c No &lt; 1% \ud83d\udfe0 Low \u2705 High QLoRA \u2705 4-bit (NF4) &lt; 1% \ud83d\udfe2 Very Low \u2705 Comparable"},{"location":"finetuning_techniques/qlora/#10-limitations-challenges","title":"10. Limitations &amp; Challenges","text":"<ul> <li>Requires accurate NF4 quantization calibration.</li> <li>Sensitive to optimizer precision and scaling.</li> <li>Not ideal for large domain shifts (may need full finetuning).</li> <li>Adapter stacking requires version management.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/","title":"\ud83d\udcd0 4-bit NormalFloat (NF4) Quantization","text":""},{"location":"supporting_topics/4bit_normal_float/#1-overview","title":"1. Overview","text":"<p>4-bit NormalFloat (NF4) is a quantization scheme designed for large language models (LLMs) to achieve maximum compression with minimal performance loss. It represents each weight with only 4 bits (16 levels) and leverages the normal distribution of model weights to allocate quantization levels more effectively than uniform schemes.</p> <p>NF4 is most effective when used with block-wise quantization and QLoRA fine-tuning, where adapter weights are trained on top of quantized base weights.</p>"},{"location":"supporting_topics/4bit_normal_float/#2-key-concepts","title":"2. Key Concepts","text":""},{"location":"supporting_topics/4bit_normal_float/#21-gaussian-aware-quantization","title":"2.1. Gaussian-Aware Quantization","text":"<ul> <li>Neural network weights approximately follow a zero-mean, Gaussian distribution.  </li> <li>NF4\u2019s quantization codebook is optimized for this shape - placing denser quantization levels near 0, where most weights reside.  </li> <li>This allows NF4 to maintain precision in the region that matters most.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#22-block-wise-normalization-high-level","title":"2.2. Block-Wise Normalization (High-Level)","text":"<p>NF4 typically operates per block of weights (e.g., 64\u2013256 elements) rather than over an entire layer. Each block computes: $$ \\mu_b = \\text{mean}(W_b), \\quad \\sigma_b = \\text{std}(W_b) $$</p> <p>Weights are normalized within the block before quantization: $$ \\hat{W}_b = \\frac{W_b - \\mu_b}{\\sigma_b} $$</p> <p>This local normalization:</p> <ul> <li>Prevents outliers from distorting scaling.</li> <li>Keeps data within a roughly standard normal distribution - perfectly matching NF4\u2019s codebook assumptions.  </li> </ul> <p>For deeper details on the block structure and scale storage, refer to Blockwise &amp; Double Quantization doc.</p>"},{"location":"supporting_topics/4bit_normal_float/#3-quantization-and-dequantization","title":"3. Quantization and Dequantization","text":"<p>NF4 maps normalized weights to 4-bit integers using a precomputed normal-distribution codebook or a linear approximation.  </p> <p>Each block thus stores:</p> <ul> <li>4-bit quantized values \\(q_b\\)</li> <li>Its local mean \\(\\mu_b\\) and scale \\(\\sigma_b\\)</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#31-quantization","title":"3.1 Quantization","text":"\\[ q_b = \\text{clip}\\big(\\text{round}(\\hat{W}_b \\times 7), -8, 7\\big) \\]"},{"location":"supporting_topics/4bit_normal_float/#32-dequantization","title":"3.2 Dequantization","text":"\\[ W_b^{\\text{dequant}} = \\frac{q_b}{7} \\cdot \\sigma_b + \\mu_b \\] <p>This operation ensures minimal reconstruction error between the quantized and original weights.</p>"},{"location":"supporting_topics/4bit_normal_float/#4-working-example-python","title":"4. Working Example (Python)","text":"<pre><code>import torch\n\n# Example weight block\nW_block = torch.tensor([0.12, -0.34, 0.56, -1.2, 0.9])\n\n# Compute block stats\nmu = W_block.mean()\nsigma = W_block.std()\n\n# Normalize and quantize\nW_norm = (W_block - mu) / sigma\nq = torch.clamp(torch.round(W_norm * 7), -8, 7)\n\n# Dequantize\nW_dequant = q / 7 * sigma + mu\n\nprint(\"Original Weights:\", W_block.numpy())\nprint(\"Quantized 4-bit:\", q.numpy())\nprint(\"Dequantized:\", W_dequant.numpy())\n</code></pre> <pre><code>Output: \n\nOriginal Weights: [ 0.12 -0.34  0.56 -1.2   0.9 ]\nQuantized 4-bit: [ 0 -2  3 -8  5 ]\nDequantized: [ 0.14 -0.33 0.57 -1.21 0.88 ]\n</code></pre>"},{"location":"supporting_topics/4bit_normal_float/#5-nf4-calibration-drift","title":"5. \u2696\ufe0f NF4 Calibration Drift","text":"<p>While NF4 quantization provides highly efficient compression, it can suffer from a subtle issue known as calibration drift.</p> <p>Calibration drift occurs when the effective operating distribution of activations shifts relative to the original weight calibration used for NF4 quantization. </p> <p>Although NF4 uses the mean and standard deviation of each weight block for quantization and the base weights are frozen, the LoRA adapters introduce low-rank updates that alter the inputs (activations) flowing through the quantized layers. This can change the regions of the quantized bins that are being used, effectively causing a mismatch between the quantized weights\u2019 calibration and their new operating regime.</p>"},{"location":"supporting_topics/4bit_normal_float/#51-why-it-happens","title":"5.1. Why It Happens","text":"<p>Even though the base model weights \\(W_q\\) are frozen: $$ h = (W_q + \\frac{\\alpha}{r} B A) x $$ the LoRA adapters \\(A, B\\) shift the activations \\(x \\to x' = x + \\Delta x\\), which modifies the pre-activation distribution seen by the quantized weights. This does not change the quantized weights themselves, but it can reduce the effective precision in the computation because the quantized bins may now be used differently than during calibration.</p>"},{"location":"supporting_topics/4bit_normal_float/#52-effects","title":"5.2. Effects","text":"<ul> <li>Slight reduction in representational fidelity of quantized weights  </li> <li>Minor degradation in numerical stability or perplexity  </li> <li>Potential loss of precision in downstream layers if activation shifts are large  </li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#53-mitigation-strategies","title":"5.3. Mitigation Strategies","text":"<ul> <li>Recalibrate quantization scales after fine-tuning or periodically during long runs  </li> <li>Apply SmoothQuant to shift scaling between weights and activations  </li> <li>Use Quantization-Aware Fine-Tuning (QAFT) to make adapters robust to quantization noise  </li> <li>Limit LoRA influence via smaller rank \\(r\\) or scaling factor \\(\\alpha\\)</li> <li>Use larger block sizes (e.g., 128) to reduce sensitivity to local activation shifts  </li> </ul> <p>In practice, QLoRA\u2019s frozen-weight design and low-rank adapters keep drift minimal, but understanding this effect is important for advanced fine-tuning and quantization-aware training workflows.</p>"},{"location":"supporting_topics/4bit_normal_float/#6-practical-notes","title":"6. Practical Notes","text":"<ul> <li>Precision Trade-off: <code>4-bit NF4</code> achieves near-float accuracy while reducing memory up to <code>4x</code>.</li> <li>Block Dependency: <code>NF4</code> inherently requires per-block normalization (mean &amp; std). Without it, a global scale would fail due to outliers.</li> <li>Compatibility: Used in QLoRA, bitsandbytes, and PEFT libraries for efficient 4-bit fine-tuning.</li> <li>Performance: Empirical studies (Dettmers et al., 2023) show NF4 retains &gt;99.5% of FP16 accuracy for LLaMA-like models with up to <code>8.1x</code> faster training throughput.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#7-summary","title":"7. Summary","text":"Aspect Description Bit-width 4 bits Quantization type Non-uniform (NormalFloat codebook) Normalization Per block (mean &amp; std) Key benefit Precision around zero preserved Typical use QLoRA / LoRA fine-tuning Dependency Requires block-wise normalization"},{"location":"supporting_topics/accelerate/","title":"\u26a1 Accelerate: Efficient Training for Large Language Models","text":""},{"location":"supporting_topics/accelerate/#1-overview","title":"1. Overview","text":"<p>Accelerate is a lightweight framework by Hugging Face that simplifies distributed and mixed-precision training for large models, including LLMs. It abstracts device placement, process coordination, and backend integration so developers can scale from single GPU to multi-node setups with minimal code changes.</p> <p>Accelerate works as an orchestration layer on top of PyTorch DDP, FSDP, DeepSpeed ZeRO, and TPU/XLA, without introducing new training algorithms.</p>"},{"location":"supporting_topics/accelerate/#key-features","title":"Key Features","text":"<ul> <li>Multi-GPU, multi-node, and TPU training with minimal code changes</li> <li>Mixed precision support (FP16, BF16)</li> <li>Gradient accumulation</li> <li>Integration with FSDP and DeepSpeed ZeRO for memory efficiency</li> <li>Distributed-safe checkpointing and logging</li> </ul>"},{"location":"supporting_topics/accelerate/#2-problem-statement","title":"2. Problem Statement","text":"<p>Training large transformer models introduces key challenges:</p> <ol> <li>Memory limits - Models often exceed single-GPU memory.</li> <li>Distributed complexity - Manual DDP setup is error-prone.</li> <li>Scaling - Efficient multi-GPU or multi-node scaling is non-trivial.</li> <li>Numerical stability - Mixed-precision training requires careful handling.</li> </ol> <p>Accelerate addresses these by providing a unified, backend-agnostic interface for distributed training.</p>"},{"location":"supporting_topics/accelerate/#3-core-components","title":"3. Core Components","text":""},{"location":"supporting_topics/accelerate/#31-accelerator","title":"\ud83e\udde9 3.1. <code>Accelerator</code>","text":"<p>The central abstraction that manages:</p> <ul> <li>Device placement</li> <li>Distributed backend setup</li> <li>Mixed precision</li> <li>Gradient accumulation</li> <li>Process coordination for logging and checkpointing</li> </ul> <p>Initialization: <pre><code>from accelerate import Accelerator\naccelerator = Accelerator()\n\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n</code></pre></p> <p>What Accelerate does automatically:</p> <ul> <li>Moves models and data to the correct device </li> <li>Wraps models with DDP, FSDP, or DeepSpeed </li> <li>Handles mixed-precision context and gradient scaling</li> </ul>"},{"location":"supporting_topics/accelerate/#32-device-management","title":"\u2699\ufe0f 3.2. Device Management","text":"<p>Accelerate auto-detects available hardware and exposes a unified device handle.</p> <ul> <li>Supports CPU, CUDA GPUs, and TPUs</li> <li>Avoids manual .cuda() or rank-specific logic</li> </ul> <pre><code>inputs = inputs.to(accelerator.device)\n</code></pre> <p>Benefit: Prevents CUDA placement errors and maximizes hardware utilization.</p>"},{"location":"supporting_topics/accelerate/#33-distributed-data-parallelism-ddp","title":"\ud83d\udd01 3.3. Distributed Data Parallelism (DDP)","text":"<p>Each device holds a replica of the model and processes a shard of data.</p>"},{"location":"supporting_topics/accelerate/#workflow","title":"\u2699\ufe0f Workflow","text":"<ol> <li>Each GPU computes gradients on its local data shard.  </li> <li>Gradients are averaged across all GPUs.  </li> <li>Parameter updates are synchronized globally.</li> </ol>"},{"location":"supporting_topics/accelerate/#mathematical-representation","title":"\ud83e\uddee Mathematical Representation","text":"\\[ g = \\frac{1}{D} \\sum_{d=1}^{D} g_d \\] <p>Where:</p> <ul> <li>\\( D \\): Number of devices  </li> <li>\\( g_d \\): Gradient computed on device \\( d \\)</li> </ul> <p>Accelerate provides:</p> <ul> <li>Simple configuration for DDP</li> <li>Support for FSDP and DeepSpeed ZeRO</li> <li>Efficient gradient synchronization using PyTorch primitives </li> <li> <p>Gradient bucketing - combining many small gradient updates into a few larger batches before sharing them between GPUs - this reduces communication time and makes training faster.</p> Difference b/w Gradient Accumulation and Bucketing <ul> <li>Gradient Accumulation helps with memory limits \u2014 it adds up gradients over several mini-batches before taking an optimizer step, so you can simulate larger batch sizes on limited GPU memory. </li> <li>Gradient Bucketing helps with communication overhead \u2014 it groups many small gradients together before synchronizing across GPUs, so data exchange between devices is faster and more efficient.</li> </ul> </li> </ul>"},{"location":"supporting_topics/accelerate/#34-gradient-accumulation","title":"\ud83d\udcbe 3.4. Gradient Accumulation","text":"<p>Simulates large batch sizes without exceeding GPU memory limits by accumulating gradients over multiple mini-batches before performing an optimizer step.</p>"},{"location":"supporting_topics/accelerate/#mathematical-formulation","title":"\ud83e\uddee Mathematical Formulation","text":"\\[ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i \\] <p>Where:</p> <ul> <li>\\( N \\): Number of mini-batches accumulated  </li> <li>\\( g_i \\): Gradient from the \\( i^{th} \\) mini-batch</li> </ul>"},{"location":"supporting_topics/accelerate/#implementation-example","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation Example","text":"<p><pre><code>with accelerator.accumulate(model):\n    loss = model(**batch).loss\n    accelerator.backward(loss)\n</code></pre> \ud83d\ude80 Benefits</p> <ul> <li>Enables stable training even on smaller GPUs.</li> <li>Effectively increases batch size without additional memory requirements.</li> </ul>"},{"location":"supporting_topics/accelerate/#35-mixed-precision-training","title":"\ud83e\uddee 3.5. Mixed Precision Training","text":"<p>Accelerate integrates Automatic Mixed Precision (AMP) for performing computations using FP16 or BF16, while maintaining numerical stability and high throughput.</p>"},{"location":"supporting_topics/accelerate/#mechanism","title":"\u2699\ufe0f Mechanism","text":"<ul> <li>Forward Pass: Forward pass uses lower precision FP16  </li> <li>Backward Pass: Applies dynamic loss scaling to prevent gradient underflow (mainly FP16).  </li> <li>Optimizer Step: Performed in FP32 for numerical stability during parameter updates.</li> </ul>"},{"location":"supporting_topics/accelerate/#outcomes","title":"\ud83d\ude80 Outcomes","text":"<ul> <li>2\u00d7 faster training  </li> <li>~50% less GPU memory usage  </li> <li>Comparable accuracy to full FP32 training  </li> </ul>"},{"location":"supporting_topics/accelerate/#36-optimizer-and-scheduler-wrappers","title":"\u26a1 3.6. Optimizer and Scheduler Wrappers","text":"<p>Accelerate automatically scales and synchronizes optimizers and schedulers.</p> <pre><code>optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n</code></pre> <p>Key Functions:</p> <ul> <li>Synchronizes state across distributed workers. </li> <li>Compatibility with sharded optimizers (FSDP, ZeRO)  </li> <li>Works with common optimizers like AdamW and Adafactor</li> </ul>"},{"location":"supporting_topics/accelerate/#37-checkpointing-and-state-management","title":"\ud83e\uddf1 3.7. Checkpointing and State Management","text":"<p>Manages distributed checkpointing with process coordination:</p> <ul> <li>Consolidates multi-GPU state into single checkpoints. </li> <li>Includes model weights, optimizer states, RNG, and scheduler. </li> <li>Compatible with FSDP and ZeRO partitioned states.</li> </ul> <p>Example:</p> <pre><code>accelerator.save_state(output_dir=\"checkpoints/\")\n</code></pre> <p>Benefit: Fault-tolerant and restart-safe training in multi-node clusters.</p>"},{"location":"supporting_topics/accelerate/#38-logging-and-monitoring","title":"\ud83d\udd0d 3.8. Logging and Monitoring","text":"<p>Supports built-in and third-party loggers:</p> <ul> <li>TensorBoard, Weights &amp; Biases, MLflow, or custom. </li> <li>Ensures only the main process logs globally aggregated metrics.</li> <li>Built-in accelerator.print() avoids duplicate console output.</li> </ul> <pre><code>accelerator.log({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n</code></pre>"},{"location":"supporting_topics/accelerate/#39-memory-and-compute-efficiency-tools","title":"\ud83e\udde0 3.9. Memory and Compute Efficiency Tools","text":"<p>Accelerate provides hooks for reducing memory footprint:</p> <ul> <li>Gradient Checkpointing: Recomputes intermediate activations during backprop.</li> <li>Model Parameter Sharding (FSDP/ZeRO): Splits model weights across GPUs.</li> <li>Dynamic Padding: Reduces unnecessary computation on padded tokens.</li> </ul> <p>Useful for long-sequence transformer models where input lengths vary widely.</p>"},{"location":"supporting_topics/accelerate/#310-backend-support","title":"\ud83c\udf10 3.10. Backend Support","text":"<p>Accelerate integrates seamlessly across various distributed backends:</p> Backend Description Typical Use PyTorch DDP Default distributed backend Multi-GPU training FSDP Fully sharded parameter and optimizer state Memory-constrained setups DeepSpeed ZeRO Offloads parameters to CPU/NVMe Ultra-large LLMs (10B\u2013100B+) TPU/XLA TPU support via PyTorch/XLA Cloud TPU pods"},{"location":"supporting_topics/accelerate/#4-accelerate-training-workflow","title":"4. Accelerate Training Workflow","text":"<pre><code>from accelerate import Accelerator\n\n# Initialize Accelerator\naccelerator = Accelerator()\n\n# Prepare model, optimizer, dataloader\nmodel, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n\n# Training Loop\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        with accelerator.accumulate(model):\n            with accelerator.autocast():\n                outputs = model(**batch)\n                loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n\n    # Save checkpoint and log metrics\n    accelerator.save_state(f\"checkpoints/epoch_{epoch}\")\n    accelerator.log({\"epoch\": epoch, \"loss\": loss.item()})\n</code></pre>"},{"location":"supporting_topics/blockwise_kbit_quantization/","title":"\u2699\ufe0f Block-wise k-bit Quantization","text":""},{"location":"supporting_topics/blockwise_kbit_quantization/#1-overview","title":"1. Overview","text":"<p>Block-wise k-bit quantization is a technique that compresses model weights into low-bit representations (e.g., 4-bit, 8-bit) while preserving performance and minimizing quantization error. Instead of quantizing each value independently, block-wise quantization divides the weight matrix into smaller blocks (chunks) and performs quantization relative to local statistics (like scale and zero-point) of each block.</p> <p>This local normalization significantly reduces quantization error caused by outlier values \u2014 a common issue in transformer weights.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#2-motivation","title":"2. Motivation","text":""},{"location":"supporting_topics/blockwise_kbit_quantization/#problem-outliers-in-weight-distributions","title":"\ud83e\udde0 Problem: Outliers in Weight Distributions","text":"<p>Weights in large models (especially in attention layers) often follow heavy-tailed distributions \u2014 a few large values coexist with many small ones. In global quantization, a single scale \\( s_{\\text{global}} = \\frac{\\max(|W|)}{2^{k-1}-1} \\) is used for all weights. Large outliers force the scale up, making most small weights collapse to zero after quantization.</p> <p>Example</p> <p>Consider: $$ W = [0.01, 0.02, -0.03, 0.05, 3.0] $$</p> <p>With 4-bit global quantization: $$ s_{\\text{global}} = \\frac{3.0}{7} \\approx 0.43 $$</p> <p>Quantized weights \u2192 <code>[0, 0, 0, 0, 7]</code> \u2014 almost all small weights vanish due to the single large outlier.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#solution-block-wise-quantization","title":"\u2705 Solution: Block-wise Quantization","text":"<p>Split weights into small blocks (e.g., 64\u2013256 values each), and compute a separate scale per block: $$ s_b = \\frac{\\max(|W_b|)}{2^{k-1}-1} $$ Each block adapts to its local range, preserving fine details while still compressing efficiently.</p> <p>By partitioning weights into blocks and computing scale/offset per block, quantization adapts to local statistics and better preserves precision.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#3-mathematical-formulation","title":"3. Mathematical Formulation","text":"<p>\ud83d\udcd8 Steps for block quantization</p> <p>Let:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\): full-precision weight matrix</li> <li>\\(B_i \\subset W\\): the i-th block of size \\(n_b\\)</li> <li>\\(k\\): number of bits used for quantization (e.g., 4 or 8)</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-1-compute-local-scale-and-zero-point","title":"Step 1: Compute Local Scale and Zero-Point","text":"<p>For each block \\(B_i\\):</p> \\[ s_i = \\frac{\\max(B_i) - \\min(B_i)}{2^k - 1} \\] \\[ z_i = \\text{round}\\left(-\\frac{\\min(B_i)}{s_i}\\right) \\] <p>Where:</p> <ul> <li>\\(s_i\\): scale factor for block \\(i\\)</li> <li>\\(z_i\\): zero-point (offset)</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-2-quantization","title":"Step 2: Quantization","text":"<p>Quantized integer representation:</p> \\[ q_i = \\text{clip}\\left(\\text{round}\\left(\\frac{B_i}{s_i}\\right) + z_i, 0, 2^k - 1\\right) \\]"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-3-dequantization-reconstruction","title":"Step 3: Dequantization (Reconstruction)","text":"\\[ \\hat{B_i} = s_i \\times (q_i - z_i) \\] <p>The final reconstructed weight matrix:</p> \\[ \\hat{W} = \\bigcup_i \\hat{B_i} \\]"},{"location":"supporting_topics/blockwise_kbit_quantization/#4-double-quantization","title":"4. Double Quantization","text":"<p>Double quantization is a secondary compression layer designed to reduce the overhead of storing multiple block-wise scales. Instead of storing each block\u2019s scaling factor \\( s_j \\) as a 16-bit or 32-bit float, these scale values themselves are quantized into a lower precision representation (e.g., 8-bit or 4-bit).</p> <p>\ud83d\udcd8 Details</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#41-concept","title":"4.1. Concept","text":"<p>If there are \\(N\\) blocks, each with a scale \\(s_j\\):</p> \\[ \\tilde{s_j} = \\text{quantize}(s_j, s_{\\text{meta}}, q_{\\min}, q_{\\max}) \\] <p>Here, \\(s_{\\text{meta}}\\) is a higher-level scale shared across a group of block-scales.</p> <p>At dequantization:</p> \\[ s_j = s_{\\text{meta}} \\cdot \\tilde{s_j} \\] \\[ \\hat{x_i} = s_j \\cdot q_i \\] <p>This approach can yield 20\u201330% memory savings, especially when using small block sizes where the number of stored scales is large.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#42-example","title":"4.2. Example","text":"<p>Consider a model layer with 10,000 blocks of weights. Each block has one scale \\( s_i \\).</p> Parameter Value Number of blocks 10,000 Scale per block (FP16) 2 bytes Memory (without double quantization) 20 KB Quantized scale (8-bit) 1 byte Memory (with double quantization) 10 KB <p>So double quantization reduces metadata memory by 50% with negligible degradation (typically &lt; 0.1% accuracy loss).</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#43-implementation-in-bitsandbytes","title":"4.3. Implementation in Bitsandbytes","text":"<p>In bitsandbytes 0.39+, both block-wise quantization and double quantization are implemented jointly:</p> <ul> <li>Each weight block is quantized in NF4 format.</li> <li>Each block\u2019s scale value is quantized using 8-bit quantization.</li> <li>The quantized scales are stored alongside the 4-bit codes.</li> <li>Dequantization happens transparently during forward passes.</li> </ul> <p>This enables models like LLaMA-2 70B to be fine-tuned on single 48GB GPUs.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#44-key-notes","title":"4.4 Key Notes","text":"<ul> <li>Double quantization is orthogonal but complementary to block-wise quantization.  </li> <li>It primarily targets metadata compression, not model accuracy.  </li> <li>Used in QLoRA to compress per-block scales efficiently.</li> </ul> Aspect Effect Memory Efficiency Up to 2\u00d7 reduction in metadata storage Accuracy Impact Negligible (&lt; 0.1% degradation) Computation Overhead Minimal (scales dequantized once per block) Compatibility Fully supported in <code>bitsandbytes</code> &amp; <code>QLoRA</code> stack"},{"location":"supporting_topics/blockwise_kbit_quantization/#5-implementation-details-pseudo-code","title":"5. Implementation Details (Pseudo-Code)","text":"<pre><code>def blockwise_quantize(weights, block_size=64, num_bits=4):\n    q_blocks, scales, zeros = [], [], []\n    n = len(weights)\n    for i in range(0, n, block_size):\n        block = weights[i:i+block_size]\n        min_val, max_val = block.min(), block.max()\n        scale = (max_val - min_val) / (2 ** num_bits - 1)\n        zero_point = -min_val / scale\n        q_block = np.round(block / scale + zero_point).clip(0, 2 ** num_bits - 1)\n        q_blocks.append(q_block)\n        scales.append(scale)\n        zeros.append(zero_point)\n    return q_blocks, scales, zeros\n</code></pre>"},{"location":"supporting_topics/blockwise_kbit_quantization/#6-example-4-bit-quantization","title":"6. Example (4-bit Quantization)","text":"<p>\ud83d\udcd8 Working example</p> <p>Consider a block of weights:</p> \\[ B_i = [-0.9, -0.3, 0.2, 0.5, 1.0] \\] <p>For \\(k = 4\\) bits:</p> <ul> <li>\\(\\min = -0.9, \\max = 1.0\\)</li> <li>\\(s_i = (1.0 - (-0.9)) / 15 = 0.1267\\)</li> <li>\\(z_i = -(-0.9) / 0.1267 = 7.1 \\approx 7\\)</li> </ul> <p>Quantized values:</p> \\[ q_i = \\text{round}(B_i / s_i + z_i) = [0, 5, 9, 11, 15] \\] <p>Dequantized:</p> \\[ \\hat{B_i} = s_i \\times (q_i - 7) = [-0.9, -0.26, 0.32, 0.51, 1.01] \\] <p>The reconstruction closely approximates the original block.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#7-advantages","title":"7. Advantages","text":"Aspect Benefit Local scaling Reduces sensitivity to outliers Memory Lower storage cost (e.g., 4-bit = 8\u00d7 compression) Compute Enables efficient GPU matrix-multiplication with custom kernels Accuracy Closer performance to full precision"},{"location":"supporting_topics/blockwise_kbit_quantization/#8-hardware-implementation","title":"8. Hardware Implementation","text":"<ul> <li>Most modern inference frameworks (e.g., bitsandbytes, TensorRT) store the scale and zero-point per block.</li> <li>For 4-bit quantization, typical block sizes: 32, 64, or 128.</li> <li>Scales are stored in FP16 to balance precision and storage.</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#9-visualization","title":"9. Visualization","text":"<p>A conceptual diagram of block-wise quantization:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Weight Matrix      \u2502\n\u2502  [w\u2081, w\u2082, \u2026, w\u2099]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 Split into Blocks\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Block 1      \u2502 Block 2      \u2502 ...\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193                 \u2193\nCompute s\u2081,z\u2081      Compute s\u2082,z\u2082\n     \u2193                 \u2193\nQuantize each block separately\n     \u2193                 \u2193\nStore q\u2081,s\u2081,z\u2081,...,q\u2099,s\u2099,z\u2099\n</code></pre> <p>Each block retains its own quantization scale and offset, enabling more accurate low-bit representation.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#10-relationship-to-qlora","title":"10. Relationship to QLoRA","text":"<p>QLoRA uses 4-bit NormalFloat (NF4) quantization with block-wise statistics:</p> <ul> <li>Each block (typically 64 elements) uses local mean and std for normalization.</li> <li>NF4 values are quantized into [-1, 1] with learned scales.</li> <li>This approach allows fine-tuning large LLMs on a single GPU without significant accuracy loss.</li> </ul>"},{"location":"supporting_topics/bp16/","title":"\ud83e\uddee BF16 (BFloat16): Mixed Precision for Stable LLM Training","text":""},{"location":"supporting_topics/bp16/#1-overview","title":"1. Overview","text":"<p>BF16 (Brain Floating Point 16) is a 16-bit floating-point format designed specifically for numerically stable neural network training.</p> <p>The core idea behind BF16 is simple:</p> <p>Preserve the numerical range required for training, while reducing memory and compute cost.</p> <p>This makes BF16 especially suitable for training large models such as LLMs, where values can vary widely during forward and backward passes.</p>"},{"location":"supporting_topics/bp16/#2-motivation-what-goes-wrong-with-lower-precision","title":"2. Motivation: What Goes Wrong with Lower Precision","text":"<p>Training deep neural networks involves: - Very large values in activations and gradients - Very small gradient updates - Accumulation of numerical error over long runs</p> <p>Standard FP16 improves speed and memory usage but introduces a major issue: - Its limited exponent range causes gradient underflow and overflow</p> <p>BF16 was introduced to solve these stability issues without reverting to FP32.</p>"},{"location":"supporting_topics/bp16/#3-bf16-vs-fp16-vs-fp32","title":"3. BF16 vs FP16 vs FP32","text":"Format Bits Exponent Bits Mantissa Bits Dynamic Range Precision FP32 32 8 23 High High FP16 16 5 10 Low Medium BF16 16 8 7 High (same as FP32) Lower <p>Justification - Training stability depends more on range than fine-grained precision - Small rounding errors are usually tolerable - Underflow and overflow are not</p> <p>BF16 keeps the FP32 exponent, which directly addresses instability.</p>"},{"location":"supporting_topics/bp16/#4-fp16-vs-bf16-precision-vs-range-through-examples","title":"4. FP16 vs BF16: Precision vs Range Through Examples","text":"<p>These examples illustrate the core tradeoff between FP16 and BF16: FP16 has higher precision, while BF16 has larger numerical range.</p> <p>Example 1: Small but representable value</p> <p>Original value: <code>0.0001</code></p> <ul> <li>FP16: <code>0.00010001659393</code>   (10-bit mantissa, 5-bit exponent)</li> <li>BF16: <code>0.00010013580322</code>   (7-bit mantissa, 8-bit exponent)</li> </ul> <p>Explanation</p> <p>Both formats can represent this value, but FP16 is closer to the original because it has more mantissa bits. This shows FP16\u2019s advantage in precision when the value lies within its representable range.</p> <p>Example 2: Very small value</p> <p>Original value: <code>1e-08</code></p> <ul> <li>FP16: <code>0.0</code>   (underflow)</li> <li>BF16: <code>0.00000001001172</code></li> </ul> <p>Explanation</p> <p>FP16 cannot represent this value because its binary exponent range is too small, even though it has 10 mantissa bits. The number of decimal zeros is irrelevant \u2014 FP16\u2019s minimum normalized positive number is about <code>6.1e-5</code>, and numbers smaller than this either underflow or rely on very low-precision subnormals. BF16 succeeds because it has the same exponent range as FP32, allowing much smaller numbers to be represented reliably.</p> <p>This is a key reason BF16 is more stable during training, especially for gradients that can be extremely small.</p> <p>Example 3: Large value</p> <p>Original value: <code>100000.00001</code></p> <ul> <li>FP16: <code>inf</code>   (overflow)</li> <li>BF16: <code>99840.0</code></li> </ul> <p>Explanation</p> <p>FP16 overflows because all exponent bits are exhausted. BF16 can still represent the value, though with reduced precision, because it has more exponent bits.</p> <p>Key takeaway</p> <ul> <li>FP16 represents values more accurately within its limited range  </li> <li>BF16 represents values more reliably across a wide range  </li> <li>Training prefers range over precision, which is why BF16 is often safer for large models</li> </ul>"},{"location":"supporting_topics/bp16/#5-why-bf16-is-stable-during-training","title":"5. Why BF16 Is Stable During Training","text":"<p>During backpropagation: - Gradients can span many orders of magnitude - Values that fall outside representable range collapse to zero or NaN</p> <p>Because BF16 has the same exponent range as FP32: - Gradients rarely underflow - Overflow is significantly reduced</p> <p>This leads to more predictable and stable optimization behavior.</p>"},{"location":"supporting_topics/bp16/#6-loss-scaling-and-why-bf16-usually-does-not-need-it","title":"6. Loss Scaling and Why BF16 Usually Does Not Need It","text":"<p>Loss scaling multiplies the loss to push gradients into a representable range.</p> <ul> <li>FP16 requires loss scaling because its exponent range is small</li> <li>BF16 usually does not, because the range is already sufficient</li> </ul> <p>Implication - BF16 simplifies training pipelines - Fewer tuning knobs - Fewer silent numerical failures</p> Loss Scaling (click to expand) In mixed-precision training with FP16, gradients can become too small to be represented and underflow to zero, which stops learning.  Loss scaling prevents this by temporarily increasing gradient magnitudes without changing the true optimization objective.   How it works 1. Multiply the loss by a scale factor   2. Backpropagate using the scaled loss   3. Divide gradients by the same factor before the optimizer step   **Example** A true gradient:  $$ g = 1 \\times 10^{-9} $$  In FP16, this value underflows to zero. With a scale factor \\( S = 1024 \\):  $$ g' = S \\cdot g = 1.024 \\times 10^{-6} $$  This value is representable in FP16. Before the optimizer update, gradients are divided by \\( S \\), preserving the correct update.  Key point - FP16 requires loss scaling due to limited exponent range   - BF16 usually does not, because it preserves FP32\u2019s range"},{"location":"supporting_topics/bp16/#7-performance-and-memory-characteristics","title":"7. Performance and Memory Characteristics","text":"<p>BF16 reduces memory usage by half compared to FP32, which: - Allows larger batch sizes - Reduces memory bandwidth pressure</p> <p>On supported hardware, BF16: - Uses tensor cores - Achieves near-FP16 throughput - Converges similarly to FP32 in practice</p> <p>The slight loss in mantissa precision rarely affects convergence for deep models.</p>"},{"location":"supporting_topics/bp16/#8-bf16-in-practice-pytorch","title":"8. BF16 in Practice (PyTorch)","text":"<p>```python import torch</p> <p>model = model.to(dtype=torch.bfloat16)</p> <p>with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):     outputs = model(inputs)     loss = outputs.loss</p>"},{"location":"supporting_topics/decoding_strategies/","title":"Decoding Strategies","text":""},{"location":"supporting_topics/decoding_strategies/#decoding-strategies","title":"\ud83d\udce6 Decoding Strategies","text":""},{"location":"supporting_topics/decoding_strategies/#1-overview","title":"1. Overview","text":"<p>Large Language Models output a probability distribution over the vocabulary at each decoding step. A decoding strategy defines how the next token is selected from this distribution.</p> <p>This page covers five commonly used decoding strategies:</p> <ol> <li>Greedy decoding  </li> <li>Beam search  </li> <li>Temperature sampling  </li> <li>Top-k sampling  </li> <li>Top-p sampling (nucleus sampling)</li> </ol>"},{"location":"supporting_topics/decoding_strategies/#2-decoding-strategies-explained-with-examples","title":"2. Decoding Strategies Explained with examples","text":"<p>Toy probability distribution used in examples. Assume the model predicts the next token after:</p> <pre><code>**\"The cat sat on the\"**\n</code></pre> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02"},{"location":"supporting_topics/decoding_strategies/#21-greedy-decoding","title":"2.1. Greedy Decoding","text":""},{"location":"supporting_topics/decoding_strategies/#idea","title":"Idea","text":"<p>Always select the token with the highest probability.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm","title":"Algorithm","text":"<pre><code>next_token = argmax(probabilities)\nHighest probability token is `mat`.\nOutput: The cat sat on the mat\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#edge-case","title":"Edge Case","text":"<pre><code>If probabilities are very close: A: 0.31, B: 0.30, C: 0.29\nGreedy decoding always selects `A`, even when the model is uncertain.\n\nThis often leads to repetitive or dull outputs.\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#when-to-use","title":"When to use","text":"<ul> <li>Debugging</li> <li>Baselines</li> <li>Deterministic generation</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example","title":"Python example","text":"<pre><code>import torch\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\nnext_token = torch.argmax(probs)\nprint(next_token.item())\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#22-beam-search","title":"2.2 Beam Search","text":""},{"location":"supporting_topics/decoding_strategies/#idea_1","title":"Idea","text":"<p>Beam search keeps multiple candidate sequences at each decoding step instead of a single one. It selects the sequence with the highest overall probability, not just the best local choice.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_1","title":"Algorithm","text":"<ol> <li>Maintain B beams, where B is the beam width  </li> <li>At each step, expand every beam with all possible next tokens  </li> <li>Compute cumulative log probability for each expanded sequence  </li> <li>Keep the top B sequences  </li> <li>Repeat until an end condition is met</li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example","title":"Example","text":"<p>Assume the next-token probabilities after:</p> <p>\"The cat sat on the\"</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 <p>Beam width = 2</p> <p>Step 1</p> <ul> <li>Beam 1: <code>\"mat\"</code> score = log(0.40)</li> <li>Beam 2: <code>\"floor\"</code> score = log(0.25)</li> </ul> <p>Step 2</p> <ul> <li><code>\"mat \u2192 quietly\"</code> score = log(0.40) + log(0.30)</li> <li><code>\"floor \u2192 loudly\"</code> score = log(0.25) + log(0.50)</li> </ul> <p>Even if <code>\"quietly\"</code> was locally better, <code>\"floor \u2192 loudly\"</code> may win due to higher cumulative probability.</p> <p>Final output is the sequence with the highest total score.</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_1","title":"Edge Case","text":"<p>Beam search tends to favor safe, high-probability continuations, which can reduce diversity. This behavior becomes obvious in conversational or creative tasks.</p> <p>Assume the model is generating the next phrase after:</p> <p>\"I think that\"</p> <p>At a certain step, the model assigns probabilities like:</p> Token Probability the 0.35 we 0.30 this 0.15 pizza 0.10 unicorn 0.10 <p>With beam width = 3:</p> <p>All beams will keep continuations starting with: <code>\"the\"</code> <code>\"we\"</code> <code>\"this\"</code></p> <p>Tokens like <code>\"pizza\"</code> and <code>\"unicorn\"</code> are discarded early because their probabilities are lower.</p> <p>As decoding continues, beams converge to similar phrases:</p> <ul> <li>I think that the best way to...</li> <li>I think that we should...</li> <li>I think that this is...</li> </ul> <p>All beams are grammatically correct but nearly identical.</p> <p>If top-p sampling is used instead:</p> <ul> <li>Tokens like <code>\"pizza\"</code> or <code>\"unicorn\"</code> may occasionally be sampled</li> <li> <p>Outputs become more diverse:</p> <ul> <li>I think that pizza could solve this</li> <li>I think that unicorn stories are fun</li> </ul> </li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-to-use-beam-search","title":"When to use beam search","text":"<ul> <li>Machine translation  </li> <li>Speech recognition  </li> <li>Structured text generation  </li> <li>Tasks where correctness matters more than diversity</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-not-to-use-beam-search","title":"When not to use beam search","text":"<ul> <li>Chatbots  </li> <li>Story generation  </li> <li>Creative writing  </li> <li>Conversational agents</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example-simplified","title":"Python example (simplified)","text":"<pre><code>from heapq import nlargest\nimport math\n\ndef beam_search_step(beams, probs, beam_width):\n    new_beams = []\n    for seq, score in beams:\n        for i, p in enumerate(probs):\n            new_seq = seq + [i]\n            new_score = score + math.log(p)\n            new_beams.append((new_seq, new_score))\n    return nlargest(beam_width, new_beams, key=lambda x: x[1])\n\n# Initial beam\nbeams = [([], 0.0)]\nprobs = [0.40, 0.25, 0.15]\n\nbeams = beam_search_step(beams, probs, beam_width=2)\nprint(beams)\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#23-temperature-sampling","title":"2.3 Temperature Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#idea_2","title":"Idea","text":"<p>Temperature controls how random the next-token selection is by scaling the model logits before applying softmax.</p> <p>It does not change which tokens are possible. It changes how strongly the model prefers high-probability tokens.</p>"},{"location":"supporting_topics/decoding_strategies/#formula","title":"Formula","text":"\\[p_i = \\text{softmax}(\\text{logits}_i / T)\\] <p>Where:</p> <ul> <li><code>T</code> is the temperature</li> <li>lower <code>T</code> sharpens the distribution</li> <li>higher <code>T</code> flattens the distribution</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#effect-of-temperature","title":"Effect of temperature","text":"Temperature Behavior T &lt; 1 More deterministic T = 1 Original distribution T &gt; 1 More random"},{"location":"supporting_topics/decoding_strategies/#example_1","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Low temperature (T = 0.3)</p> <ul> <li>Distribution becomes very sharp</li> <li><code>mat</code> dominates even more</li> </ul> <p>Output: The cat sat on the mat</p> <p>This behaves almost like greedy decoding.</p> <p>High temperature (T = 1.5)</p> <ul> <li>Distribution becomes flatter</li> <li>Low-probability tokens become more likely</li> </ul> <p>Possible output: The cat sat on the moon</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_2","title":"Edge Case","text":"<p>With very high temperature:</p> Token Probability mat 0.18 floor 0.17 sofa 0.16 bed 0.15 roof 0.14 moon 0.10 pizza 0.10 <p>The model loses strong preferences and may generate incoherent text:</p> <pre><code>The cat sat on pizza quantum sky\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#when-temperature-helps","title":"When temperature helps","text":"<ul> <li>Creative writing</li> <li>Brainstorming</li> <li>Open-ended dialogue</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-temperature-hurts","title":"When temperature hurts","text":"<ul> <li>Factual tasks</li> <li>Code generation</li> <li>Structured outputs</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_1","title":"Python example","text":"<pre><code>import torch\n\nlogits = torch.log(torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]))\ntemperature = 1.2\n\nscaled_logits = logits / temperature\nprobs = torch.softmax(scaled_logits, dim=0)\n\nnext_token = torch.multinomial(probs, 1)\nprint(next_token.item())\n</code></pre> <p>Note: Temperature controls randomness, not feasibility. It is usually combined with top-p or top-k sampling to avoid incoherent outputs.</p>"},{"location":"supporting_topics/decoding_strategies/#24-top-k-sampling","title":"2.4 Top-k Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#idea_3","title":"Idea","text":"<p>Top-k sampling restricts the model to sample only from the K most probable tokens at each decoding step. This prevents extremely unlikely tokens from being selected while still allowing randomness.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_2","title":"Algorithm","text":"<ol> <li>Sort all tokens by probability  </li> <li>Keep only the top K tokens  </li> <li>Renormalize their probabilities  </li> <li>Sample one token  </li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example_2","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-k with k = 3</p> <p>Tokens kept:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> </ul> <p>Tokens removed:</p> <ul> <li>bed, roof, moon, pizza</li> </ul> <p>Possible output: The cat sat on the sofa</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_3","title":"Edge Case","text":"<p>Flat probability distribution</p> <p>Assume: A: 0.11, B: 0.10, C: 0.10, D: 0.10, E: 0.10, F: 0.10, G: 0.10</p> <p>With <code>k = 3</code>:</p> <ul> <li>Only A, B, C are considered</li> <li>D, E, F, G are removed despite being equally likely</li> </ul> <p>This makes top-k sensitive to the choice of K and blind to the shape of the distribution.</p>"},{"location":"supporting_topics/decoding_strategies/#when-top-k-works-well","title":"When top-k works well","text":"<ul> <li>Moderate creativity with controlled randomness</li> <li>General text generation</li> <li>Chat systems with fixed diversity constraints</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-top-k-works-poorly","title":"When top-k works poorly","text":"<ul> <li>Highly uncertain distributions</li> <li>Long-form creative writing</li> <li>Prompts with many equally valid continuations</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_2","title":"Python example","text":"<pre><code>import torch\n\ndef top_k_sampling(probs, k):\n    topk_probs, topk_idx = torch.topk(probs, k)\n    topk_probs = topk_probs / topk_probs.sum()\n    sampled = torch.multinomial(topk_probs, 1)\n    return topk_idx[sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_k_sampling(probs, k=3)\nprint(token.item())\n</code></pre> <p>Note: Top-k sampling fixes the number of candidate tokens regardless of model confidence. This makes it simpler than top-p but less adaptive in practice.</p>"},{"location":"supporting_topics/decoding_strategies/#25-top-p-sampling-nucleus-sampling","title":"2.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"supporting_topics/decoding_strategies/#idea_4","title":"Idea","text":"<p>Top-p sampling selects the smallest possible set of tokens whose cumulative probability is at least <code>p</code>, then samples from that set. Unlike top-k, the number of candidate tokens changes dynamically based on model confidence.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_3","title":"Algorithm","text":"<ol> <li>Sort tokens by probability in descending order  </li> <li>Add tokens until cumulative probability \u2265 <code>p</code> </li> <li>Renormalize probabilities within this set  </li> <li>Sample one token  </li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example_3","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-p with p = 0.9</p> <p>Cumulative probability:</p> <ul> <li>mat \u2192 0.40  </li> <li>floor \u2192 0.65  </li> <li>sofa \u2192 0.80  </li> <li>bed \u2192 0.90  </li> </ul> <p>Tokens selected:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> <li>bed</li> </ul> <p>Possible output: The cat sat on the bed</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case-key-difference-from-top-k","title":"Edge Case (Key Difference from Top-k)","text":"<p>Highly confident model</p> <p>Assume: A: 0.85, B: 0.07, C: 0.03, D: 0.03, E: 0.02</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B  </li> <li>Effective K = 2</li> </ul> <p>With top-k (k = 5):</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> </ul> <p>Top-p automatically reduces randomness when the model is confident.</p>"},{"location":"supporting_topics/decoding_strategies/#another-edge-case","title":"Another Edge Case","text":"<p>Uncertain model</p> <p>Assume: A: 0.20, B: 0.20, C: 0.20, D: 0.20, E: 0.20</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> <li>Effective K = 5</li> </ul> <p>Top-p expands the candidate set when uncertainty is high.</p>"},{"location":"supporting_topics/decoding_strategies/#when-top-p-works-well","title":"When top-p works well","text":"<ul> <li>Conversational agents</li> <li>Long-form text generation</li> <li>Creative writing with coherence</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-top-p-works-poorly","title":"When top-p works poorly","text":"<ul> <li>Strictly deterministic tasks</li> <li>Code generation with exact formatting requirements</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_3","title":"Python example","text":"<pre><code>import torch\n\ndef top_p_sampling(probs, p):\n    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n    cumulative = torch.cumsum(sorted_probs, dim=0)\n\n    cutoff_mask = cumulative &lt;= p\n    cutoff_mask[cutoff_mask.sum()] = True\n\n    filtered_probs = sorted_probs[cutoff_mask]\n    filtered_probs = filtered_probs / filtered_probs.sum()\n\n    sampled = torch.multinomial(filtered_probs, 1)\n    return sorted_idx[cutoff_mask][sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_p_sampling(probs, p=0.9)\nprint(token.item())\n</code></pre> <p>Note : Top-p sampling adapts to the probability distribution shape, making it more robust than top-k for real-world language generation.</p>"},{"location":"supporting_topics/decoding_strategies/#3-pros-and-cons-of-decoding-strategies-in-large-language-models","title":"3. Pros and Cons of Decoding Strategies in Large Language Models","text":""},{"location":"supporting_topics/decoding_strategies/#31-greedy-decoding","title":"3.1 Greedy Decoding","text":""},{"location":"supporting_topics/decoding_strategies/#pros","title":"Pros","text":"<ul> <li>Extremely fast and simple</li> <li>Fully deterministic and reproducible</li> <li>Easy to debug and analyze</li> <li>Works well when the model is very confident</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons","title":"Cons","text":"<ul> <li>No diversity at all</li> <li>Easily gets stuck in repetitive loops</li> <li>Early mistakes cannot be corrected</li> <li>Often produces dull or incomplete responses</li> <li>Poor performance for long or open-ended generation</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#32-beam-search","title":"3.2 Beam Search","text":""},{"location":"supporting_topics/decoding_strategies/#pros_1","title":"Pros","text":"<ul> <li>Optimizes global sequence likelihood</li> <li>Reduces early local decision errors</li> <li>Produces fluent and grammatically correct text</li> <li>Effective for tasks with a single correct output</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_1","title":"Cons","text":"<ul> <li>Computationally expensive</li> <li>Produces generic and safe outputs</li> <li>Very low diversity</li> <li>All beams often converge to similar sequences</li> <li>Performs poorly for dialogue and creative tasks</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#33-temperature-sampling","title":"3.3 Temperature Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#pros_2","title":"Pros","text":"<ul> <li>Simple and intuitive control over randomness</li> <li>Enables creative and diverse outputs</li> <li>Easy to combine with other sampling methods</li> <li>Useful for brainstorming and storytelling</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_2","title":"Cons","text":"<ul> <li>High temperature can cause incoherent text</li> <li>Low temperature collapses to greedy behavior</li> <li>Does not prevent sampling of very unlikely tokens</li> <li>Sensitive to temperature tuning</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#34-top-k-sampling","title":"3.4 Top-k Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#pros_3","title":"Pros","text":"<ul> <li>Prevents extremely low-probability tokens</li> <li>Provides controlled randomness</li> <li>Simple to implement</li> <li>More diverse than greedy and beam search</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_3","title":"Cons","text":"<ul> <li>Fixed K ignores distribution shape</li> <li>Sensitive to the choice of K</li> <li>Removes valid tokens in flat distributions</li> <li>Not adaptive to model confidence</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#35-top-p-sampling-nucleus-sampling","title":"3.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"supporting_topics/decoding_strategies/#pros_4","title":"Pros","text":"<ul> <li>Adapts automatically to model confidence</li> <li>Better diversity-quality tradeoff than top-k</li> <li>Stable across different prompts</li> <li>Widely used in modern chat models</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_4","title":"Cons","text":"<ul> <li>Slightly more complex than top-k</li> <li>Still stochastic and non-deterministic</li> <li>Can include many tokens in very flat distributions</li> <li>Less suitable for strictly deterministic tasks</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#4-high-level-comparison","title":"4. High-level Comparison","text":"Strategy Diversity Determinism Adaptivity Typical Usage Greedy Very low High No Baselines, debugging Beam Search Low Medium No Translation, ASR Temperature Medium to high Low Partial Creative text Top-k Medium Low No General generation Top-p Medium to high Low Yes Chat and dialogue"},{"location":"supporting_topics/flash_attention/","title":"Flash Attention","text":""},{"location":"supporting_topics/flash_attention/#1-overview","title":"1. Overview","text":"<p>FlashAttention is a fast and memory-efficient implementation of the attention mechanism used in Transformer models. This repository explains what FlashAttention is, why it is faster than standard attention, and how it works under the hood, with a focus on interview preparation and practical understanding.</p>"},{"location":"supporting_topics/flash_attention/#2-motivation","title":"2. Motivation","text":"<p>Attention is the core operation behind Transformers, but standard attention becomes a major bottleneck for long sequences. The main problem is not only compute, but memory movement, which is often the true limiter on modern GPUs.</p> <p>FlashAttention was introduced to:</p> <ul> <li>Reduce memory usage  </li> <li>Minimize expensive GPU memory reads and writes  </li> <li>Scale efficiently to long sequences  </li> </ul>"},{"location":"supporting_topics/flash_attention/#3-standard-attention-and-its-limitations","title":"3. Standard Attention and Its Limitations","text":"<p>Given query, key, and value matrices:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V \\] <p>While simple and elegant, this formulation has serious performance and memory issues for long sequences.</p>"},{"location":"supporting_topics/flash_attention/#31-quadratic-memory-growth","title":"3.1 Quadratic Memory Growth","text":"<p>Assume:</p> <ul> <li>Sequence length \\(N = 16{,}384\\) </li> <li>FP16 precision (2 bytes per element)  </li> </ul> <p>The attention score matrix \\(QK^T\\) has:</p> \\[ N^2 = 16{,}384^2 \\approx 268 \\text{ million elements} \\] <p>Memory required just for the attention matrix:</p> \\[ 268\\text{M} \\times 2 \\text{ bytes} \\approx 512 \\text{ MB} \\] <p>This does not include the softmax output, gradients during training, or activations from other layers, which can easily exceed GPU memory limits.</p>"},{"location":"supporting_topics/flash_attention/#32-excessive-memory-traffic","title":"3.2 Excessive Memory Traffic","text":"<p>Standard attention performs multiple memory-heavy steps:</p> <ol> <li>Compute \\(QK^T\\) and write to GPU global memory  </li> <li>Read \\(QK^T\\) back to apply softmax  </li> <li>Write softmax output back to memory  </li> <li>Read softmax output again to compute weighted sum with \\(V\\) </li> </ol> <p>Even with fast compute, repeated global memory reads and writes dominate runtime, making GPUs often memory-bound rather than compute-bound.</p>"},{"location":"supporting_topics/flash_attention/#33-inefficient-for-long-sequences-code-example","title":"3.3 Inefficient for Long Sequences (Code Example)","text":"<p>A simplified PyTorch-style implementation:</p> <pre><code>import torch\nimport math\n\n# Q, K, V shape: (batch, seq_len, num_heads, head_dim)\nscores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d)\nattn = torch.softmax(scores, dim=-1)\noutput = torch.matmul(attn, V)\n</code></pre> <p>What happens internally:</p> <ul> <li>scores materializes a full \\(N\u00d7N\\) tensor</li> <li>attn creates another \\(N\u00d7N\\) tensor</li> <li>Both tensors live in global memory</li> </ul> <p>As N grows, memory usage and latency grow quadratically.</p>"},{"location":"supporting_topics/flash_attention/#34-numerical-issues-with-low-precision","title":"3.4 Numerical Issues with Low Precision","text":"<p>With FP16 or BF16:</p> <ul> <li>Large dot products in \\(QK^T\\) can overflow</li> <li>Small values can underflow to zero</li> </ul> <p>Standard attention often requires casting to FP32 for stability, which further increases memory usage and slows execution.</p>"},{"location":"supporting_topics/flash_attention/#4-what-is-flashattention-and-how-it-works","title":"4. What Is FlashAttention and How It Works","text":"<p>FlashAttention is an exact, memory-efficient attention algorithm. It computes the same result as standard attention but avoids materializing the full \\(N \\times N\\) attention matrix. This makes it much faster and reduces GPU memory usage, especially for long sequences.</p> <p>Key advantages:</p> <ul> <li>Handles long sequences efficiently (e.g., 4k+ tokens)  </li> <li>Works in FP16 and BF16 without numerical issues  </li> <li>Reduces memory bandwidth usage with minimal extra compute  </li> </ul> <p>FlashAttention achieves this through three main ideas: tiling, fused computation, and single-pass attention with online softmax.</p>"},{"location":"supporting_topics/flash_attention/#41-tiling","title":"4.1 Tiling","text":"<p>Instead of computing attention for the full sequence at once, FlashAttention splits the query, key, and value matrices into small tiles that fit into GPU shared memory.</p> <p>Example:</p> <ul> <li>Sequence length: \\(N = 16{,}384\\)</li> <li>Tile size: \\(B = 128\\) </li> </ul> <p>Memory usage for a tile: \\(128 \\times 128 = 16{,}384\\) elements (much smaller than \\((16{,}384)^2\\))  </p> <p>Code-style intuition:</p> <pre><code># pseudo-code for tiling\nfor q_tile in Q_tiles:\n    for k_tile, v_tile in zip(K_tiles, V_tiles):\n        partial_scores = q_tile @ k_tile.T\n        # accumulate results incrementally\n</code></pre> <p>Benefit: Only a small block is in memory at a time, reducing GPU memory footprint dramatically.</p>"},{"location":"supporting_topics/flash_attention/#42-fused-computation","title":"4.2 Fused Computation","text":"<p>FlashAttention fuses multiple steps into a single kernel:</p> <ol> <li>Matrix multiplication \\((Q \\cdot K^T)\\) </li> <li>Scaling by \\((1/\\sqrt{d})\\) </li> <li>Softmax computation  </li> <li>Weighted sum with \\((V)\\) </li> </ol> <p>Why this matters: </p> <ul> <li>Standard attention performs each step separately, writing intermediate results to global memory.  </li> <li>FlashAttention keeps all intermediate computations in shared memory, avoiding costly reads/writes.</li> </ul> <p>Example intuition:</p> <pre><code># pseudo-code for fused attention\noutput_tile = flash_attention(q_tile, k_tile, v_tile)\n</code></pre> <p>Here, flash_attention does all four steps at once, producing the final output for that tile.</p>"},{"location":"supporting_topics/flash_attention/#43-single-pass-attention-and-online-softmax","title":"4.3 Single-Pass Attention and Online Softmax","text":"<p>FlashAttention computes attention in one streaming pass:</p> <ul> <li>Compute partial scores for each tile</li> <li>Update running maximum and normalization term for softmax</li> <li>Accumulate output incrementally</li> </ul> <p>This allows numerically stable softmax in FP16/BF16 without ever storing the full attention matrix.</p> <p>Example numerical intuition:</p> <ul> <li>Tile 1 contributes scores [0.1, 0.5, 0.3]</li> <li>Tile 2 contributes [0.2, 0.4, 0.1]</li> <li>Running softmax computes the final normalized weights across tiles incrementally</li> </ul> <p>Benefit:</p> <ul> <li>Exact same result as full attention</li> <li>Avoids overflow/underflow in low precision</li> <li>Reduces memory reads/writes drastically</li> </ul>"},{"location":"supporting_topics/flash_attention/#44-practical-impact","title":"4.4 Practical Impact","text":"<ul> <li>Memory complexity reduced from \\(O(N^2) \u2192 O(N\u22c5B)\\) where \\(B\\) is tile size</li> <li>Enables training with longer sequences or larger batch sizes</li> <li>Provides 2\u20134x speedups for long sequences on modern GPUs</li> </ul> <p>Code example using PyTorch API: <pre><code>from flash_attn import flash_attn_func\n\n# q, k, v shape: (batch, seq_len, num_heads, head_dim)\noutput = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n</code></pre></p> <p>This produces exact attention results while being faster and more memory-efficient than standard attention.</p>"},{"location":"supporting_topics/flash_attention/#5-when-flashattention-helps-and-when-it-does-not","title":"5. When FlashAttention Helps (and When It Does Not)","text":"<p>Works best when:</p> <ul> <li>Sequence length is large (typically 2k tokens or more)</li> <li>Using FP16 or BF16</li> <li>Running on modern NVIDIA GPUs with fast shared memory</li> </ul> <p>Less useful when:</p> <ul> <li>Sequence length is very short</li> <li>CPU-based inference</li> <li>Custom attention patterns not supported by FlashAttention kernels</li> </ul>"},{"location":"supporting_topics/flash_attention/#6-why-is-online-softmax-needed","title":"6. Why is online softmax needed?","text":""},{"location":"supporting_topics/flash_attention/#61-numerical-stability-problem","title":"6.1. Numerical Stability Problem","text":"<p>Standard softmax is computed as:</p> \\[ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\] <p>Issue in FP16/BF16:</p> <ul> <li>FP16 has limited precision (~3\u20134 decimal digits) and a small exponent range.  </li> <li>Large values of \\((x_i)\\) (e.g., 50) cause \\(e^{x_i}\\) to overflow.  </li> <li>Very negative values of \\((x_i)\\) (e.g., -50) cause \\((e^{x_i})\\) to underflow to zero.  </li> <li>Long sequences exacerbate the problem because summing hundreds or thousands of exponentials increases the risk of overflow/underflow.  </li> </ul> <p>Without precautions, computing softmax in FP16 can produce NaNs or zeros, breaking both training and inference.</p>"},{"location":"supporting_topics/flash_attention/#62-why-online-softmax-helps","title":"6.2. Why \u201cOnline\u201d Softmax Helps","text":"<p>FlashAttention computes attention tile by tile, so it cannot store the full \\(N \\times N\\) attention matrix. To compute softmax correctly across the entire sequence in FP16/BF16, it uses online softmax.</p>"},{"location":"supporting_topics/flash_attention/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Maintain a running maximum \\(m\\) across tiles.</p> <ul> <li>Shift scores before exponentiating: \\(e^{x_i - m}\\)</li> <li>Prevents overflow in exponential.</li> </ul> </li> <li> <p>Maintain a running sum of exponentials across tiles.</p> <ul> <li>Partial sums from each tile are combined incrementally.  </li> <li>Ensures correct normalization for the softmax over the full sequence.</li> </ul> </li> <li> <p>Compute the weighted sum with \\(V\\) incrementally.</p> <ul> <li>No full softmax matrix is stored in memory.  </li> <li>Output is accumulated as each tile is processed.</li> </ul> </li> </ol>"},{"location":"supporting_topics/flash_attention/#example","title":"Example","text":"<p>Suppose we have 2 tiles with attention scores:</p> <ul> <li>Tile 1: <code>[0.1, 0.5, 0.3]</code> </li> <li>Tile 2: <code>[0.2, 0.4, 0.1]</code></li> </ul> <p>Standard softmax (if we could store all scores):</p> \\[ \\text{softmax}([0.1, 0.5, 0.3, 0.2, 0.4, 0.1]) \\] <p>Online softmax computation:</p> <ol> <li> <p>Tile 1 </p> <ul> <li>Running max \\(m = 0.5\\) </li> <li>Compute shifted exponentials: <code>[exp(0.1-0.5), exp(0.5-0.5), exp(0.3-0.5)] \u2248 [0.67, 1.0, 0.82]</code> </li> <li>Running sum \\(s = 0.67 + 1.0 + 0.82 = 2.49\\) </li> <li>Partial weighted sum with \\(V\\) stored in output</li> </ul> </li> <li> <p>Tile 2 </p> <ul> <li>New max \\(m = \\max(0.5, 0.4) = 0.5\\) (same in this case)  </li> <li>Shifted exponentials: <code>[exp(0.2-0.5), exp(0.4-0.5), exp(0.1-0.5)] \u2248 [0.74, 0.90, 0.61]</code> </li> <li>Update running sum: \\(s = 2.49 + 0.74 + 0.90 + 0.61 = 4.74\\) </li> <li>Accumulate weighted sum with \\(V\\)</li> </ul> </li> <li> <p>Normalization </p> <ul> <li>Each accumulated output is divided by the final sum \\(s = 4.74\\) </li> <li>Produces exact same softmax result as computing on the full sequence</li> </ul> </li> </ol>"},{"location":"supporting_topics/flash_attention/#key-benefits","title":"Key Benefits","text":"<ul> <li>Computes exact attention even in FP16/BF16  </li> <li>Works efficiently with long sequences and large tiles </li> <li>Avoids storing huge intermediate matrices  </li> <li>Reduces GPU memory usage and memory bandwidth overhead</li> </ul> <p>In short: Online softmax allows FlashAttention to compute attention tile by tile while staying numerically stable and memory-efficient.</p>"},{"location":"supporting_topics/flash_attention/#7-end-to-end-flashattention-example","title":"7. End-to-End FlashAttention Example","text":"<p>Suppose we have:</p> <ul> <li>Sequence length \\(N = 8\\) (small for simplicity)  </li> <li>Head dimension \\(d = 2\\) </li> <li>Tile size \\(B = 4\\) </li> </ul> <p>We want to compute attention for a single head:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V \\]"},{"location":"supporting_topics/flash_attention/#step-1-prepare-q-k-v","title":"Step 1: Prepare Q, K, V","text":"<pre><code>import torch\nimport math\n\nQ = torch.tensor([[0.1, 0.2],\n                  [0.3, 0.1],\n                  [0.0, 0.4],\n                  [0.5, 0.2],\n                  [0.3, 0.3],\n                  [0.1, 0.5],\n                  [0.4, 0.0],\n                  [0.2, 0.1]])  # shape: (8, 2)\n\nK = Q.clone()  # for simplicity\nV = torch.arange(8*2).reshape(8,2).float()  # dummy value matrix\n</code></pre>"},{"location":"supporting_topics/flash_attention/#step-2-split-into-tiles","title":"Step 2: Split into Tiles","text":"<p>To reduce memory usage, FlashAttention splits the sequence into smaller tiles that fit into GPU shared memory.</p> <ul> <li>Tile size \\(B=4\\) \u2192 2 tiles along the sequence  </li> </ul> <pre><code># Split Q, K, V into tiles\nQ_tiles = [Q[:4], Q[4:]]  # tile 1 and tile 2\nK_tiles = [K[:4], K[4:]]\nV_tiles = [V[:4], V[4:]]\n</code></pre> <p>Benefit: Only a small portion of the sequence is in memory at a time, avoiding the need to materialize the full attention matrix.</p>"},{"location":"supporting_topics/flash_attention/#step-3-process-tile-1","title":"Step 3: Process Tile 1","text":"<ol> <li> <p>Compute partial scores in shared memory:</p> \\[ \\text{scores} = Q_\\text{tile1} \\cdot K_\\text{tile1}^T / \\sqrt{d} \\] <pre><code>scores_tile1 = Q_tiles[0] @ K_tiles[0].T / math.sqrt(2)\n</code></pre> </li> <li> <p>Apply online softmax:</p> <ul> <li>Compute max of scores: m = scores_tile1.max(dim=1)</li> <li>Shift and exponentiate: exp_scores = torch.exp(scores_tile1 - m)</li> <li>Running sum: s = exp_scores.sum(dim=1)</li> <li>Partial weighted sum with V: output_tile1 = (exp_scores @ V_tiles[0]) / s</li> </ul> </li> </ol> <p>Memory benefit: only a 4\u00d74 matrix exists at a time.</p>"},{"location":"supporting_topics/flash_attention/#step-4-process-tile-2-incrementally","title":"Step 4: Process Tile 2 Incrementally","text":"<ul> <li>Compute partial scores of Q_tile1 \u00d7 K_tile2^T</li> <li>Update running max and running sum for online softmax</li> <li>Accumulate weighted outputs with V_tile2</li> <li>Repeat for Q_tile2 \u00d7 K_tile1^T and Q_tile2 \u00d7 K_tile2^T</li> </ul> <p>No full 8\u00d78 attention matrix is ever materialized.</p>"},{"location":"supporting_topics/flash_attention/#step-5-accumulate-output","title":"Step 5: Accumulate Output","text":"<ul> <li>Incrementally compute the weighted sum across all tiles</li> <li>Resulting output shape (8, 2) matches standard attention</li> <li>Softmax computed exactly using online normalization</li> </ul>"},{"location":"supporting_topics/kv_caching/","title":"KV Caching","text":""},{"location":"supporting_topics/kv_caching/#1-self-attention-recap","title":"\ud83d\udce61. Self Attention Recap","text":"<p>Given hidden states \\(X \\in \\mathbb{R}^{T \\times d}\\):</p> \\[ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V \\] <p>Per head attention:</p> \\[ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V \\] <p>Autoregressive decoding generates one token at a time with causal masking.</p>"},{"location":"supporting_topics/kv_caching/#2-why-kv-cache-is-needed","title":"\ud83d\udce62. Why KV Cache Is Needed","text":"<p>At decoding step \\(t\\), keys and values for tokens \\(1 \\ldots t-1\\) are unchanged but would be recomputed without caching.</p> <p>This repeated computation dominates inference latency and wastes FLOPs.</p>"},{"location":"supporting_topics/kv_caching/#3-kv-cache-mechanism","title":"\ud83d\udce63. KV Cache Mechanism","text":"<p>For each transformer layer \\(\\ell\\):</p> \\[ \\text{KVCache}_\\ell = \\{K_\\ell^{1:t}, V_\\ell^{1:t}\\} \\] <p>At decoding step \\(t\\):</p> <ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to the cache</li> <li>Attend over all cached keys and values</li> </ul> \\[ \\text{Attn}_t = \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Only the current token requires new computation.</p>"},{"location":"supporting_topics/kv_caching/#4-toy-example","title":"\ud83d\udce64. Toy Example","text":"<p>Prompt: \"I like neural\"</p> <p>Step 1: generate <code>\"networks\"</code></p> <ul> <li>Compute and cache keys and values for the prompt</li> <li>Attend to all cached tokens</li> </ul> <p>Step 2: generate <code>\"models\"</code></p> <ul> <li>Reuse cached keys and values</li> <li>Compute keys and values only for <code>\"networks\"</code></li> </ul> <p>Previously generated tokens are never recomputed.</p>"},{"location":"supporting_topics/kv_caching/#5-complexity-analysis","title":"\ud83d\udce65. Complexity Analysis","text":""},{"location":"supporting_topics/kv_caching/#notation","title":"Notation","text":"<ul> <li>\\(T\\): number of generated tokens</li> <li>\\(L\\): number of transformer layers</li> <li>\\(H\\): number of attention heads</li> <li>\\(d_h\\): head dimension</li> </ul>"},{"location":"supporting_topics/kv_caching/#without-kv-cache","title":"Without KV Cache","text":"<p>At each decoding step, attention is recomputed for all previous tokens:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^3) \\]"},{"location":"supporting_topics/kv_caching/#with-kv-cache","title":"With KV Cache","text":"<p>Only attention against cached keys and values is computed:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^2) \\] <p>KV caching removes one full factor of \\(T\\) from decoding complexity.</p>"},{"location":"supporting_topics/kv_caching/#6-memory-cost","title":"\ud83d\udce66. Memory Cost","text":"<p>Each layer stores:</p> \\[ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} \\] <p>Total KV cache memory across all layers:</p> \\[ O(L \\cdot H \\cdot T \\cdot d_h) \\] <p>For long context inference, KV cache memory is often the dominant bottleneck.</p>"},{"location":"supporting_topics/kv_caching/#7-inference-vs-training-usage","title":"\ud83d\udce67. Inference v/s Training Usage","text":""},{"location":"supporting_topics/kv_caching/#71-during-inference","title":"7.1 During Inference","text":"<p>This is the most common and important usage.</p> <p>Inference Workflow</p> <ul> <li>Encode prompt</li> <li>Initialize empty KV cache per layer</li> <li>For each generated token:<ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to cache</li> </ul> </li> <li>Compute attention using cached tensors</li> </ul> <p>Practical Benefits</p> <ul> <li>Faster decoding</li> <li>Lower FLOPs</li> <li>Enables long context generation</li> <li>Essential for streaming and chat systems</li> </ul>"},{"location":"supporting_topics/kv_caching/#72-during-training","title":"7.2 During Training","text":"<p>KV caching is not used in standard full sequence training.</p> <p>Why?</p> <ul> <li>Training processes full sequences in parallel</li> <li>All tokens attend to each other simultaneously</li> <li>No repeated computation across steps</li> </ul>"},{"location":"supporting_topics/kv_caching/#8-scaling-kv-cache-for-long-context","title":"\ud83d\udce68. Scaling KV Cache for Long Context","text":"<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>"},{"location":"supporting_topics/kv_caching/#81-sliding-window-attention","title":"8.1 Sliding Window Attention","text":"<p>Only retain keys and values for the most recent \\(W\\) tokens:</p> \\[ K_{t-W:t}, V_{t-W:t} \\] <p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>"},{"location":"supporting_topics/kv_caching/#82-kv-cache-quantization","title":"8.2 KV Cache Quantization","text":"<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>"},{"location":"supporting_topics/kv_caching/#what-gets-quantized","title":"What Gets Quantized","text":"<p>Both keys and values can be quantized, but they have different sensitivity:</p> <ul> <li>Keys (K) directly affect attention scores \\(QK^T\\)</li> <li>Values (V) affect the weighted sum after softmax</li> </ul> <p>As a result:</p> <ul> <li>Keys usually require higher precision</li> <li>Values tolerate more aggressive quantization</li> </ul>"},{"location":"supporting_topics/kv_caching/#common-quantization-schemes","title":"Common Quantization Schemes","text":"Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios <p>Mixed precision KV cache is widely used in practice.</p>"},{"location":"supporting_topics/kv_caching/#quantization-granularity","title":"Quantization Granularity","text":"<p>KV cache quantization can be applied at different levels:</p> <ul> <li>Per tensor: One scale for entire K or V tensor</li> <li>Per head: Separate scale per attention head</li> <li>Per channel: Separate scale per head dimension</li> </ul> <p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>"},{"location":"supporting_topics/kv_caching/#dequantization-during-attention","title":"Dequantization During Attention","text":"<p>At decoding step \\(t\\):</p> <ol> <li>Load quantized \\(K, V\\) from cache</li> <li>Dequantize to FP16 or BF16</li> <li>Compute attention normally:</li> </ol> \\[ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Dequantization cost is small compared to memory bandwidth savings.</p>"},{"location":"supporting_topics/kv_caching/#impact-on-performance","title":"Impact on Performance","text":"<p>Benefits:</p> <ul> <li>2x to 4x KV memory reduction</li> <li>Higher batch size and longer context</li> <li>Improved inference throughput due to reduced memory traffic</li> </ul> <p>Tradeoffs:</p> <ul> <li>Slight loss in generation quality</li> <li>Additional dequantization overhead</li> </ul> <p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>"},{"location":"supporting_topics/kv_caching/#interaction-with-other-optimizations","title":"Interaction with Other Optimizations","text":"<ul> <li>GQA further reduces KV cache size and works well with quantization</li> <li>Paged KV cache benefits from smaller KV blocks</li> <li>FlashAttention amortizes dequantization overhead inside fused kernels</li> </ul>"},{"location":"supporting_topics/kv_caching/#83-prefix-caching","title":"8.3 Prefix Caching","text":"<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>"},{"location":"supporting_topics/kv_caching/#84-paged-kv-cache","title":"8.4 Paged KV Cache","text":"<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>"},{"location":"supporting_topics/kv_caching/#9-grouped-query-attention-gqa","title":"\ud83d\udce69. Grouped Query Attention (GQA)","text":"<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>"},{"location":"supporting_topics/kv_caching/#91-head-configuration","title":"9.1 Head Configuration","text":"\\[ H_q &gt; H_k = H_v \\] <p>Example:</p> <ul> <li>Query heads \\(H_q = 32\\)</li> <li>Key value heads \\(H_k = 8\\)</li> </ul> <p>This reduces KV cache memory by a factor of \\(H_q / H_k\\).</p>"},{"location":"supporting_topics/kv_caching/#92-qk-computation-with-mismatched-heads","title":"9.2 QK Computation with Mismatched Heads","text":"<p>Each key value head is shared by a fixed group of query heads.</p> <p>Let:</p> \\[ g = \\frac{H_q}{H_k} \\] <p>Each key value head serves \\(g\\) query heads.</p> <p>For query head \\(i\\), the corresponding key value head index is:</p> \\[ \\left\\lfloor \\frac{i}{g} \\right\\rfloor \\] <p>The attention computation becomes:</p> \\[ \\text{Attn}_i = \\text{softmax}\\left(\\frac{Q_i K_{\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} \\] <p>Keys and values are reused directly without additional projection or averaging.</p>"},{"location":"supporting_topics/kv_caching/#93-why-gqa-is-effective","title":"9.3 Why GQA Is Effective","text":"<ul> <li>Query heads retain expressive power</li> <li>Keys and values capture shared context</li> <li>KV cache size and memory bandwidth are significantly reduced</li> </ul> <p>GQA is widely used in production LLMs.</p>"},{"location":"supporting_topics/kv_caching/#10-other-common-optimizations","title":"\ud83d\udce610. Other Common Optimizations","text":""},{"location":"supporting_topics/kv_caching/#flashattention","title":"FlashAttention","text":"<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>"},{"location":"supporting_topics/kv_caching/#chunked-prefill","title":"Chunked Prefill","text":"<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>"},{"location":"supporting_topics/kv_caching/#speculative-decoding","title":"Speculative Decoding","text":"<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p>"},{"location":"supporting_topics/paged_optimizers/","title":"Paged optimizers","text":""},{"location":"supporting_topics/paged_optimizers/#5-paged-optimizers","title":"\ud83d\udce6 5. Paged Optimizers","text":""},{"location":"supporting_topics/paged_optimizers/#overview","title":"Overview","text":"<p>Paged optimizers are designed to manage memory more efficiently during training by dynamically moving optimizer states between CPU and GPU memory. This approach is particularly useful when dealing with large models that exceed GPU memory capacity.</p>"},{"location":"supporting_topics/paged_optimizers/#mechanism","title":"Mechanism","text":"<ul> <li>Dynamic Memory Management: Optimizer states are stored in CPU memory and paged into GPU memory as needed.</li> <li>Efficient Data Transfer: Minimizes data transfer overhead by batching optimizer state updates.</li> </ul>"},{"location":"supporting_topics/paged_optimizers/#advantages","title":"Advantages","text":"<ul> <li>Reduced GPU Memory Usage: Allows training of larger models on GPUs with limited memory.</li> <li>Scalability: Facilitates scaling to models with billions of parameters.</li> </ul>"},{"location":"supporting_topics/prefix_caching/","title":"Prefix caching","text":""},{"location":"supporting_topics/prefix_caching/#paged-attention","title":"Paged Attention","text":""},{"location":"supporting_topics/speculative_decoding/","title":"Speculative Decoding & Medusa","text":""},{"location":"supporting_topics/speculative_decoding/#1-speculative-decoding-overview","title":"\ud83d\udce61. Speculative Decoding: Overview","text":"<p>Speculative decoding reduces inference latency in decoder-only LLMs while preserving the exact output distribution of a large target model.</p>"},{"location":"supporting_topics/speculative_decoding/#11-why-standard-decoding-is-slow","title":"1.1 Why standard decoding is slow","text":"<p>In standard autoregressive decoding:</p> <ul> <li>The target model generates one token per forward pass</li> <li>The model is large and expensive</li> <li>Latency grows linearly with output length</li> </ul> <p>KV cache reduces computation but does not remove the sequential bottleneck.</p>"},{"location":"supporting_topics/speculative_decoding/#12-core-idea","title":"1.2 Core idea","text":"<p>Speculative decoding separates token proposal from token verification:</p> <ul> <li>A draft model proposes multiple tokens cheaply</li> <li>A target model verifies them efficiently</li> </ul> <p>If most draft tokens are accepted, multiple tokens are generated per expensive target model forward pass.</p>"},{"location":"supporting_topics/speculative_decoding/#2-background-how-decoding-works-in-decoder-only-llms","title":"\ud83d\udce62. Background: How Decoding Works in Decoder-Only LLMs","text":"<p>Before speculative decoding, it is critical to understand standard autoregressive decoding.</p>"},{"location":"supporting_topics/speculative_decoding/#21-autoregressive-modeling-assumption","title":"2.1 Autoregressive modeling assumption","text":"<p>A decoder-only LLM models a sequence of tokens using the following factorization:</p> \\[ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\dots, x_{t-1}) \\] <p>Key points:</p> <ul> <li>Tokens are generated left to right</li> <li>Each new token depends on all previous tokens</li> <li>There is no notion of predicting multiple future tokens independently</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#22-what-happens-in-one-forward-pass","title":"2.2 What happens in one forward pass","text":"<p>Assume the current sequence length is \\(T\\).</p>"},{"location":"supporting_topics/speculative_decoding/#step-1-embedding","title":"Step 1: Embedding","text":"<p>Each token \\(x_t\\) is mapped to a vector representation:</p> \\[ \\mathbf{e}_t = \\text{TokenEmbed}(x_t) + \\text{PosEmbed}(t) \\]"},{"location":"supporting_topics/speculative_decoding/#step-2-masked-self-attention","title":"Step 2: Masked self-attention","text":"<p>For each token position \\(t\\):</p> \\[ \\mathbf{q}_t = \\mathbf{e}_t W_Q,\\quad \\mathbf{k}_t = \\mathbf{e}_t W_K,\\quad \\mathbf{v}_t = \\mathbf{e}_t W_V \\] <p>Attention scores are computed as:</p> \\[ \\alpha_{t,i} = \\frac{\\mathbf{q}_t \\cdot \\mathbf{k}_i}{\\sqrt{d_k}} \\] <p>A causal mask ensures token \\(t\\) can only attend to tokens \\(i \\le t\\).</p> <p>The attended representation is:</p> \\[ \\mathbf{a}_t = \\sum_{i=1}^{t} \\text{softmax}(\\alpha_{t,i}) \\mathbf{v}_i \\] <p>This is followed by a linear projection:</p> \\[ \\mathbf{o}_t = \\mathbf{a}_t W_O \\]"},{"location":"supporting_topics/speculative_decoding/#step-3-feed-forward-network-ffn","title":"Step 3: Feed Forward Network (FFN)","text":"<p>Each token is processed independently:</p> \\[ \\mathbf{h}_t = \\text{FFN}(\\mathbf{o}_t) \\] <p>Residual connections and layer normalization are applied around both attention and FFN blocks.</p> <p>This completes one decoder layer. The same process repeats across multiple stacked layers.</p>"},{"location":"supporting_topics/speculative_decoding/#23-computing-logits-and-selecting-the-next-token","title":"2.3 Computing logits and selecting the next token","text":"<p>After the final decoder layer, each token position has a hidden state \\(\\mathbf{h}_t\\).</p> <p>These are projected to vocabulary logits:</p> \\[ \\mathbf{z}_t = \\mathbf{h}_t W_{\\text{vocab}} \\quad \\text{where } \\mathbf{z}_t \\in \\mathbb{R}^{|V|} \\] <p>Important clarification:</p> <ul> <li>Logits are computed for every token position</li> <li>Softmax is applied over the vocabulary</li> <li>During decoding, only the last position is used</li> </ul> \\[ P(x_{T+1} \\mid x_{\\le T}) = \\text{softmax}(\\mathbf{z}_T) \\] <p>A token is selected using greedy decoding or sampling and appended to the sequence.</p>"},{"location":"supporting_topics/speculative_decoding/#24-autoregressive-decoding-loop","title":"2.4 Autoregressive decoding loop","text":"<p>For each generated token:</p> <ol> <li>Run the Transformer forward pass</li> <li>Take logits from the last token position</li> <li>Apply softmax over the vocabulary</li> <li>Select one token</li> <li>Append it to the sequence</li> <li>Repeat</li> </ol> <p>This process continues until an end-of-sequence token is produced or a maximum length is reached.</p>"},{"location":"supporting_topics/speculative_decoding/#25-key-limitation-of-standard-decoding","title":"2.5 Key limitation of standard decoding","text":"<ul> <li>Each generated token requires a new forward pass of the model</li> <li>Latency grows linearly with output l</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#3-step-by-step-algorithm-for-speculative-decoding","title":"\ud83d\udce63. Step-by-Step Algorithm for Speculative Decoding","text":"<p>This section describes the speculative decoding algorithm precisely, step by step, focusing on what each model does and why it is needed.</p> <p>Assume:</p> <ul> <li>Prompt tokens: \\(x\\)</li> <li>Draft model: \\(q\\)</li> <li>Target model: \\(p\\)</li> <li>Draft length: \\(k\\)</li> <li>Drafted tokens: \\(y_1, y_2, \\dots, y_k\\)</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-1-draft-model-proposes-tokens","title":"Step 1: Draft model proposes tokens","text":"<p>The draft model generates tokens autoregressively, starting from the prompt:</p> \\[ y_i \\sim q(\\cdot \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1 \\dots k \\] <p>Key points:</p> <ul> <li>This step is fast because the draft model is small</li> <li>Tokens are sampled, not greedily selected</li> <li>The draft model also records the probability of each sampled token</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-2-target-model-verifies-the-draft","title":"Step 2: Target model verifies the draft","text":"<p>The target model is run once on the combined sequence: \\([x, y1, y2, ..., yk]\\)</p> <p>This produces target model probabilities:</p> \\[ p(y_i \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1 \\dots k \\] <p>Important clarification:</p> <ul> <li>The target model naturally computes logits for all positions</li> <li>Only logits corresponding to the drafted tokens are used</li> <li>Logits for the prompt tokens are ignored</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-3-acceptance-test","title":"Step 3: Acceptance test","text":"<p>Each drafted token is accepted independently using:</p> \\[ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{&lt;i})}{q(y_i \\mid x, y_{&lt;i})}\\right) \\] <p>Procedure:</p> <ol> <li>Sample \\(u \\sim \\text{Uniform}(0, 1)\\)</li> <li>Accept token \\(y_i\\) if \\(u \\le \\alpha_i\\)</li> <li>Stop at the first rejected token</li> </ol>"},{"location":"supporting_topics/speculative_decoding/#step-4-rejection-handling-and-fallback","title":"Step 4: Rejection handling and fallback","text":"<p>If token \\(y_j\\) is rejected:</p> <ul> <li>Tokens \\(y_j, y_{j+1}, \\dots, y_k\\) are discarded</li> <li>The next token is sampled directly from the target model:   $$   x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{&lt;j})   $$</li> <li>Speculative decoding restarts from the new prefix</li> </ul> <p>If no token is rejected, all \\(k\\) draft tokens are accepted.</p>"},{"location":"supporting_topics/speculative_decoding/#why-this-preserves-correctness","title":"Why this preserves correctness","text":"<ul> <li>The acceptance rule implements rejection sampling</li> <li>Bias introduced by the draft model is corrected</li> <li>The final output distribution exactly matches the target model</li> </ul> <p>This guarantees that speculative decoding is statistically equivalent to standard decoding with the target model.</p>"},{"location":"supporting_topics/speculative_decoding/#4-why-speculative-decoding-is-faster","title":"\ud83d\udce64. Why Speculative Decoding Is Faster","text":"<p>Speculative decoding reduces inference latency by decreasing how often the expensive target model must be executed.</p>"},{"location":"supporting_topics/speculative_decoding/#41-standard-decoding-vs-speculative-decoding","title":"4.1 Standard decoding vs speculative decoding","text":"<p>Standard decoding</p> <ul> <li>One target model forward pass per generated token</li> <li>For \\(N\\) tokens, \\(N\\) forward passes are required</li> </ul> <p>Speculative decoding</p> <ul> <li>The draft model proposes \\(k\\) tokens cheaply</li> <li>A single target model forward pass verifies up to \\(k\\) tokens</li> <li>Multiple tokens can be generated per target model invocation</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#42-source-of-the-speedup","title":"4.2 Source of the speedup","text":"<p>The speedup comes from two properties:</p> <ul> <li>The draft model is significantly cheaper than the target model</li> <li>The target model can evaluate multiple draft tokens in parallel</li> </ul> <p>If the acceptance rate is high, the target model is called much less frequently.</p>"},{"location":"supporting_topics/speculative_decoding/#43-what-speculative-decoding-does-not-optimize","title":"4.3 What speculative decoding does not optimize","text":"<p>Speculative decoding does not reduce:</p> <ul> <li>The total number of FLOPs in the target model</li> <li>The per-token computation inside the Transformer</li> </ul> <p>It primarily reduces latency, not theoretical compute.</p>"},{"location":"supporting_topics/speculative_decoding/#44-practical-speedups","title":"4.4 Practical speedups","text":"<p>In practice, speculative decoding often achieves:</p> <ul> <li>1.5x to 3x latency improvement</li> <li>Higher gains when draft and target distributions are close</li> </ul> <p>Actual speedup depends on model sizes, hardware, and acceptance rate.</p>"},{"location":"supporting_topics/speculative_decoding/#5-relationship-to-logits-for-all-tokens","title":"5. Relationship to Logits for All Tokens","text":"<p>Speculative decoding does not introduce a new requirement to compute logits for all tokens.</p>"},{"location":"supporting_topics/speculative_decoding/#51-standard-transformer-behavior","title":"5.1 Standard Transformer behavior","text":"<p>A Transformer forward pass naturally produces:</p> <ul> <li>One hidden state per token position</li> <li>One vocabulary logits vector per token position</li> </ul> <p>This is true during both training and inference.</p>"},{"location":"supporting_topics/speculative_decoding/#52-how-logits-are-used-in-speculative-decoding","title":"5.2 How logits are used in speculative decoding","text":"<p>During verification:</p> <ul> <li>The target model is run once on the prompt plus draft tokens</li> <li>Logits for prompt tokens are ignored</li> <li>Only logits corresponding to the draft token positions are used</li> </ul> <p>Speculative decoding simply reuses standard per-position logits.</p>"},{"location":"supporting_topics/speculative_decoding/#53-what-speculative-decoding-does-not-do","title":"5.3 What speculative decoding does not do","text":"<p>Speculative decoding does not:</p> <ul> <li>Perform softmax over token positions</li> <li>Predict future tokens independently</li> <li>Generate tokens in parallel from the target model</li> </ul> <p>The target model still defines an autoregressive distribution.</p>"},{"location":"supporting_topics/speculative_decoding/#6-interaction-with-kv-cache","title":"6. Interaction with KV Cache","text":"<p>KV cache improves performance in speculative decoding but does not change the algorithm.</p>"},{"location":"supporting_topics/speculative_decoding/#61-kv-cache-in-the-draft-model","title":"6.1 KV cache in the draft model","text":"<ul> <li>The draft model maintains its own KV cache</li> <li>Draft tokens are generated autoregressively</li> <li>KV cache allows fast token proposal</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#62-kv-cache-in-the-target-model","title":"6.2 KV cache in the target model","text":"<ul> <li>The target model computes KV cache for the entire speculative window</li> <li>KV states corresponding to accepted tokens are reused</li> <li>KV states for rejected tokens are discarded</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#63-why-kv-cache-matters","title":"6.3 Why KV cache matters","text":"<p>KV cache:</p> <ul> <li>Avoids recomputing attention for previously processed tokens</li> <li>Reduces per-step computation</li> <li>Improves practical throughput</li> </ul> <p>KV cache affects efficiency only, not correctness.</p>"},{"location":"training_techniques/distributed_training_systems/","title":"Distributed Training Systems","text":"<p>This page covers the system architecture, parallelism strategies, and engineering trade offs required to train large language models at scale.</p>"},{"location":"training_techniques/distributed_training_systems/#1-data-parallelism-dp","title":"1. Data Parallelism (DP)","text":"<p>The most widely used baseline parallelism strategy where the full model is replicated across \\(N\\) workers and each worker processes a different shard of the input batch.</p>"},{"location":"training_techniques/distributed_training_systems/#core-mechanism","title":"Core Mechanism","text":"<ul> <li>Each GPU performs forward and backward passes on its local mini batch.</li> <li>Gradients are synchronized across all workers using All Reduce.</li> <li>After synchronization, every replica applies the same optimizer update, keeping parameters identical.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#pytorch-distributed-data-parallel-ddp-details","title":"PyTorch Distributed Data Parallel (DDP) Details","text":""},{"location":"training_techniques/distributed_training_systems/#gradient-bucketing","title":"Gradient Bucketing","text":"<ul> <li>Model parameters are grouped into buckets based on size (The default bucket size is about 25 MB, configurable via bucket_cap_mb).</li> <li>Instead of waiting for all gradients to be computed, DDP starts communication as soon as a bucket is ready.</li> <li>This reduces idle time by avoiding one large synchronization at the end of backward pass.</li> </ul> <p>Why this matters: Smaller, incremental communication keeps GPUs busy and reduces the impact of network latency.</p>"},{"location":"training_techniques/distributed_training_systems/#asynchronous-gradient-reduction","title":"Asynchronous Gradient Reduction","text":"<ul> <li>When gradients for a bucket are computed, DDP launches an asynchronous All Reduce operation.</li> <li>While communication is happening, the backward pass continues computing gradients for later layers.</li> <li>This overlaps computation and communication.</li> </ul> <p>Key insight: Ideally, by the time backward computation finishes, most gradient communication is already done.</p>"},{"location":"training_techniques/distributed_training_systems/#synchronization-timing","title":"Synchronization Timing","text":"<ul> <li>Gradients are synchronized once per training iteration, not per layer.</li> <li>Each parameter\u2019s gradient is reduced exactly once after it is computed.</li> <li>The optimizer step is performed only after all gradient reductions complete.</li> </ul> <p>Common misconception: DDP does not pause after every layer to synchronize. Synchronization is event driven and overlaps with backpropagation.</p>"},{"location":"training_techniques/distributed_training_systems/#limitations","title":"Limitations","text":"<ul> <li>Model parameters, gradients, and optimizer states must fit on a single GPU.</li> <li>Scaling is limited by global batch size and optimizer stability.</li> <li>Communication cost grows linearly with the number of GPUs.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#2-tensor-parallelism-tp","title":"2. Tensor Parallelism (TP)","text":"<p>Tensor Parallelism splits individual layers across devices so that a single layer computation is distributed.</p>"},{"location":"training_techniques/distributed_training_systems/#21-core-mechanism","title":"2.1 Core Mechanism","text":"<p>Large weight matrices are partitioned across GPUs.</p> <p>Consider a linear layer:</p> <p>Y = XW</p> <p>where:</p> <ul> <li>X has shape <code>[batch, hidden_in]</code></li> <li>W has shape <code>[hidden_in, hidden_out]</code></li> <li>Y has shape <code>[batch, hidden_out]</code></li> <li>Column parallel splits <code>W</code> by output dimension.</li> <li>Row parallel splits <code>W</code> by input dimension.</li> <li>Each GPU computes a partial result which is combined using collective communication.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#column-parallelism-output-dimension-split","title":"Column Parallelism (Output Dimension Split)","text":"<p>In column parallelism, the weight matrix W is split by columns:</p> <p>W = [W\u2081 | W\u2082 | ... | W\u2096]</p> <p>Each GPU holds:</p> <ul> <li>W\u1d62 with shape <code>[hidden_in, hidden_out / k]</code></li> </ul> <p>Each GPU computes:</p> <ul> <li>Y\u1d62 = X \u00b7 W\u1d62</li> </ul> <p>At this point:</p> <ul> <li>Each Y\u1d62 is only a partial output.</li> <li>The full output Y is formed by concatenating all Y\u1d62 along the feature dimension.</li> </ul> <p>Communication Pattern</p> <ul> <li>An All Gather is used to assemble the full Y across GPUs.</li> <li>This happens during the forward pass.</li> <li>During backpropagation, gradients w.r.t. X require an All Reduce.</li> </ul> <p>Key Insight</p> <p>Column parallelism parallelizes the output features and is commonly used in feed forward layers.</p>"},{"location":"training_techniques/distributed_training_systems/#row-parallelism-input-dimension-split","title":"Row Parallelism (Input Dimension Split)","text":"<p>In row parallelism, the weight matrix W is split by rows:</p> <p>W = [ W\u2081       W\u2082       ...       W\u2096 ]</p> <p>Each GPU holds:</p> <ul> <li>W\u1d62 with shape <code>[hidden_in / k, hidden_out]</code></li> </ul> <p>The input X is also split:</p> <ul> <li>X = [X\u2081 | X\u2082 | ... | X\u2096]</li> </ul> <p>Each GPU computes:</p> <ul> <li>Y\u1d62 = X\u1d62 \u00b7 W\u1d62</li> </ul> <p>Now:</p> <ul> <li>Each Y\u1d62 is a partial sum of the final output.</li> </ul> <p>Communication Pattern</p> <ul> <li>An All Reduce is required to sum Y\u1d62 across GPUs.</li> <li>The final Y is identical on all GPUs after reduction.</li> <li>Backward pass mirrors this communication pattern.</li> </ul> <p>Key Insight</p> <p>Row parallelism parallelizes the input features and avoids an All Gather in the forward pass.</p>"},{"location":"training_techniques/distributed_training_systems/#why-two-parallelization-schemes-exist","title":"Why Two Parallelization Schemes Exist","text":"<p>Column and row parallelism are complementary: - Column parallelism produces partial outputs that must be gathered. - Row parallelism produces partial sums that must be reduced.</p> <p>Modern transformer implementations alternate between them: - Column parallel for linear projections. - Row parallel for output projections.</p> <p>This design minimizes total communication while keeping memory balanced.</p>"},{"location":"training_techniques/distributed_training_systems/#22-communication-characteristics","title":"2.2 Communication Characteristics","text":"<ul> <li>Requires All Reduce or All Gather inside the forward and backward pass.</li> <li>Communication is frequent and latency sensitive.</li> <li>Performance depends heavily on interconnect bandwidth.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#insights","title":"Insights","text":"<p>Strengths</p> <ul> <li>Enables training when individual layers do not fit on a single GPU.</li> <li>Reduces per GPU activation and parameter memory.</li> </ul> <p>Constraints</p> <ul> <li>Communication overhead is high.</li> <li>Usually restricted within a node due to bandwidth requirements.</li> <li>Sensitive to imbalance across tensor shards.</li> </ul> <p>Practical Usage</p> <ul> <li>Often combined with Data Parallelism.</li> <li>Popularized by Megatron LM style architectures.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#3-pipeline-parallelism-pp","title":"3. Pipeline Parallelism (PP)","text":"<p>Pipeline Parallelism splits the model by layer depth across devices.</p>"},{"location":"training_techniques/distributed_training_systems/#core-mechanism_1","title":"Core Mechanism","text":"<ul> <li>Each GPU owns a contiguous block of layers.</li> <li>Activations flow forward through the pipeline.</li> <li>Gradients flow backward in reverse order.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#the-pipeline-bubble","title":"The Pipeline Bubble","text":"<ul> <li>Without micro batching, only one stage is active at a time.</li> <li>GPUs sit idle waiting for inputs or gradients.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#micro-batching","title":"Micro Batching","text":"<ul> <li>The global batch is split into micro batches.</li> <li>Multiple micro batches are in flight simultaneously.</li> <li>Improves utilization at the cost of activation memory.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#insights_1","title":"Insights","text":"<p>Strengths</p> <ul> <li>Reduces per GPU parameter memory.</li> <li>Enables training extremely deep models.</li> </ul> <p>Trade-offs</p> <ul> <li>Increased activation memory footprint.</li> <li>Pipeline schedule complexity.</li> <li>Backward pass latency increases.</li> </ul> <p>Systems Insight</p> <p>Pipeline parallelism improves memory scaling but hurts latency sensitive workloads.</p>"},{"location":"training_techniques/distributed_training_systems/#4-zero-zero-redundancy-optimizer","title":"4. ZeRO (Zero Redundancy Optimizer)","text":"<p>ZeRO is designed to eliminate memory redundancy in Data Parallel training. In standard DP, every GPU stores:</p> <ol> <li>Model parameters</li> <li>Gradients</li> <li>Optimizer states (e.g., Adam\u2019s momentum and variance)</li> </ol> <p>This redundancy quickly becomes a bottleneck for very large models. ZeRO partitions these states across GPUs so that each GPU only stores a fraction of the total memory, enabling training of models that would otherwise not fit.</p>"},{"location":"training_techniques/distributed_training_systems/#zero-stages","title":"ZeRO Stages","text":"Stage Partitioned States Memory Savings Key Idea Stage 1 Optimizer states ~4\u00d7 Each GPU keeps a shard of the optimizer states instead of a full copy. Gradients and parameters are still replicated. Stage 2 Gradients ~8\u00d7 Gradients are also sharded. Each GPU contributes its shard to global All Reduce during backprop. Stage 3 Parameters N\u00d7 (number of GPUs) Even the model parameters are partitioned. Each GPU only holds a subset and fetches other shards when needed for computation. <p>Example: Suppose you have a 10B parameter model across 8 GPUs: - Stage 1: Each GPU holds 1/1 of parameters but 1/8 of optimizer states. - Stage 2: Each GPU holds 1/8 of gradients. - Stage 3: Each GPU holds 1/8 of parameters, gradients, and optimizer states.  </p>"},{"location":"training_techniques/distributed_training_systems/#key-insight","title":"Key Insight","text":"<ul> <li>Stage 1 and 2 mainly reduce memory replication.</li> <li>Stage 3 reduces the largest memory cost: the parameters themselves.</li> <li>At higher stages, computation must fetch shards of parameters or gradients from other GPUs on the fly.</li> <li>Communication becomes the primary bottleneck, requiring overlap with computation for efficiency.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#zero-offload","title":"ZeRO Offload","text":"<p>ZeRO can also offload states to CPU RAM or even NVMe storage to free GPU memory:</p> <ul> <li>Only the portion of optimizer states or parameters actively needed resides on the GPU.</li> <li>Trades GPU memory pressure for PCIe / NVMe bandwidth.</li> <li>Makes training extremely large models possible even with limited GPU memory (e.g., 10B+ parameters on 4\u00d7A100 40GB).</li> </ul> <p>Practical Note: Offloading is especially useful when the model is too big for Stage 3 alone or when using lower-end GPU clusters.</p>"},{"location":"training_techniques/distributed_training_systems/#communication-patterns","title":"Communication Patterns","text":"<ul> <li>Stage 1: Minimal communication; only optimizer state shards are reduced.</li> <li>Stage 2: Requires All Reduce for gradient shards.</li> <li>Stage 3: Each forward/backward pass may require fetching remote parameter shards.</li> <li>Communication-computation overlap is critical to avoid GPU idling.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#summary","title":"Summary","text":"<ul> <li>ZeRO scales memory linearly with the number of GPUs.</li> <li>Stage 3 maximizes memory saving but communication overhead increases.</li> <li>Poor network bandwidth or high latency can dominate runtime.</li> <li>Elastic / hybrid parallelism often combines ZeRO with DP, TP, or PP for large-scale training.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#5-communication-and-memory-trade-offs","title":"5. Communication and Memory Trade-offs","text":"<p>Distributed training is fundamentally constrained by a three way trade off between memory capacity, compute throughput, and communication bandwidth. Improving one dimension often degrades another, and real systems are designed to balance all three.</p>"},{"location":"training_techniques/distributed_training_systems/#51-the-memorycomputecommunication-triangle","title":"5.1 The Memory\u2013Compute\u2013Communication Triangle","text":"<ul> <li>Memory limits how large a model and batch can fit on a single GPU.</li> <li>Compute determines how fast forward and backward passes can be executed.</li> <li>Communication determines how quickly GPUs can synchronize parameters, gradients, or activations.</li> </ul> <p>For large models, memory is usually the first bottleneck. Techniques that reduce memory usage often increase either compute or communication cost.</p>"},{"location":"training_techniques/distributed_training_systems/#52-memory-accounting-with-adam-and-fp16","title":"5.2 Memory Accounting with Adam and FP16","text":"<p>Assume a model with <code>\u03a8</code> parameters trained using Adam and mixed precision.</p>"},{"location":"training_techniques/distributed_training_systems/#parameter-storage","title":"Parameter Storage","text":"<ul> <li>Model weights stored in FP16 or BF16: <code>2\u03a8</code> bytes</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#gradient-storage","title":"Gradient Storage","text":"<ul> <li>Gradients stored in FP16 or BF16 during backpropagation: <code>2\u03a8</code> bytes</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#optimizer-states","title":"Optimizer States","text":"<p>Adam maintains:</p> <ul> <li>FP32 master weights: <code>4\u03a8</code> bytes  </li> <li>First moment (momentum): <code>4\u03a8</code> bytes  </li> <li>Second moment (variance): <code>4\u03a8</code> bytes  </li> </ul> <p>Total optimizer memory: <code>12\u03a8</code> bytes</p>"},{"location":"training_techniques/distributed_training_systems/#total-training-memory","title":"Total Training Memory","text":"<p>Adding all components:</p> <ul> <li>Parameters: <code>2\u03a8</code></li> <li>Gradients: <code>2\u03a8</code></li> <li>Optimizer states: <code>12\u03a8</code></li> </ul> <p>Total: approximately <code>16\u03a8</code> bytes per parameter</p>"},{"location":"training_techniques/distributed_training_systems/#concrete-example","title":"Concrete Example","text":"<p>For a 7B parameter model:</p> <ul> <li>7B \u00d7 16 bytes \u2248 112 GB</li> </ul> <p>This number excludes:</p> <ul> <li>Activation memory</li> <li>Temporary buffers</li> <li>Communication workspaces</li> </ul> <p>This explains why even a single training replica of a 7B model cannot fit on an 80 GB GPU without memory optimization techniques.</p>"},{"location":"training_techniques/distributed_training_systems/#53-why-communication-becomes-the-bottleneck","title":"5.3 Why Communication Becomes the Bottleneck","text":"<p>As memory saving techniques reduce per GPU state:</p> <ul> <li>More parameter shards must be fetched remotely.</li> <li>Gradients must be synchronized more frequently.</li> <li>Communication moves from being infrequent and bulk to frequent and latency sensitive.</li> </ul> <p>High bandwidth interconnects and overlap with compute become critical.</p>"},{"location":"training_techniques/distributed_training_systems/#54-memory-reduction-techniques-and-their-trade-offs","title":"5.4 Memory Reduction Techniques and Their Trade-offs","text":""},{"location":"training_techniques/distributed_training_systems/#activation-checkpointing","title":"Activation Checkpointing","text":"<ul> <li>Saves memory by discarding activations during forward pass.</li> <li>Recomputes activations during backward pass.</li> <li>Trades memory for additional compute, typically 20 to 40 percent overhead.</li> </ul> <p>When to use: </p> <p>Memory constrained training where compute is not the bottleneck.</p>"},{"location":"training_techniques/distributed_training_systems/#mixed-precision-training","title":"Mixed Precision Training","text":"<ul> <li>Uses FP16 or BF16 for forward and backward computation.</li> <li>Keeps optimizer states in FP32 for numerical stability.</li> <li>Reduces memory usage and communication bandwidth.</li> </ul> <p>Key benefit: </p> <p>Improves both memory efficiency and throughput with minimal accuracy loss.</p>"},{"location":"training_techniques/distributed_training_systems/#parameter-sharding","title":"Parameter Sharding","text":"<ul> <li>Splits parameters, gradients, or optimizer states across GPUs.</li> <li>Removes redundancy present in Data Parallelism.</li> <li>Increases communication during forward and backward passes.</li> </ul> <p>Typical examples: </p> <p>ZeRO Stage 2 and Stage 3.</p> <p>Takeaway: Large scale training is constrained by a memory\u2013compute\u2013communication trade off. Techniques like mixed precision, activation checkpointing, and parameter sharding reduce memory pressure but introduce additional compute or communication costs. Efficient systems overlap communication with computation to avoid performance collapse.</p>"},{"location":"training_techniques/distributed_training_systems/#6-checkpointing-and-fault-tolerance","title":"6. Checkpointing and Fault Tolerance","text":"<p>At large cluster scale, hardware and network failures are expected. Training systems must be designed to recover quickly with minimal loss of progress.</p>"},{"location":"training_techniques/distributed_training_systems/#61-checkpointing-strategies","title":"6.1 Checkpointing Strategies","text":"<p>Full Checkpoints</p> <ul> <li>Store model parameters, optimizer states, and RNG state.</li> <li>Enable exact training resumption.</li> <li>Expensive in terms of storage size and write time.</li> </ul>  RNG State (Random Number Generator State)   1. Why RNG State Matters  Randomness is used in multiple parts of training:  - Weight initialization - Dropout masks - Data shuffling - Data augmentation - Stochastic layers or kernels  If training is resumed **without restoring RNG state**:  - Dropout patterns change - Data order may differ - Gradient noise changes  As a result, training can diverge from the original run, making debugging and reproducibility difficult.  2. What Is Typically Included  A full checkpoint usually stores RNG states for:  - Python `random` - NumPy RNG - PyTorch CPU RNG - PyTorch CUDA RNG (per GPU)  In distributed training, each worker maintains its own RNG state, which must be saved and restored independently.   <p>Sharded Checkpoints</p> <ul> <li>Each worker writes only its shard of parameters or optimizer state.</li> <li>Reduces checkpoint time and I/O contention.</li> <li>Requires coordinated restore logic.</li> </ul> <p>Asynchronous Checkpointing</p> <ul> <li>Checkpointing runs in the background while training continues.</li> <li>Avoids blocking GPUs on slow storage.</li> <li>Slightly increases complexity and memory usage.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#62-fault-tolerance","title":"6.2 Fault Tolerance","text":"<p>Elastic Training</p> <ul> <li>Allows workers to join or leave during training.</li> <li>Automatically rebalances data and workloads.</li> <li>Commonly implemented using PyTorch Elastic or TorchRun.</li> </ul> <p>Health Checks</p> <ul> <li>Detect hung, slow, or non communicating workers.</li> <li>Prevents a single faulty GPU from stalling the entire job.</li> </ul> <p>Straggler Detection</p> <ul> <li>Identifies workers that are consistently slower.</li> <li>Helps avoid synchronization delays in collective operations.</li> </ul> <p>Takeaway: Checkpointing is fundamentally an I/O and systems problem. Efficient training requires minimizing checkpoint overhead while ensuring fast and correct recovery from failures.</p>"},{"location":"training_techniques/distributed_training_systems/#7-advanced-parallelism-and-optimization-topics","title":"7. Advanced Parallelism and Optimization Topics","text":"<p>These topics often differentiate strong systems candidates.</p>"},{"location":"training_techniques/distributed_training_systems/#71-flashattention","title":"7.1 FlashAttention","text":"<ul> <li>Computes attention in tiles to avoid materializing full attention matrices.</li> <li>Reduces memory from quadratic to linear in sequence length.</li> <li>Improves both speed and memory usage.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#72-mixture-of-experts-moe","title":"7.2 Mixture of Experts (MoE)","text":"<ul> <li>Sparse activation where only a subset of parameters are used per token.</li> <li>Requires expert parallelism and routing strategies.</li> <li>Trades compute efficiency for model capacity.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#73-sequence-parallelism","title":"7.3 Sequence Parallelism","text":"<p>Sequence parallelism splits the sequence length dimension of the input across multiple devices, rather than splitting model parameters or batch elements.</p>"},{"location":"training_techniques/distributed_training_systems/#core-idea","title":"Core Idea","text":"<p>For an input tensor with shape:</p> <p><code>[batch, sequence_length, hidden_dim]</code></p> <ul> <li>The sequence length dimension is partitioned across GPUs.</li> <li>Each GPU processes a contiguous chunk of tokens from the sequence.</li> <li>This reduces per GPU activation memory, which scales linearly with sequence length.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#why-it-is-useful","title":"Why It Is Useful","text":"<ul> <li>Attention and activation memory grow with sequence length.</li> <li>Very long context models (e.g., 32k, 64k, or 128k tokens) quickly exceed GPU memory limits.</li> <li>Sequence parallelism allows long sequences to fit by distributing token level computation.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#communication-pattern","title":"Communication Pattern","text":"<ul> <li>Certain operations, such as attention and layer normalization, require communication across sequence shards.</li> <li>Communication typically uses All Gather or All Reduce.</li> <li>Efficient overlap with computation is necessary to avoid performance loss.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#relationship-with-tensor-parallelism","title":"Relationship with Tensor Parallelism","text":"<ul> <li>Sequence parallelism is often combined with tensor parallelism.</li> <li>Tensor parallelism splits hidden dimensions, while sequence parallelism splits tokens.</li> <li>This combination balances memory usage and communication overhead.</li> </ul> <p>Takeaway: Sequence parallelism distributes tokens across GPUs to reduce activation memory for long context models, trading additional communication for the ability to train with very large sequence lengths.</p>"},{"location":"training_techniques/distributed_training_systems/#74-low-precision-training","title":"7.4 Low Precision Training","text":"<ul> <li>FP8 reduces bandwidth and memory.</li> <li>Requires careful scaling and error management.</li> <li>Hardware dependent and increasingly common on newer accelerators.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#8-additional-common-interview-topics","title":"8. Additional Common Interview Topics","text":"<p>These are frequently asked in senior level ML systems interviews.</p>"},{"location":"training_techniques/distributed_training_systems/#81-hybrid-parallelism","title":"8.1 Hybrid Parallelism","text":"<ul> <li>Real systems combine DP, TP, PP, and ZeRO.</li> <li>Interviewers often ask how to scale from 1 GPU to hundreds or thousands.</li> <li>A strong answer mentions hierarchical parallelism.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#82-throughput-vs-latency","title":"8.2 Throughput vs Latency","text":"<p>Throughput and latency represent two different optimization goals, and distributed systems make very different design choices depending on which one is prioritized.</p>"},{"location":"training_techniques/distributed_training_systems/#training-throughput-oriented","title":"Training: Throughput Oriented","text":"<ul> <li>The primary goal in training is maximum tokens processed per second.</li> <li>Large batch sizes are used to fully utilize GPUs.</li> <li>Parallelism strategies favor efficiency even if individual requests are slow.</li> </ul> <p>Common choices in training</p> <ul> <li>Data parallelism with large global batches.</li> <li>Pipeline parallelism with micro batching.</li> <li>Aggressive overlap of communication and computation.</li> <li>Higher tolerance for end to end latency per batch.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#inference-latency-oriented","title":"Inference: Latency Oriented","text":"<ul> <li>The primary goal in inference is fast response time per request.</li> <li>Batch sizes are often small or dynamic.</li> <li>Minimizing synchronization and communication is critical.</li> </ul> <p>Common choices in inference</p> <ul> <li>Replication rather than sharding for small models.</li> <li>Limited or no pipeline parallelism due to bubble overhead.</li> <li>Kernel fusion and caching over global synchronization.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#why-parallelism-strategies-differ","title":"Why Parallelism Strategies Differ","text":"<ul> <li>Training can amortize communication over large batches.</li> <li>Inference cannot hide communication latency as easily.</li> <li>Techniques like tensor or pipeline parallelism may improve throughput but often increase per request latency.</li> </ul> <p>Takeaway: Training systems optimize for throughput using large batches and heavy parallelism, while inference systems optimize for low latency by minimizing synchronization and communication, leading to fundamentally different parallelism strategies.</p>"},{"location":"training_techniques/distributed_training_systems/#83-gradient-accumulation","title":"8.3 Gradient Accumulation","text":"<p>Gradient accumulation is a technique used to simulate a larger batch size by accumulating gradients over multiple forward and backward passes before performing an optimizer update.</p>"},{"location":"training_techniques/distributed_training_systems/#core-idea_1","title":"Core Idea","text":"<ul> <li>Instead of updating model parameters after every mini batch, gradients are accumulated over <code>K</code> steps.</li> <li>The optimizer step is executed only once after all <code>K</code> gradients are accumulated.</li> <li>This creates an effective batch size of <code>K \u00d7 mini_batch_size</code>.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#why-it-saves-memory","title":"Why It Saves Memory","text":"<ul> <li>Each step processes a small mini batch that fits in GPU memory.</li> <li>Activations are freed after each backward pass.</li> <li>Only gradients are accumulated, avoiding the need to store a large batch at once.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#communication-benefits-in-distributed-training","title":"Communication Benefits in Distributed Training","text":"<ul> <li>In Data Parallel training, gradient synchronization normally happens every step.</li> <li>With gradient accumulation, synchronization happens only once every <code>K</code> steps.</li> <li>This reduces communication frequency and improves scalability.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#common-confusion-with-data-parallelism","title":"Common Confusion with Data Parallelism","text":"<ul> <li>Data parallelism distributes different data samples across GPUs.</li> <li>Gradient accumulation repeats multiple steps on the same GPU before synchronization.</li> <li>The two techniques are complementary and often used together.</li> </ul>"},{"location":"training_techniques/distributed_training_systems/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Learning rate schedules often need adjustment for large effective batch sizes.</li> <li>Loss values are usually scaled to avoid gradient magnitude changes.</li> <li>Very large accumulation steps can slow convergence.</li> </ul> <p>Takeaway: Gradient accumulation increases effective batch size by accumulating gradients across multiple steps, reducing memory usage and communication frequency, and is often combined with data parallelism to scale training efficiently.</p>"},{"location":"training_techniques/foundation/","title":"Foundation","text":""},{"location":"training_techniques/foundation/#1-what-is-an-llm-optimizing","title":"1 What is an LLM Optimizing?","text":"<p>At its core, a Large Language Model (LLM) is a probabilistic system designed to model the distribution of natural language. Despite emergent reasoning and planning behaviors, the training objective itself is simple: reduce uncertainty about the next token given prior context.</p>"},{"location":"training_techniques/foundation/#11-the-autoregressive-objective","title":"1.1 The Autoregressive Objective","text":"<p>Most LLMs are trained using an Autoregressive (AR) or Causal Language Modeling (CLM) objective. The joint probability of a token sequence is factorized as a product of conditional probabilities:</p> \\[ P(x_1, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{&lt;t}) \\] <p>This formulation assumes:</p> <ul> <li>Tokens are conditionally independent given their history</li> <li>Knowledge is captured implicitly through long context dependencies</li> <li>Tokenization defines the atomic units of prediction</li> </ul>"},{"location":"training_techniques/foundation/#12-the-loss-function-negative-log-likelihood","title":"1.2 The Loss Function: Negative Log-Likelihood","text":"<p>Training minimizes the Negative Log-Likelihood (NLL) of the observed data, which is equivalent to Cross-Entropy Loss:</p> \\[ \\mathcal{L}(\\theta) = - \\mathbb{E}_{(x_1,\\dots,x_T) \\sim D} \\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{&lt;t}) \\] <p>Where: - \\(\\theta\\) are the model parameters - \\(x_{&lt;t}\\) is the context window - \\(P_\\theta\\) is the predicted probability distribution produced by a Softmax layer</p>"},{"location":"training_techniques/foundation/#13-cross-entropy-kl-divergence-and-learning","title":"1.3 Cross-Entropy, KL Divergence, and Learning","text":"<p>Minimizing cross-entropy implicitly minimizes the KL divergence between the true data distribution \\(P_{data}\\) and the model distribution \\(P_\\theta\\):</p> \\[ H(P_{data}, P_\\theta) = H(P_{data}) + D_{KL}(P_{data} \\| P_\\theta) \\] <p>Since \\(H(P_{data})\\) is fixed, training pushes the model distribution closer to the real language distribution. This connects LLM training directly to information theory and compression.</p>"},{"location":"training_techniques/foundation/#14-why-next-token-prediction-works-the-compression-hypothesis","title":"1.4 Why Next-Token Prediction Works (The Compression Hypothesis)","text":"<p>Predicting the next token forces the model to internalize structure across many domains.</p> <ul> <li>Compression implies abstraction: To compress diverse text, the model must learn syntax, semantics, facts, and procedures.</li> <li>Softmax competition: Increasing probability mass for the correct token necessarily decreases mass for alternatives, encouraging fine-grained representations.</li> <li>Generalization pressure: Predicting well across domains requires reusable internal features, which later appear as reasoning, translation, or coding skills.</li> </ul>"},{"location":"training_techniques/foundation/#2-language-modeling-and-likelihood-minimization","title":"2 Language Modeling and Likelihood Minimization","text":""},{"location":"training_techniques/foundation/#21-maximum-likelihood-estimation-mle","title":"2.1 Maximum Likelihood Estimation (MLE)","text":"<p>LLM training is an instance of Maximum Likelihood Estimation, where we seek parameters \\(\\theta^*\\) that maximize the likelihood of observed text:</p> \\[ \\theta^* = \\arg\\max_\\theta \\mathbb{E}_{x \\sim D} [\\log P_\\theta(x)] \\] <p>MLE provides a stable and scalable objective but does not encode preferences such as helpfulness, safety, or instruction following.</p>"},{"location":"training_techniques/foundation/#22-teacher-forcing","title":"2.2 Teacher Forcing","text":"<p>Teacher forcing is a training strategy where the model is conditioned on the ground-truth previous token rather than its own prediction when computing the next-token loss. For a sequence \\(x_1, \\dots, x_T\\), the model always receives \\(x_{t-1}\\) as input when predicting \\(x_t\\), even if it would have predicted a different token at step \\(t-1\\).</p> <p>Small Practical Example</p> <p>Task: Predict the sentence  </p> <p>\u201cThe cat sat on the mat\u201d</p>"},{"location":"training_techniques/foundation/#step-1-training-data","title":"Step 1: Training data","text":"<p>Tokens: <code>[The, cat, sat, on, the, mat]</code></p>"},{"location":"training_techniques/foundation/#step-2-training-with-teacher-forcing","title":"Step 2: Training with teacher forcing","text":"<p>At each step, the model is conditioned on the ground-truth previous token.</p> <ol> <li> <p>Input: <code>The</code> Target: <code>cat</code></p> </li> <li> <p>Input: <code>The cat</code> Target: <code>sat</code></p> </li> <li> <p>Input: <code>The cat sat</code> Target: <code>on</code></p> </li> <li> <p>Input: <code>The cat sat on</code> Target: <code>the</code></p> </li> <li> <p>Input: <code>The cat sat on the</code> Target: <code>mat</code></p> </li> </ol> <p>Even if the model predicts an incorrect token at any step, the next input still uses the true token from the dataset.</p>"},{"location":"training_techniques/foundation/#step-3-inference-time-behavior","title":"Step 3: Inference time behavior","text":"<p>At inference, the model must condition on its own predictions.</p> <ol> <li> <p>Input: <code>The</code> Prediction: <code>dog</code></p> </li> <li> <p>Next input: <code>The dog</code>    Errors now propagate forward</p> </li> </ol> <p>This train\u2013test mismatch is known as exposure bias and is a key limitation of teacher forcing.</p> <p>Advantages:</p> <ul> <li>Full parallelization of loss computation across all positions in a Transformer</li> <li>Stable gradients, since errors do not compound during training</li> </ul> <p>Limitation: Exposure Bias</p> <ul> <li>Introduces exposure bias: during inference, the model must condition on its own past predictions, a distribution shift that can lead to error accumulation. This gap motivates post-training techniques such as supervised fine-tuning and reinforcement learning based alignment.</li> </ul>"},{"location":"training_techniques/foundation/#23-perplexity-as-an-evaluation-metric","title":"2.3 Perplexity as an Evaluation Metric","text":"<p>Perplexity measures how uncertain a language model is, on average, when predicting the next token in a sequence.</p> <p>Formally, if a model is trained using cross-entropy loss \\(\\mathcal{L}\\), then perplexity is defined as:</p> \\[ \\text{PPL} = \\exp(\\mathcal{L}) \\] <p>where \\(\\mathcal{L}\\) is the average negative log-probability assigned to the correct next token.</p>"},{"location":"training_techniques/foundation/#intuition","title":"Intuition","text":"<p>A language model repeatedly answers the question:</p> <p>How many plausible choices do I believe the next token could be?</p> <p>Perplexity converts log probabilities back into an interpretable scale representing the effective number of choices.</p> <ul> <li> <p>PPL = 1   The model is fully confident and assigns probability 1 to the correct token.</p> </li> <li> <p>PPL = 10   The model behaves as if it is choosing uniformly among about 10 tokens.</p> </li> <li> <p>PPL = 100   The model is highly uncertain, with roughly 100 plausible next tokens.</p> </li> </ul> <p>This is why perplexity is often described as the model\u2019s average branching factor.</p>"},{"location":"training_techniques/foundation/#why-lower-perplexity-is-better","title":"Why Lower Perplexity Is Better","text":"<ul> <li>Lower PPL means the model assigns higher probability to the correct next token.</li> <li>This corresponds to lower uncertainty and better next-token prediction.</li> <li>Minimizing cross-entropy during training directly minimizes perplexity.</li> </ul>"},{"location":"training_techniques/foundation/#important-clarification","title":"Important Clarification","text":"<p>Perplexity is not a direct measure of: - Reasoning ability - Factual correctness - Alignment or instruction following</p> <p>It only measures how well the model predicts the data distribution. While lower perplexity often correlates with better text quality, it does not guarantee better downstream performance.</p>"},{"location":"training_techniques/foundation/#3-why-scaling-works-and-where-it-breaks","title":"3. Why Scaling Works and Where it Breaks","text":""},{"location":"training_techniques/foundation/#31-empirical-scaling-laws","title":"3.1 Empirical Scaling Laws","text":"<p>Performance improves predictably as a power-law function of:</p> <ul> <li>Model parameters</li> <li>Training tokens</li> <li>Compute budget</li> </ul> <p>This empirical behavior explains the rapid gains from larger models.</p>"},{"location":"training_techniques/foundation/#kaplan-scaling-laws-2020","title":"Kaplan Scaling Laws (2020)","text":"<p>Early results suggested scaling model size was the dominant factor.</p> <ul> <li>Loss scales roughly as a power-law in parameter count</li> <li>Data was treated as effectively unlimited</li> </ul>"},{"location":"training_techniques/foundation/#chinchilla-scaling-laws-2022","title":"Chinchilla Scaling Laws (2022)","text":"<p>Later work showed most large models were undertrained.</p> <p>Key findings:</p> <ul> <li>Optimal performance requires balancing parameters and tokens</li> <li>Roughly 20 training tokens per parameter is compute optimal</li> <li>Smaller models trained on more data can outperform larger undertrained models</li> </ul> <p>This led to data-centric model design such as LLaMA and Mistral.</p>"},{"location":"training_techniques/foundation/#32-where-scaling-breaks","title":"3.2 Where Scaling Breaks","text":""},{"location":"training_techniques/foundation/#1-data-scarcity-and-synthetic-feedback-loops","title":"1. Data Scarcity and Synthetic Feedback Loops","text":"<ul> <li>High-quality human text is limited</li> <li>Synthetic data risks reducing diversity</li> <li>Repeated self-training can lead to model collapse</li> </ul>"},{"location":"training_techniques/foundation/#2-capability-saturation","title":"2. Capability Saturation","text":"<ul> <li>Loss improves smoothly, but abilities emerge discontinuously</li> <li>Reasoning, planning, and tool use do not scale linearly with perplexity</li> <li>Small loss gains can hide large behavioral differences</li> </ul>"},{"location":"training_techniques/foundation/#3-inference-cost-and-latency","title":"3. Inference Cost and Latency","text":"<ul> <li>Larger models increase memory, latency, and cost</li> <li>This motivates inference-efficient designs</li> </ul>"},{"location":"training_techniques/foundation/#4-test-time-scaling","title":"4. Test-Time Scaling","text":"<ul> <li>Recent systems scale inference compute rather than parameters</li> <li>Models generate longer internal reasoning traces</li> <li>This shifts scaling from training time to inference time</li> </ul> <p>Examples include OpenAI o1 and DeepSeek-R1.</p>"},{"location":"training_techniques/training_optimization_and_stability/","title":"Training Optimization and Stability","text":"<p>Training large neural networks, especially Transformers, requires careful choices of optimizers, learning rate schedules, and numerical techniques to ensure fast convergence, stable training, and good generalization. This section covers the most important concepts commonly discussed in interviews and used in practice.</p>"},{"location":"training_techniques/training_optimization_and_stability/#1-optimizers-and-schedules","title":"1. Optimizers and Schedules","text":""},{"location":"training_techniques/training_optimization_and_stability/#11-adam-vs-adamw","title":"1.1 Adam vs AdamW","text":""},{"location":"training_techniques/training_optimization_and_stability/#adam-optimizer","title":"Adam Optimizer","text":"<p>Adam (Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning.</p> <p>Key ideas:</p> <ul> <li>Maintains an exponential moving average of gradients (first moment)</li> <li>Maintains an exponential moving average of squared gradients (second moment)</li> <li>Uses bias correction for both moments</li> <li>Adapts the learning rate per parameter</li> </ul> <p>Update rule (simplified):</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] <p>Where:</p> <ul> <li>\\(\\theta_t\\): Model parameters at training step \\(t\\).</li> <li>\\(\\theta_{t+1}\\): Updated model parameters after applying one optimization step.</li> <li>\\(\\eta\\) (Learning Rate): Global step size that controls how large the parameter update is.</li> <li>\\(\\hat{m}_t\\) (Bias Corrected First Moment): Exponentially decayed moving average of past gradients, corrected for initialization bias.   Represents the estimated mean of the gradients.   $$   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}   $$</li> <li>\\(\\hat{v}_t\\) (Bias Corrected Second Moment): Exponentially decayed moving average of squared gradients, corrected for initialization bias.   Represents the estimated variance of the gradients.   $$   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}   $$</li> <li>\\(\\beta_1\\): Exponential decay rate for the first moment estimate.   Typical value: 0.9</li> <li>\\(\\beta_2\\): Exponential decay rate for the second moment estimate.   Typical value: 0.999</li> <li>\\(\\epsilon\\): Small constant added for numerical stability to prevent division by zero.   Typical value: \\(10^{-8}\\)</li> </ul> <p>Advantages:</p> <ul> <li>Fast convergence</li> <li>Works well with sparse gradients</li> <li>Requires minimal tuning compared to SGD</li> </ul> <p>Limitations:</p> <ul> <li>Implicitly couples L2 regularization with adaptive learning rates</li> <li>Often leads to worse generalization compared to SGD or AdamW in large models</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#adamw-optimizer","title":"AdamW Optimizer","text":"<p>AdamW decouples weight decay from the gradient-based update.</p> <p>Key difference from Adam:</p> <ul> <li>Weight decay is applied directly to parameters, not via the gradient</li> </ul> <p>Update rule (conceptually):</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\lambda \\theta_t \\] <p>where:</p> <ul> <li>\\(\\lambda\\) (Weight Decay Coefficient): Controls the strength of weight decay regularization.   Larger values enforce stronger penalization of large parameter magnitudes.</li> <li>\\(- \\eta \\lambda \\theta_t\\) (Decoupled Weight Decay Term): Applies weight decay directly to the parameters, independent of the gradient-based update.   This ensures consistent regularization regardless of adaptive learning rates.</li> </ul> <p>Why AdamW matters:</p> <ul> <li>Correctly implements weight decay as regularization</li> <li>Prevents adaptive learning rates from weakening regularization</li> <li>Standard optimizer for modern Transformers (BERT, GPT, ViT)</li> </ul> <p>Note: AdamW is almost always preferred over Adam for large-scale Transformer training.</p>"},{"location":"training_techniques/training_optimization_and_stability/#12-learning-rate-warmup-and-decay","title":"1.2 Learning Rate Warmup and Decay","text":""},{"location":"training_techniques/training_optimization_and_stability/#learning-rate-warmup","title":"Learning Rate Warmup","text":"<p>What it is:</p> <ul> <li>Gradually increases the learning rate from a small value to the target value over the first few training steps</li> </ul> <p>Why it is needed:</p> <ul> <li>Large models have unstable gradients at initialization</li> <li>Prevents divergence caused by large updates early in training</li> <li>Especially critical for Transformers and mixed precision training</li> </ul> <p>Common strategies:</p> <ul> <li>Linear warmup</li> <li>Warmup for a fixed number of steps (e.g., 1k to 10k steps)</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#learning-rate-decay","title":"Learning Rate Decay","text":"<p>After warmup, the learning rate is gradually reduced to improve convergence.</p> <p>Common decay schedules:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Step decay</li> <li>Polynomial decay</li> </ul> <p>Cosine decay (widely used):</p> <ul> <li>Smoothly reduces learning rate</li> <li>Avoids sudden drops that can destabilize training</li> </ul> <p>Insight: Warmup handles early instability, decay improves late-stage convergence and generalization.</p>"},{"location":"training_techniques/training_optimization_and_stability/#13-weight-decay-and-regularization","title":"1.3 Weight Decay and Regularization","text":""},{"location":"training_techniques/training_optimization_and_stability/#weight-decay","title":"Weight Decay","text":"<ul> <li>Penalizes large weights to prevent overfitting</li> </ul> <p>Important distinction:</p> <ul> <li>L2 regularization modifies the loss</li> <li>Weight decay directly modifies the parameter update</li> </ul> <p>Why decoupling matters:</p> <ul> <li>With adaptive optimizers, L2 regularization is not equivalent to weight decay</li> <li>AdamW fixes this mismatch</li> </ul> Explanation: L2 regularization is not equivalent to weight decay  1. L2 Regularization: It adds a penalty term to the loss function:  $$ \\mathcal{L}' = \\mathcal{L} + \\frac{\\lambda}{2} \\|\\theta\\|^2 $$  Taking the gradient:  $$ \\nabla_\\theta \\mathcal{L}' = \\nabla_\\theta \\mathcal{L} + \\lambda \\theta $$  So the optimizer update becomes:  $$ \\theta_{t+1} = \\theta_t - \\eta \\left( \\nabla_\\theta \\mathcal{L} + \\lambda \\theta_t \\right) $$  This means regularization is applied through the gradient.   2. Why This Works for SGD   For SGD, the update is:  $$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L} - \\eta \\lambda \\theta_t $$  Here, L2 regularization is mathematically equivalent to weight decay because:  - All parameters use the same learning rate  - No per parameter scaling is applied   So both methods shrink weights uniformly.   3. What Breaks with Adam   Adam modifies the update using adaptive, per parameter learning rates:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  When L2 regularization is added, the penalty term $\\lambda \\theta_t$ is also scaled by the adaptive denominator:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t + \\lambda \\theta_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  Consequence   - Parameters with large gradient variance receive less regularization - Parameters with small gradient variance receive more regularization - Regularization strength becomes parameter dependent and inconsistent  This is not true weight decay.  4. What Weight Decay Actually Means  True weight decay directly shrinks parameters independently of gradients:  $$ \\theta_{t+1} = (1 - \\eta \\lambda)\\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$  Key properties:  - Uniform shrinkage across parameters - Independent of gradient statistics - Matches the intended regularization behavior  5. How AdamW Fixes the Problem   AdamW explicitly decouples weight decay from the gradient update:  $$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\lambda \\theta_t $$   6. Why this works   - Weight decay is applied directly to parameters - Adaptive learning rates affect only the gradient update - Regularization strength remains consistent"},{"location":"training_techniques/training_optimization_and_stability/#intuition-summary","title":"Intuition Summary","text":"Method How Regularization Is Applied Problem SGD + L2 Uniform parameter shrinkage None Adam + L2 Scaled by adaptive learning rates Inconsistent regularization AdamW Direct parameter decay Correct behavior"},{"location":"training_techniques/training_optimization_and_stability/#other-regularization-techniques","title":"Other Regularization Techniques","text":"<ul> <li>Dropout</li> <li>Label smoothing</li> <li>Data augmentation</li> <li>Early stopping</li> </ul> <p>Transformer-specific note:</p> <ul> <li>Biases and LayerNorm parameters often exclude weight decay</li> <li>This is a common best practice in large models</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#2-numerical-stability","title":"2 Numerical Stability","text":""},{"location":"training_techniques/training_optimization_and_stability/#21-fp16-vs-bf16","title":"2.1 FP16 vs BF16","text":""},{"location":"training_techniques/training_optimization_and_stability/#fp16-half-precision","title":"FP16 (Half Precision)","text":"<p>Characteristics:</p> <ul> <li>16-bit floating point</li> <li>Limited exponent range</li> <li>Higher risk of overflow and underflow</li> </ul> <p>Challenges:</p> <ul> <li>Gradient underflow for small values</li> <li>Requires loss scaling for stability</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#bf16-brain-floating-point","title":"BF16 (Brain Floating Point)","text":"<p>Characteristics:</p> <ul> <li>16-bit floating point with larger exponent range</li> <li>Same exponent range as FP32</li> <li>Lower mantissa precision than FP16</li> </ul> <p>Advantages:</p> <ul> <li>Much more numerically stable than FP16</li> <li>Usually does not require loss scaling</li> <li>Widely supported on TPUs and newer GPUs</li> </ul> <p>Insight: BF16 is preferred when hardware supports it due to better stability with minimal complexity.</p>"},{"location":"training_techniques/training_optimization_and_stability/#22-mixed-precision-training","title":"2.2 Mixed Precision Training","text":"<p>This technique uses both FP16/BF16 and FP32 to get the \"best of both worlds\": the speed of 16-bit and the accuracy of 32-bit.</p> <ul> <li>Forward Pass: Done in FP16 for speed.</li> <li>Loss Scaling: Since FP16 has a narrow range, gradients can \"underflow\" (become zero). We multiply the loss by a large scale factor to push gradients into a representable range.</li> <li>Master Weights: A copy of the weights is kept in FP32. The gradients are converted to FP32 to update these master weights, ensuring precision isn't lost over time.</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#benefits","title":"Benefits","text":"<ul> <li>Faster training</li> <li>Lower memory usage</li> <li>Enables larger batch sizes and models</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#loss-scaling","title":"Loss Scaling","text":"<p>Why it is needed (mainly for FP16):</p> <ul> <li>Prevents gradients from underflowing to zero</li> </ul> <p>How it works:</p> <ul> <li>Multiply loss by a scale factor before backprop</li> <li>Divide gradients by the same factor before optimizer step</li> </ul> <p>Dynamic loss scaling:</p> <ul> <li>Automatically adjusts scale based on overflow detection</li> <li>Common in frameworks like PyTorch AMP</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#23-gradient-clipping","title":"2.3 Gradient Clipping","text":""},{"location":"training_techniques/training_optimization_and_stability/#what-is-gradient-clipping","title":"What is Gradient Clipping?","text":"<p>To prevent \"Exploding Gradients\" (where a large update ruins the model weights), we cap the gradients.</p> <ul> <li>Value Clipping: Caps each element of the gradient at a min/max.</li> <li>Norm Clipping: Scales the entire gradient vector so its \\(L_2\\) norm does not exceed a threshold. This preserves the direction of the gradient while limiting the magnitude.</li> </ul> <p>Norm Clipping is more common.</p> \\[ g \\leftarrow g \\cdot \\min\\left(1, \\frac{\\tau}{\\|g\\|}\\right) \\] <p>where \\(\\tau\\) is the clipping threshold.</p>"},{"location":"training_techniques/training_optimization_and_stability/#why-it-matters","title":"Why it matters","text":"<ul> <li>Prevents exploding gradients</li> <li>Stabilizes training in deep or recurrent models</li> <li>Especially important for large learning rates or noisy gradients</li> </ul> <p>Typical values:</p> <ul> <li>Global norm between 0.5 and 1.0 for Transformers</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#24-loss-spikes-and-divergence-diagnosis","title":"2.4 Loss Spikes and Divergence Diagnosis","text":""},{"location":"training_techniques/training_optimization_and_stability/#common-causes-of-loss-spikes","title":"Common Causes of Loss Spikes","text":"<ul> <li>Learning rate too high</li> <li>Insufficient warmup</li> <li>Numerical overflow in FP16</li> <li>Poor initialization</li> <li>Data outliers or corrupted batches</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#diagnosis-checklist","title":"Diagnosis Checklist","text":"<ul> <li>Check learning rate schedule and warmup length</li> <li>Monitor gradient norms</li> <li>Enable gradient clipping</li> <li>Inspect loss scaling behavior</li> <li>Compare FP16 vs BF16 runs</li> <li>Verify data preprocessing and labels</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#practical-debugging-tips","title":"Practical Debugging Tips","text":"<ul> <li>Reduce learning rate and re-run</li> <li>Increase warmup steps</li> <li>Switch from FP16 to BF16 if possible</li> <li>Enable anomaly detection for NaNs and Infs</li> <li>Log per-layer gradient norms</li> </ul>"},{"location":"training_techniques/training_optimization_and_stability/#summary","title":"Summary","text":"<ul> <li>AdamW is the default optimizer for modern large models</li> <li>Learning rate warmup is critical for early stability</li> <li>Weight decay must be decoupled from adaptive updates</li> <li>BF16 offers better numerical stability than FP16</li> <li>Mixed precision improves efficiency but requires care</li> <li>Gradient clipping and monitoring are essential debugging tools</li> </ul> <p>These concepts form the backbone of stable and efficient training for large-scale neural networks and are frequently tested in machine learning interviews.</p>"},{"location":"training_techniques/pre_training/RoPE/","title":"RoPe","text":"<p>Rotary Positional Embedding (RoPE) is the state-of-the-art method for encoding positional information in Transformers. It is the default choice for modern LLMs, including Llama 2/3, Mistral, and PaLM.</p>"},{"location":"training_techniques/pre_training/RoPE/#1-the-core-concept","title":"1. The Core Concept","text":"<p>Traditional embeddings like Sinusoidal or Learned Absolute Embeddings are additive:</p> \\[ x_i = e_i + p_i \\] <p>RoPE is multiplicative. It treats the embedding vector as a set of complex numbers and rotates them in a high-dimensional space based on their position.</p>"},{"location":"training_techniques/pre_training/RoPE/#why-rotate","title":"Why rotate?","text":"<p>The primary goal of RoPE is to ensure that the dot product between two tokens (Query and Key) depends only on their relative distance, not their absolute positions.</p> \\[ \\langle f_q(x_m, m), f_k(x_n, n) \\rangle = g(x_m, x_n, m - n) \\] <p>Where:</p> <ul> <li> <p>\\(x_m, x_n\\) : The original content embeddings of the tokens at positions \\(m\\) and \\(n\\). These vectors come from the token embedding layer or the previous Transformer block and do not yet include positional information.</p> </li> <li> <p>\\(m, n\\): The absolute token positions in the sequence. For example, \\(m = 5\\) and \\(n = 12\\).</p> </li> <li> <p>\\(f_q(x_m, m)\\): The position-aware Query representation at position \\(m\\).   This function applies the RoPE rotation to the query vector derived from \\(x_m\\).</p> </li> <li> <p>\\(f_k(x_n, n)\\): The position-aware Key representation at position \\(n\\).   This function applies the same RoPE rotation scheme to the key vector derived from \\(x_n\\).</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The dot product used in scaled dot-product attention to compute attention scores.</p> </li> <li> <p>\\(g(x_m, x_n, m - n)\\): A function whose output depends on the token contents and only the relative position \\((m - n)\\), not on the absolute positions themselves.</p> </li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#why-this-matters","title":"Why This Matters","text":"<p>This equation states that after applying RoPE:</p> <ul> <li>Absolute positions \\(m\\) and \\(n\\) do not independently affect the attention score.</li> <li>Only the relative distance \\((m - n)\\) influences how strongly two tokens attend to each other.</li> <li>This property enables strong length extrapolation, a natural locality bias, and robust long-context behavior.</li> <li>This allows the model to understand relationships between tokens regardless of where they appear in the sequence.</li> </ul> <p>In contrast, additive positional embeddings typically produce attention scores that depend on both absolute and relative positions, which limits generalization to longer sequences.</p>"},{"location":"training_techniques/pre_training/RoPE/#2-the-algorithm","title":"2. The Algorithm","text":"<p>RoPE works by pairing elements of the \\(d\\)-dimensional embedding and applying a 2D rotation matrix to each pair.</p> <p>For a pair of dimensions \\([x^{(1)}, x^{(2)}]\\) at position \\(m\\), the transformation is:</p> \\[ \\begin{pmatrix} \\cos m\\theta &amp; -\\sin m\\theta \\\\ \\sin m\\theta &amp; \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} x^{(1)} \\\\ x^{(2)} \\end{pmatrix} \\] <ul> <li> <p>The Angle (\\(\\theta\\)): The rotation frequency follows a geometric progression   $$   \\theta_i = 10000^{-2i/d}   $$   similar to sinusoidal embeddings.</p> </li> <li> <p>The Result: As position \\(m\\) increases, the vector rotates further. Because the rotation matrix is orthogonal, the vector norm is preserved while positional information is encoded.</p> </li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#3-where-exactly-is-rope-applied","title":"3. Where Exactly Is RoPE Applied?","text":"<p>RoPE is applied only to the Query and Key vectors in self-attention.</p> <ul> <li>Queries and Keys are rotated based on token position.</li> <li>Values are left unchanged.</li> </ul> <p>Reason:</p> <ul> <li>Attention scores are computed using the dot product \\(QK^\\top\\).</li> <li>RoPE ensures this dot product captures relative position.</li> <li>Applying RoPE to Values does not improve positional reasoning and can degrade performance.</li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#4-why-rope-works-for-dot-product-attention","title":"4. Why RoPE Works for Dot-Product Attention","text":"<p>The key mathematical insight behind RoPE is rotation composition.</p> <p>Consider the equation:</p> \\[ \\langle R(m)x, R(n)y \\rangle = \\langle x, R(n - m)y \\rangle \\] <p>Where:</p> <ul> <li> <p>\\(x, y\\): Content vectors representing token embeddings after linear projection into Query or Key space. These vectors do not contain positional information by themselves.</p> </li> <li> <p>\\(m, n\\): Absolute token positions in the sequence. For example, \\(m = 4\\) and \\(n = 10\\)</p> </li> <li> <p>\\(R(m)\\), \\(R(n)\\): Orthogonal rotation matrices parameterized by position.   Each matrix applies a set of 2D rotations across paired embedding dimensions, with rotation angles proportional to the position index.</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The standard dot product used in attention score computation.</p> </li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#step-by-step-intuition","title":"Step-by-Step Intuition","text":"<ol> <li> <p>Position Encoding via Rotation    Applying \\(R(m)\\) to \\(x\\) rotates the vector in embedding space by an amount determined by position \\(m\\).    Similarly, \\(R(n)\\) rotates \\(y\\) according to position \\(n\\).</p> </li> <li> <p>Orthogonality Property    Rotation matrices are orthogonal, meaning:    $$    R(m)^\\top = R(-m)    $$</p> </li> <li> <p>Rewriting the Dot Product    Using orthogonality:    $$    \\langle R(m)x, R(n)y \\rangle    =    \\langle x, R(m)^\\top R(n) y \\rangle    =    \\langle x, R(n - m) y \\rangle    $$</p> </li> <li> <p>Cancellation of Absolute Position    The absolute positions \\(m\\) and \\(n\\) collapse into a single relative offset \\((n - m)\\).</p> </li> </ol> <p>This identity captures the core mathematical reason why RoPE encodes relative position rather than absolute position. Below is a detailed explanation of each term and why the equation holds.</p> <p>Key implications:</p> <ul> <li>Absolute positions cancel out.</li> <li>Only the relative offset \\((n - m)\\) matters.</li> <li>This property aligns perfectly with dot-product attention.</li> <li>Tokens with the same relative spacing produce the same positional interaction, regardless of where they appear in the sequence.</li> </ul> <p>This is why RoPE integrates naturally into Transformer architectures.</p>"},{"location":"training_techniques/pre_training/RoPE/#5-rope-and-multi-head-attention","title":"5. RoPE and Multi-Head Attention","text":"<p>In multi-head attention:</p> <ul> <li>RoPE is applied independently within each attention head.</li> <li>Each head operates on a lower-dimensional subspace and applies the same frequency schedule.</li> </ul> <p>As a result:</p> <ul> <li>Low-frequency dimensions capture long-range dependencies.</li> <li>High-frequency dimensions focus on local structure.</li> </ul> <p>This creates a multi-scale positional representation across heads.</p>"},{"location":"training_techniques/pre_training/RoPE/#6-comparison-why-rope-won","title":"6. Comparison: Why RoPE Won","text":"Feature Absolute (Sinusoidal) RoPE Operation Addition Rotation (Multiplication) Relative Distance Not explicit Naturally captured via dot product Extrapolation Weak for long contexts Strong with scaling Decay No natural decay Distance-based interaction decay Implementation Simple Moderate complexity"},{"location":"training_techniques/pre_training/RoPE/#7-small-intuitive-example","title":"7. Small Intuitive Example","text":"<p>Consider a single 2D vector:</p> \\[ x = [1, 0] \\] <p>Assume one frequency \\(\\theta = 0.1\\).</p>"},{"location":"training_techniques/pre_training/RoPE/#position-1","title":"Position 1","text":"<p>Rotation angle = \\(0.1\\)</p> \\[ R(0.1)x = [\\cos 0.1, \\sin 0.1] \\approx [0.995, 0.100] \\]"},{"location":"training_techniques/pre_training/RoPE/#position-3","title":"Position 3","text":"<p>Rotation angle = \\(0.3\\)</p> \\[ R(0.3)x = [\\cos 0.3, \\sin 0.3] \\approx [0.955, 0.296] \\] <p>Dot product between the two:</p> \\[ \\langle R(0.1)x, R(0.3)x \\rangle = \\cos(0.2) \\] <p>Observation: - The dot product depends only on the positional difference \\((3 - 1)\\). - Vector magnitude is preserved. - Relative distance is directly encoded into attention.</p>"},{"location":"training_techniques/pre_training/RoPE/#8-interview-deep-dive-topics","title":"8. Interview Deep Dive Topics","text":""},{"location":"training_techniques/pre_training/RoPE/#a-context-window-extension-rope-scaling","title":"A. Context Window Extension (RoPE Scaling)","text":"<p>Question: If a model is trained on 4k context, how can it be extended to 128k?</p> <ul> <li> <p>Linear Interpolation: Scale positions as   $$   m \\rightarrow m \\cdot \\frac{L}{L'}   $$   to avoid unseen large angles.</p> </li> <li> <p>NTK-aware Scaling: Scale different frequencies differently to preserve high-frequency information for nearby tokens.</p> </li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#b-long-term-decay-property","title":"B. Long-Term Decay Property","text":"<p>RoPE introduces a natural decay in interaction strength as \\(|m - n|\\) increases. This provides a locality bias without enforcing a hard attention window, aligning well with natural language structure.</p>"},{"location":"training_techniques/pre_training/RoPE/#c-rope-vs-alibi","title":"C. RoPE vs ALiBi","text":"<ul> <li>ALiBi adds a linear bias to attention scores based on distance.</li> <li>RoPE encodes position directly into representations, allowing richer and more flexible positional reasoning.</li> </ul>"},{"location":"training_techniques/pre_training/RoPE/#9-practical-limitations-and-caveats","title":"9. Practical Limitations and Caveats","text":"<ul> <li>RoPE assumes evenly spaced token positions.</li> <li>Extreme extrapolation without scaling can cause numerical instability.</li> <li>Relative position is encoded strongly, but absolute position is implicit, which may matter for structured tasks.</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/","title":"Advanced Transformer Components & Layers","text":"<p>This section covers the architectural evolutions that define the Modern Transformer, as used in Llama 3, Mistral, PaLM, GPT-4, and similar large-scale language models. Moving beyond the original 2017 Transformer is a common requirement for senior-level AI and ML engineering interviews.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#1-normalization-rmsnorm","title":"1. Normalization: RMSNorm","text":"<p>Replaces: LayerNorm (LN)</p> <p>Traditional LayerNorm centers the input by subtracting the mean and then scales it by the variance. RMSNorm (Root Mean Square Layer Normalization) simplifies this by removing the mean-centering step and only normalizing by the root mean square.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#the-math","title":"The Math","text":"\\[ \\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} g_i \\quad \\text{where} \\quad \\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n a_i^2 + \\epsilon} \\] <p>Where:</p> <ul> <li> <p>\\(\\mathbf{a} \\in \\mathbb{R}^n\\): Input activation vector to the normalization layer, typically corresponding to a single token representation across the hidden dimension.</p> </li> <li> <p>\\(a_i\\): The \\(i\\)-th component of the input activation vector \\(\\mathbf{a}\\).</p> </li> <li> <p>\\(n\\): Dimensionality of the hidden representation, equal to the model hidden size.</p> </li> <li> <p>\\(\\text{RMS}(\\mathbf{a})\\): Root Mean Square of the activation vector, used to normalize the input magnitude without mean-centering.</p> </li> <li> <p>\\(\\epsilon\\): Small positive constant added for numerical stability to prevent division by zero, especially important for low-precision training.</p> </li> <li> <p>\\(g_i\\): Learnable scaling parameter applied element-wise after normalization. This replaces the gain parameter in LayerNorm while omitting the additive bias.</p> </li> <li> <p>\\(\\bar{a}_i\\): The normalized and rescaled output activation for the \\(i\\)-th dimension.</p> </li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways","title":"Takeaways","text":"<ul> <li>Efficiency: Eliminates mean computation and subtraction, reducing arithmetic and synchronization overhead on accelerators.</li> <li>Stability: Avoids cancellation errors from mean-centering, which is especially beneficial in low-precision FP16 and BF16 training.</li> <li>Simplicity: Removes the additive bias term since normalization already controls activation scale, leaving only a learnable gain.</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#2-structural-shift-pre-norm-architecture","title":"2. Structural Shift: Pre-Norm Architecture","text":"<p>The Change: Normalization is applied before attention or FFN blocks instead of after the residual connection.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#formulation","title":"Formulation","text":"<p>Post-Norm (Original): $$ x_{next} = \\text{Norm}(x + \\text{Sublayer}(x)) $$</p> <p>Pre-Norm (Modern): $$ x_{next} = x + \\text{Sublayer}(\\text{Norm}(x)) $$</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_1","title":"Takeaways","text":"<ul> <li>Gradient Flow: Clean residual paths enable stable training of very deep models</li> <li>Optimization Robustness: Reduces reliance on fragile learning rate warmup</li> <li>Industry Standard: Used in essentially all modern decoder-only LLMs</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#3-activation-excellence-swiglu","title":"3. Activation Excellence: SwiGLU","text":"<p>Replaces: ReLU, GELU</p> <p>SwiGLU is a gated linear unit variant that uses two linear projections, one gated by the Swish (SiLU) activation.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#the-math_1","title":"The Math","text":"\\[ \\text{SwiGLU}(x) = \\text{Swish}(xW + b) \\otimes (xV + c) \\]"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_2","title":"Takeaways","text":"<ul> <li>Higher Expressivity: Multiplicative gating enables richer feature interactions</li> <li>Better Scaling: Improves convergence and downstream quality at large scale</li> <li>Default Choice: Used in Llama, PaLM, and most modern LLM families</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#4-stability-choice-bias-free-linear-layers","title":"4. Stability Choice: Bias-Free Linear Layers","text":"<p>The Change: Remove additive bias terms from linear layers.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_3","title":"Takeaways","text":"<ul> <li>Training Stability: Reduces activation spikes in deep or wide networks</li> <li>Length Extrapolation: Improves robustness to longer sequences than seen during training</li> <li>Redundancy Removal: Bias is largely unnecessary when paired with normalization</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#5-positional-encoding-rotary-positional-embeddings-rope","title":"5. Positional Encoding: Rotary Positional Embeddings (RoPE)","text":"<p>Replaces: Absolute or learned positional embeddings</p> <p>RoPE encodes positional information by rotating query and key vectors in a complex plane.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_4","title":"Takeaways","text":"<ul> <li>Relative Positioning: Attention depends on relative token distance</li> <li>Natural Decay: Long-range attention strength decreases smoothly</li> <li>Context Extension: Enables interpolation and scaling for long context windows</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#6-efficient-attention-grouped-query-attention-gqa","title":"6. Efficient Attention: Grouped-Query Attention (GQA)","text":"<p>The Middle Ground: Between Multi-Head Attention (MHA) and Multi-Query Attention (MQA)</p> <p>In GQA, multiple query heads share a smaller set of key and value heads.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_5","title":"Takeaways","text":"<ul> <li>KV Cache Efficiency: Substantially reduces inference memory usage</li> <li>Quality Retention: Maintains accuracy close to full MHA</li> <li>Inference Speed: Critical for fast generation on modern hardware</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#7-attention-kernel-optimization-flashattention","title":"7. Attention Kernel Optimization: FlashAttention","text":"<p>What It Is: A fused attention kernel that avoids materializing the full attention matrix in memory.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_6","title":"Takeaways","text":"<ul> <li>Memory Bandwidth Optimization: Turns attention into a compute-efficient operation</li> <li>Asymptotic Improvement: Reduces memory from O(n\u00b2) to O(n)</li> <li>Production Standard: Used in nearly all state-of-the-art LLM stacks</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#8-parallel-attention-and-ffn","title":"8. Parallel Attention and FFN","text":"<p>The Change: Attention and FFN blocks are computed in parallel rather than sequentially within a transformer layer.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_7","title":"Takeaways","text":"<ul> <li>Lower Latency: Improves throughput in training and inference</li> <li>Scalability: Works well at large model scales</li> <li>Adopted By: PaLM and related architectures</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#9-context-extension-beyond-rope","title":"9. Context Extension Beyond RoPE","text":""},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#91-rope-scaling-and-interpolation","title":"9.1 RoPE Scaling and Interpolation","text":"<ul> <li>NTK-aware scaling</li> <li>Linear or dynamic position scaling</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#92-sliding-window-attention","title":"9.2 Sliding Window Attention","text":"<ul> <li>Restricts attention to a fixed recent context</li> <li>Prevents unbounded KV cache growth</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_8","title":"Takeaways","text":"<ul> <li>These are architectural inference-time decisions</li> <li>Often combined with GQA for long-context efficiency</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#10-mixture-of-experts-moe","title":"10. Mixture of Experts (MoE)","text":"<p>What Changes: Dense FFNs are replaced with multiple expert FFNs and a learned routing mechanism.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_9","title":"Takeaways","text":"<ul> <li>Sparse Activation: Only a small subset of experts is active per token</li> <li>Efficient Scaling: Parameter count increases without proportional compute</li> <li>Systems Complexity: Routing and load balancing dominate implementation challenges</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#11-kv-cache-optimizations","title":"11. KV Cache Optimizations","text":"<p>Beyond GQA, modern systems apply several cache-level improvements:</p> <ul> <li>Reduced precision KV storage</li> <li>Prefix and prompt caching</li> <li>Cache eviction or compression strategies</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_10","title":"Takeaways","text":"<ul> <li>Inference performance is usually memory-bound</li> <li>KV cache design often dominates real-world latency</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#12-weight-tying-and-parameter-sharing","title":"12. Weight Tying and Parameter Sharing","text":"<p>What It Is: Sharing token embedding and output projection weights.</p>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#takeaways_11","title":"Takeaways","text":"<ul> <li>Improves sample efficiency</li> <li>Reduces total parameter count</li> <li>Remains standard in decoder-only LLMs</li> </ul>"},{"location":"training_techniques/pre_training/advanced_transformer_components_and_layers/#summary-table","title":"\ud83d\udcca Summary Table","text":"Component Modern Standard Key Advantage Normalization RMSNorm Stable and efficient normalization Norm Placement Pre-Norm Enables deep transformer training Activation SwiGLU Improved expressivity and convergence Positional Encoding RoPE + Scaling Relative positioning and long context Attention Type GQA Efficient KV cache usage Attention Kernel FlashAttention Memory-efficient long-context attention FFN Structure Parallel FFN Reduced latency Linear Layers Bias-Free Stability at scale Context Handling Sliding Window Bounded memory growth Scaling Strategy MoE Sparse compute scaling"},{"location":"training_techniques/pre_training/byte_pair_encoding/","title":"Byte Pair Encoding","text":""},{"location":"training_techniques/pre_training/byte_pair_encoding/#1-overview","title":"1. Overview","text":"<p>Byte Pair Encoding (BPE) and its modern variants are the dominant subword tokenization methods used in today\u2019s Large Language Models (LLMs), including GPT-3, GPT-4, and LLaMA. BPE strikes a practical balance between word-level and character-level tokenization, enabling scalable training while avoiding out-of-vocabulary failures.</p> <p>BPE addresses the out-of-vocabulary (OOV) problem by decomposing text into frequently occurring subword units instead of relying on a fixed word vocabulary. Any unseen word can still be represented as a sequence of known subcomponents.</p> <p>This makes BPE robust, flexible, and suitable for web-scale corpora with noisy and evolving language.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#2-how-bpe-works-the-algorithm","title":"2. How BPE Works (The Algorithm)","text":"<ol> <li> <p>Initialize    Start with a vocabulary of base symbols, either characters or UTF-8 bytes.</p> </li> <li> <p>Frequency Analysis    Count all adjacent token pairs in the training corpus.</p> </li> <li> <p>Merge    Replace the most frequent adjacent pair with a new token.</p> </li> <li> <p>Iterate    Repeat until a predefined vocabulary size is reached.</p> </li> </ol>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#21-step-by-step-example","title":"2.1 Step-by-Step Example","text":"<p>Consider a tiny corpus: [low, lower, newest, widest]</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#step-1-initialize","title":"Step 1: Initialize","text":"<p>Start with characters as the base vocabulary and mark word boundaries:</p> <p>l o w _  l o w e r _  n e w e s t _  w i d e s t _ </p> <p>Initial tokens are individual characters.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#step-2-frequency-analysis","title":"Step 2: Frequency Analysis","text":"<p>Count all adjacent token pairs across the corpus.</p> <p>Some frequent pairs:</p> <ul> <li><code>l o</code></li> <li><code>o w</code></li> <li><code>e s</code></li> <li><code>s t</code></li> </ul> <p>Assume <code>o w</code> is the most frequent pair.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#step-3-merge","title":"Step 3: Merge","text":"<p>Merge <code>o w \u2192 ow</code> everywhere:</p> <p>l ow _  l ow e r _  n e w e s t _  w i d e s t _ </p> <p>Vocabulary now includes <code>ow</code>.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#step-4-iterate","title":"Step 4: Iterate","text":"<p>Repeat frequency analysis and merging.</p> <p>Next merges might be:</p> <ul> <li><code>l ow \u2192 low</code></li> <li><code>e s \u2192 es</code></li> <li><code>es t \u2192 est</code></li> </ul> <p>Eventually the corpus may look like:</p> <p>low _  lower _  new est _  wid est _ </p> <p>And the vocabulary contains: [l, o, w, e, r, n, i, d, t, low, est, wid, lower, _]</p> <p>The process stops once the target vocabulary size is reached.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#22-vocabulary-size","title":"2.2 Vocabulary Size","text":"<p>The final vocabulary size is:</p> \\[ V = S + N_{\\text{merges}} \\] <p>Where: - \\(S\\) is the number of base symbols - \\(N_{\\text{merges}}\\) is the number of merge operations</p> <p>Each merge permanently adds one new token.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#23-key-properties","title":"2.3 Key Properties","text":"<ul> <li>Greedy: Always merges the most frequent pair</li> <li>Deterministic: Same data and merges produce the same vocabulary</li> <li>Irreversible: Bad early merges cannot be undone</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#24-where-bpe-shines","title":"2.4 Where BPE Shines","text":""},{"location":"training_techniques/pre_training/byte_pair_encoding/#1-common-words-and-morphemes","title":"1. Common Words and Morphemes","text":"<p>Frequently occurring patterns like:</p> <ul> <li><code>ing</code>, <code>tion</code>, <code>http</code>, <code>://</code> become single tokens, reducing sequence length and improving efficiency.</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#2-open-vocabulary","title":"2. Open Vocabulary","text":"<p>Any new word can be represented as subwords: </p> <p><code>unhappiness \u2192 un + happi + ness</code></p> <p>No <code>[UNK]</code> tokens are required.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#3-web-scale-text","title":"3. Web-Scale Text","text":"<p>BPE handles:</p> <ul> <li>URLs</li> <li>Code</li> <li>Typos</li> <li>Mixed-language text</li> </ul> <p>very well, especially in byte-level variants.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#25-where-bpe-fails-or-struggles","title":"2.5 Where BPE Fails or Struggles","text":""},{"location":"training_techniques/pre_training/byte_pair_encoding/#1-rare-words-and-low-resource-languages","title":"1. Rare Words and Low-Resource Languages","text":"<p>Languages with rich morphology or limited data are often split into many tokens, making them:</p> <ul> <li>More expensive to process</li> <li>Harder to learn effectively</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#2-numbers-and-arithmetic","title":"2. Numbers and Arithmetic","text":"<p>BPE tokenizes numbers inconsistently:</p> <p><code>1000 \u2192 [1000]</code> <code>10001 \u2192 [100, 01]</code></p> <p>This breaks digit-level reasoning and arithmetic.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#3-spelling-and-character-level-tasks","title":"3. Spelling and Character-Level Tasks","text":"<p>If a word becomes a single token: <code>strawberry \u2192 [strawberry]</code></p> <p>The model cannot directly reason about individual letters.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#4-early-merge-bias","title":"4. Early Merge Bias","text":"<p>Because merges are greedy:</p> <ul> <li>Early frequent patterns dominate the vocabulary</li> <li>Suboptimal merges persist forever</li> <li>Later data distributions cannot correct them</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#5-sensitivity-to-formatting","title":"5. Sensitivity to Formatting","text":"<p>Whitespace, casing, and punctuation affect token boundaries: <code>\"hello\" \u2260 \" hello\"</code></p> <p>Prompt formatting can significantly change tokenization.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#3-tokenization-strategies-compared","title":"3. Tokenization Strategies Compared","text":"Strategy Pros Cons Word-level High semantic meaning Huge vocabulary, OOV issues Character-level No OOV, small vocabulary Long sequences, weak semantics Subword (BPE) Balanced efficiency and flexibility Linguistically unintuitive splits"},{"location":"training_techniques/pre_training/byte_pair_encoding/#4-why-bpe-works-well-for-llms","title":"4. Why BPE Works Well for LLMs","text":"<ul> <li> <p>Token Efficiency   Frequent strings like <code>the</code>, <code>ing</code>, or <code>http</code> become single tokens, reducing sequence length.</p> </li> <li> <p>Statistical Morphology   Related words such as <code>play</code>, <code>playing</code>, and <code>played</code> often share subword units.   This is an emergent frequency effect, not explicit linguistic understanding.</p> </li> <li> <p>Byte-level Robustness   Byte-level BPE operates on UTF-8 bytes, guaranteeing that any string can be tokenized without an <code>[UNK]</code> token.</p> </li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#5-bpe-variants-used-in-practice","title":"5. BPE Variants Used in Practice","text":"<p>Modern LLMs rarely use vanilla BPE.</p> <ul> <li> <p>Byte-level BPE (GPT-2, GPT-4)   Uses bytes as base symbols, eliminating unknown tokens.</p> </li> <li> <p>SentencePiece BPE (LLaMA)   Avoids pre-tokenization and treats whitespace as a normal symbol.</p> </li> <li> <p>Unigram Language Model (SentencePiece)   Uses a probabilistic model over subwords instead of greedy merges, allowing multiple valid tokenizations.</p> </li> </ul> <p>Key distinction:</p> <ul> <li>BPE is deterministic and greedy</li> <li>Unigram LM is probabilistic and more flexible</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#6-pre-tokenization-and-whitespace-effects","title":"6. Pre-tokenization and Whitespace Effects","text":"<p>Tokenization is sensitive to whitespace and formatting.</p> <ul> <li><code>\"hello\"</code> and <code>\" hello\"</code> may map to different tokens</li> <li>Leading spaces often act as token boundaries</li> <li>Prompt formatting directly influences tokenization</li> </ul> <p>These effects explain why small formatting changes can significantly alter model behavior.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#7-tokenization-paradoxes-interview-level-insights","title":"7. Tokenization Paradoxes (Interview-Level Insights)","text":""},{"location":"training_techniques/pre_training/byte_pair_encoding/#the-spelling-paradox","title":"The Spelling Paradox","text":"<p>LLMs struggle to count letters in words. - Reason: words often appear as single tokens, hiding character-level structure.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#the-arithmetic-paradox","title":"The Arithmetic Paradox","text":"<p>LLMs fail at digit-wise arithmetic on large numbers. - Reason: numbers are split inconsistently across tokens.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#the-case-sensitivity-trap","title":"The Case Sensitivity Trap","text":"<p><code>Hello</code>, <code>hello</code>, and <code>HELLO</code> are distinct tokens. - Effect: the model must learn redundant representations.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#8-pros-and-cons-of-bpe","title":"8. Pros and Cons of BPE","text":""},{"location":"training_techniques/pre_training/byte_pair_encoding/#pros","title":"Pros","text":"<ul> <li>Adaptive to training data</li> <li>No OOV failures</li> <li>Balanced embedding matrix size (typically 32k\u2013100k tokens)</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#cons","title":"Cons","text":"<ul> <li>Greedy and irreversible merges</li> <li>English-centric bias</li> <li>Poor handling of low-resource languages</li> <li>Weak inductive bias for spelling and arithmetic</li> </ul>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#takeaway","title":"Takeaway","text":"<p>BPE is best understood as a compression-driven algorithm, not a linguistic one. It excels at efficiency and scalability but introduces structural biases that directly affect reasoning, arithmetic, and multilingual performance.</p> <p>Many LLM failure modes are rooted in tokenization choices rather than model architecture.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#9-tokenization-is-part-of-the-model","title":"9. Tokenization Is Part of the Model","text":"<p>Tokenization defines:</p> <ul> <li>The atomic prediction units</li> <li>The embedding space structure</li> <li>What patterns are easy or hard to learn</li> </ul> <p>Changing tokenization usually requires:</p> <ul> <li>Relearning embeddings</li> <li>Often retraining the entire model</li> </ul> <p>Tokenization errors cannot be fixed by fine-tuning alone.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#10-tokenization-and-cost","title":"10. Tokenization and Cost","text":"<p>Tokenization affects:</p> <ul> <li>Context length utilization</li> <li>Inference latency</li> <li>Serving cost</li> </ul> <p>Languages or domains that produce more tokens:</p> <ul> <li>Consume context faster</li> <li>Cost more to serve</li> <li>Often exhibit lower effective performance</li> </ul> <p>This creates fairness and efficiency challenges in multilingual models.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#11-beyond-bpe-emerging-alternatives","title":"11. Beyond BPE: Emerging Alternatives","text":"<p>Active research explores:</p> <ul> <li>Character-aware transformers</li> <li>Byte-level models without merges</li> <li>Token-free architectures</li> </ul> <p>Motivation:</p> <ul> <li>Reduce tokenization artifacts</li> <li>Improve multilingual equity</li> <li>Improve spelling and arithmetic</li> </ul> <p>Trade-off:</p> <ul> <li>Higher compute cost</li> <li>Slower convergence</li> </ul> <p>As of 2025, BPE-based methods remain dominant due to their efficiency and scalability.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#12-interview-faq","title":"12. Interview FAQ","text":"<p>Q: How is BPE different from WordPiece? A: BPE merges based on raw frequency, while WordPiece merges to maximize likelihood under a language model.</p> <p>Q: Why use byte-level BPE? A: It guarantees full coverage with a base vocabulary of 256 bytes, eliminating unknown tokens.</p> <p>Q: Can tokenization be changed after training? A: Not easily. Tokenization changes typically require retraining embeddings and often the full model.</p>"},{"location":"training_techniques/pre_training/byte_pair_encoding/#13-conceptual-implementation-snippet","title":"13. Conceptual Implementation Snippet","text":"<pre><code>def get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\ndef merge(ids, pair, idx):\n    new_ids = []\n    i = 0\n    while i &lt; len(ids):\n        if i &lt; len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n            new_ids.append(idx)\n            i += 2\n        else:\n            new_ids.append(ids[i])\n            i += 1\n    return new_ids\n</code></pre>"},{"location":"training_techniques/pre_training/mixture_of_experts/","title":"Mixture of Experts (MoE)","text":""},{"location":"training_techniques/pre_training/mixture_of_experts/#1-overview","title":"1. Overview","text":"<p>Mixture of Experts (MoE) is an architectural paradigm that enables scaling model capacity to frontier levels while keeping per-token inference compute manageable. It allows a model to store far more knowledge than a dense model with similar inference cost, making it a key technique behind models such as GPT-4, Mixtral, and Grok.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#2-core-concept-and-intuition","title":"2. Core Concept and Intuition","text":"<p>In a standard dense Transformer, every parameter participates in processing every token.</p> <p>The problem with dense scaling</p> <ul> <li>Increasing parameters increases capacity</li> <li>But inference cost, latency, and memory usage scale linearly with model size</li> </ul> <p>The MoE solution</p> <p>MoE decouples model capacity from inference compute by activating only a small subset of parameters for each token.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#the-specialist-analogy","title":"The Specialist Analogy","text":"<p>Instead of one generalist handling all tasks, imagine a panel of specialists.</p> <ul> <li>A routing system decides which specialists should handle each input</li> <li>Only those specialists are consulted</li> </ul> <p>Key distinction:</p> <ul> <li>Total parameters represent the full knowledge capacity</li> <li>Active parameters determine inference cost for a given token</li> </ul>"},{"location":"training_techniques/pre_training/mixture_of_experts/#3-architecture-the-sparse-transformer","title":"3. Architecture: The Sparse Transformer","text":"<p>An MoE model is identical to a standard Transformer except that the Feed-Forward Network (FFN) layers are replaced with MoE layers.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#components-of-an-moe-layer","title":"Components of an MoE Layer","text":"<ol> <li>Experts (\\(E_i\\)) </li> </ol> <p>A set of \\(N\\) independent FFNs, each with its own parameters.</p> <ol> <li>Router / Gating Network (\\(G\\)) </li> </ol> <p>A small learnable function that scores which experts should process a given token.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#routing-mechanism","title":"Routing Mechanism","text":"<p>For an input token representation \\(x\\), the output of an MoE layer is:</p> \\[ y = \\sum_{i=1}^{N} G(x)_i \\cdot E_i(x) \\] <p>In sparse MoE, a Top-k routing strategy is used:</p> <ul> <li>Only the top \\(k\\) experts receive non-zero weights</li> <li>All other experts are skipped entirely</li> <li>Typically \\(k = 1\\) or \\(k = 2\\)</li> </ul> <p>Only the selected experts are evaluated, making computation and gradient flow sparse.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#case-study-mixtral-8x7b","title":"Case Study: Mixtral 8x7B","text":"<ul> <li>Total experts: 8</li> <li>Routing: Top-2 per token</li> <li>Active parameters per token: ~13B</li> <li>Total parameters: ~47B</li> </ul> <p>The model exhibits capacity comparable to a ~50B dense model while running at the speed of a ~13B model.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#4-expert-capacity-and-token-dropping","title":"4. Expert Capacity and Token Dropping","text":"<p>Each expert has a fixed capacity, which limits how many tokens it can process in a single batch. This capacity is typically set as a multiple of the expected average load per expert.</p> <p>If too many tokens are routed to the same expert in a batch:</p> <ul> <li>Excess tokens may be dropped entirely</li> <li>Or processed with reduced routing weight, meaning their contribution to the expert\u2019s output is scaled down to maintain numerical and compute stability</li> <li>Or rerouted to fallback experts, depending on the implementation</li> </ul> <p>This mechanism prevents individual experts from becoming compute or memory bottlenecks but introduces a trade-off:</p> <ul> <li>Larger capacity improves training stability and model quality</li> <li>Smaller capacity improves efficiency but risks silent quality degradation due to dropped tokens</li> </ul> <p>Monitoring expert utilization and token dropping rates is therefore critical during training and debugging MoE models.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#why-reduce-routing-weight-even-when-the-expert-is-correct","title":"Why Reduce Routing Weight Even When the Expert Is Correct?","text":"<p>Routing decides which expert is best for a token. Capacity limits decide how much influence that token is allowed to have in a given batch.</p> <p>When an expert exceeds its capacity, all routed tokens are still correct assignments, but the system cannot afford to:</p> <ul> <li>Process unlimited tokens</li> <li>Accumulate unbounded gradients</li> <li>Let one expert dominate training</li> </ul> <p>Reducing the routing weight is a soft fallback:</p> <ul> <li>The token is still processed by the correct expert</li> <li>Its output and gradients are scaled down</li> <li>Compute and training stability are preserved</li> </ul> <p>The reduced weight does not indicate lower correctness. It limits influence to protect compute budgets and prevent expert collapse while retaining partial learning signal.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#5-training-dynamics-and-stability","title":"5. Training Dynamics and Stability","text":""},{"location":"training_techniques/pre_training/mixture_of_experts/#benefits-of-moe-training","title":"Benefits of MoE Training","text":"<ul> <li>Compute efficiency: Lower validation loss for the same training FLOPs compared to dense models</li> <li>Knowledge scaling: Experts can store long-tail facts and rare patterns efficiently</li> <li>Faster convergence: Sparse FFNs reduce redundant computation</li> </ul>"},{"location":"training_techniques/pre_training/mixture_of_experts/#mode-collapse-and-expert-imbalance","title":"Mode Collapse and Expert Imbalance","text":"<p>A common failure mode is expert collapse:</p> <ul> <li>Early-random advantages cause one expert to receive more tokens</li> <li>That expert improves faster due to more gradients</li> <li>Other experts receive fewer updates and remain undertrained</li> </ul>"},{"location":"training_techniques/pre_training/mixture_of_experts/#auxiliary-losses-for-stability","title":"Auxiliary Losses for Stability","text":"<p>To prevent collapse, MoE training includes additional losses:</p> <ul> <li>Load Balancing Loss: Penalizes uneven token distribution across experts</li> <li>Z-Loss: Penalizes large router logits to improve numerical stability</li> </ul> <p>These losses are essential for maintaining expert diversity.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#5-emergent-expert-specialization","title":"5. Emergent Expert Specialization","text":"<p>Experts are not manually assigned domains.</p> <p>Specialization emerges implicitly from:</p> <ul> <li>Routing gradients</li> <li>Data distribution</li> <li>Load balancing constraints</li> </ul> <p>In practice, experts often specialize in:</p> <ul> <li>Syntax and formatting</li> <li>Punctuation and boilerplate</li> <li>Code versus natural language</li> <li>Long-context versus short-context tokens</li> </ul> <p>MoE does not guarantee clean semantic specialization such as math or biology experts.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#6-what-moe-improves-and-what-it-does-not","title":"6. What MoE Improves and What It Does Not","text":""},{"location":"training_techniques/pre_training/mixture_of_experts/#moe-primarily-improves","title":"MoE Primarily Improves","text":"<ul> <li>Factual recall</li> <li>Coverage of rare or long-tail patterns</li> <li>Knowledge density per inference FLOP</li> </ul>"},{"location":"training_techniques/pre_training/mixture_of_experts/#moe-does-not-automatically-improve","title":"MoE Does Not Automatically Improve","text":"<ul> <li>Multi-step reasoning</li> <li>Logical consistency</li> <li>Planning and abstraction</li> </ul> <p>Reasoning quality depends more on:</p> <ul> <li>Attention mechanisms</li> <li>Data quality</li> <li>Post-training alignment and RL</li> </ul>"},{"location":"training_techniques/pre_training/mixture_of_experts/#7-inference-and-deployment-trade-offs","title":"7. Inference and Deployment Trade-offs","text":"Aspect Impact Throughput High, due to sparse computation Latency Low, driven by active parameter count VRAM Usage Very high, since all experts must be resident Communication High, requires all-to-all routing in distributed setups <p>MoE models are often memory-bandwidth bound, not compute-bound.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#8-training-cost-vs-inference-cost","title":"8. Training Cost vs Inference Cost","text":"<p>MoE reduces inference cost but increases training complexity:</p> <ul> <li>More communication overhead</li> <li>More fragile optimization</li> <li>Harder distributed orchestration</li> </ul> <p>MoE is most effective when:</p> <ul> <li>A model is trained once</li> <li>Served at massive scale</li> <li>Inference cost dominates total lifetime cost</li> </ul> <p>Dense models may be preferable for smaller-scale or latency-critical use cases.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#9-moe-in-the-scaling-toolbox","title":"9. MoE in the Scaling Toolbox","text":"Strategy Key Idea Trade-off Dense scaling Increase parameters Expensive inference MoE Sparse activation Memory and communication overhead Longer training More tokens per parameter Higher one-time cost Quantization Lower precision Potential accuracy loss <p>MoE is a powerful but specialized tool, not a universal solution.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#10-key-takeaways","title":"10. Key Takeaways","text":"<ul> <li>MoE decouples capacity from inference compute</li> <li>It is most effective for knowledge-heavy scaling</li> <li>Training is harder, inference is cheaper</li> <li>Many failures stem from routing imbalance and systems constraints</li> </ul> <p>MoE reflects a broader trend in modern LLMs: scaling is as much a systems problem as it is a modeling problem.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#11-some-questions","title":"11. Some Questions","text":"<p>Q: Does MoE reduce attention bottlenecks? A: No. MoE typically replaces FFN layers. Attention remains dense, so KV cache memory and attention compute are unchanged.</p> <p>Q: Why use Top-2 routing instead of Top-1? A: Top-2 provides smoother gradients and backup information flow, improving training stability.</p> <p>Q: What is the main bottleneck when serving MoE models? A: Memory bandwidth and communication, not raw FLOPs.</p>"},{"location":"training_techniques/pre_training/mixture_of_experts/#references","title":"References","text":"<ol> <li>https://huggingface.co/blog/moe</li> </ol>"},{"location":"training_techniques/pre_training/pre_training/","title":"Pre-Training","text":""},{"location":"training_techniques/pre_training/pre_training/#1-overview","title":"1. Overview","text":"<p>Pre-training defines the raw capability ceiling of a Large Language Model. Architectural choices, data quality, and scaling decisions made at this stage dominate downstream performance far more than post-training alignment or prompting tricks. Most limitations observed later are traceable to decisions made here.</p>"},{"location":"training_techniques/pre_training/pre_training/#2-objectives-and-scaling","title":"2. Objectives and Scaling","text":""},{"location":"training_techniques/pre_training/pre_training/#21-self-supervised-learning","title":"2.1 Self-Supervised Learning","text":"<p>LLMs are trained using self-supervision, where labels are derived directly from the data itself. Given a sequence of tokens:</p> \\[ x = (x_1, x_2, \\dots, x_T) \\] <p>the model learns to predict the next token:</p> \\[ P(x_t \\mid x_1, \\dots, x_{t-1}) \\] <p>This formulation:</p> <ul> <li>Requires no human annotation</li> <li>Scales naturally with data size</li> <li>Supports emergent behaviors such as reasoning and in-context learning</li> </ul> <p>Despite its simplicity, this objective implicitly captures syntax, semantics, world knowledge, and procedural patterns.</p>"},{"location":"training_techniques/pre_training/pre_training/#22-negative-log-likelihood-objective","title":"2.2 Negative Log Likelihood Objective","text":"<p>Training minimizes the Negative Log Likelihood (NLL):</p> \\[ \\mathcal{L} = - \\mathbb{E}_{x \\sim \\mathcal{D}} \\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{&lt;t}) \\] <p>Key properties:</p> <ul> <li>Equivalent to minimizing cross-entropy</li> <li>Strongly penalizes confident incorrect predictions</li> <li>Encourages calibration under idealized assumptions</li> </ul> <p>Important limitation:</p> <ul> <li>NLL optimizes average token prediction, not task success, reasoning correctness, or truthfulness.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#23-chinchilla-scaling-laws-training-optimality","title":"2.3 Chinchilla Scaling Laws (Training Optimality)","text":"<p>The Chinchilla results (Hoffmann et al.) showed that many prior models were over-parameterized and under-trained.</p> <p>Core insight:</p> <p>For a fixed compute budget, model performance is maximized when model size and training tokens are scaled proportionally (roughly 20 tokens per parameter).</p> <p>This shifted industry practice from \"bigger models\" to \"compute-optimal training\" with massive, high-quality corpora.</p>"},{"location":"training_techniques/pre_training/pre_training/#24-inference-optimal-scaling-the-llama-paradigm","title":"2.4 Inference-Optimal Scaling (The LLaMA Paradigm)","text":""},{"location":"training_techniques/pre_training/pre_training/#241-background-why-chinchilla-is-not-the-full-story","title":"2.4.1 Background: Why Chinchilla Is Not the Full Story","text":"<p>The Chinchilla scaling laws optimize for training compute efficiency. They answer the question:</p> <p>Given a fixed training compute budget, how should we allocate it between model size and training tokens to minimize loss?</p> <p>This framing is correct for research experiments and one-off model training runs. However, it ignores a critical real-world constraint:</p> <p>Most of the cost of an LLM is paid after training, during inference.</p> <p>In production systems, a model may be:</p> <ul> <li>Trained once</li> <li>Served millions or billions of times</li> </ul> <p>This changes the optimization target entirely.</p>"},{"location":"training_techniques/pre_training/pre_training/#242-training-cost-vs-inference-cost","title":"2.4.2 Training Cost vs Inference Cost","text":"<p>It is essential to distinguish the two:</p> <p>Training cost</p> <ul> <li>Scales with: parameters \u00d7 tokens \u00d7 optimization overhead</li> <li>Paid once</li> </ul> <p>Inference cost</p> <ul> <li>Scales primarily with: number of active parameters</li> <li>Paid per request</li> <li>Dominates total cost in deployed systems</li> </ul> <p>Key insight:</p> <p>Inference cost depends on model size, not on how many tokens the model saw during training.</p> <p>This means that Chinchilla-optimal models may be cheap to train but expensive to serve.</p>"},{"location":"training_techniques/pre_training/pre_training/#243-the-shift-in-optimization-objective","title":"2.4.3 The Shift in Optimization Objective","text":"<p>This leads to a new question:</p> <p>Given a fixed inference budget or deployment footprint, how do we maximize model quality?</p> <p>This is where inference-optimal scaling emerges.</p>"},{"location":"training_techniques/pre_training/pre_training/#244-chinchilla-optimal-vs-inference-optimal","title":"2.4.4 Chinchilla-Optimal vs Inference-Optimal","text":""},{"location":"training_techniques/pre_training/pre_training/#chinchilla-optimal-scaling","title":"Chinchilla-Optimal Scaling","text":"<ul> <li>Train very large models</li> <li>Use relatively fewer training tokens</li> <li>Minimizes training loss per unit of training compute</li> </ul> <p>This results in:</p> <ul> <li>Large parameter counts</li> <li>High inference latency and cost</li> <li>Practical difficulty deploying at scale</li> </ul> <p>Modern open-weights models (e.g., LLaMA 3) are \"over-trained\" by Chinchilla standards (e.g., &gt;100x tokens per parameter) to maximize performance for a specific deployment footprint.</p>"},{"location":"training_techniques/pre_training/pre_training/#inference-optimal-scaling","title":"Inference-Optimal Scaling","text":"<ul> <li>Train smaller models</li> <li>Train them on far more tokens than Chinchilla recommends</li> <li>Accept higher training cost to reduce inference cost</li> </ul> <p>This results in:</p> <ul> <li>Fewer parameters</li> <li>Lower latency and memory usage</li> <li>Better cost-performance trade-off in production</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#245-why-over-training-smaller-models-works","title":"2.4.5 Why Over-Training Smaller Models Works","text":"<p>Smaller models are capacity-limited, not data-limited.</p> <p>By exposing them to vastly more data:</p> <ul> <li>Representations become more robust</li> <li>Rare patterns are reinforced</li> <li>Generalization improves significantly</li> </ul> <p>Even though Chinchilla would label this as \"over-training\", the additional data:</p> <ul> <li>Continues to reduce downstream error</li> <li>Improves reasoning and instruction-following</li> <li>Makes the model competitive with much larger alternatives</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#246-the-llama-paradigm","title":"2.4.6 The LLaMA Paradigm","text":"<p>Modern open-weight models such as LLaMA 2 and LLaMA 3 follow this strategy.</p> <p>Characteristics:</p> <ul> <li>Moderate parameter count</li> <li>Extremely high tokens-per-parameter ratio</li> <li>Often tens to hundreds of times more tokens per parameter than Chinchilla-optimal</li> </ul> <p>This design choice:</p> <ul> <li>Maximizes quality for a fixed inference budget</li> <li>Enables efficient deployment on limited hardware</li> <li>Makes open models more practical for real-world use</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#247-learnings","title":"2.4.7 Learnings","text":""},{"location":"training_techniques/pre_training/pre_training/#1-scaling-laws-are-objective-dependent","title":"1. Scaling Laws Are Objective-Dependent","text":"<p>There is no single \"optimal\" scaling rule. - Chinchilla optimizes training compute - Inference-optimal scaling optimizes deployment cost</p> <p>Always ask: What is the objective being optimized?</p>"},{"location":"training_techniques/pre_training/pre_training/#2-training-longer-can-be-better-than-training-bigger","title":"2. Training Longer Can Be Better Than Training Bigger","text":"<p>For production systems: - Smaller, heavily trained models often outperform - Larger, lightly trained models are harder to serve</p> <p>This reverses earlier intuitions from the pre-Chinchilla era.</p>"},{"location":"training_techniques/pre_training/pre_training/#3-tokens-are-cheaper-than-parameters-at-inference-time","title":"3. Tokens Are Cheaper Than Parameters at Inference Time","text":"<ul> <li>Extra training tokens increase one-time cost</li> <li>Extra parameters increase recurring cost</li> </ul> <p>In large-scale deployments, recurring costs dominate.</p>"},{"location":"training_techniques/pre_training/pre_training/#4-model-design-is-a-systems-problem","title":"4. Model Design Is a Systems Problem","text":"<p>Model size, data scale, and deployment constraints are tightly coupled. Good LLM design requires: - ML theory - Systems thinking - Cost-aware decision making</p>"},{"location":"training_techniques/pre_training/pre_training/#248-compute-vs-data-vs-parameters","title":"2.4.8 Compute vs Data vs Parameters","text":"<p>Pre-training is a three-way trade-off:</p> Resource Bottleneck Effect Parameters Capacity ceiling Data Generalization and robustness Compute Training duration and batch size <p>Common failure modes:</p> <ul> <li>Too many parameters leads to memorization</li> <li>Too little data leads to brittle generalization</li> <li>Insufficient compute leads to undertrained representations</li> </ul> <p>Modern frontier models are primarily data-limited, not architecture-limited.</p>"},{"location":"training_techniques/pre_training/pre_training/#3-data-pipeline-and-quality","title":"3. Data Pipeline and Quality","text":"<p>Data quality is often more important than model size. \"Garbage in, garbage out\" applies strictly to LLMs.</p>"},{"location":"training_techniques/pre_training/pre_training/#31-raw-data-sources","title":"3.1 Raw Data Sources","text":"<p>Typical pre-training corpora include:</p> <ul> <li>Web crawl data (CommonCrawl)</li> <li>Books and long-form text</li> <li>Code repositories (GitHub)</li> <li>Mathematical and scientific text (arXiv)</li> <li>Structured and semi-structured documents</li> </ul> <p>Each domain contributes different inductive biases:</p> <ul> <li>Code improves logical consistency and state tracking</li> <li>Math improves symbolic manipulation</li> <li>Long-form text improves discourse modeling</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#32-data-cleaning-and-pii-redaction","title":"3.2 Data Cleaning and PII Redaction","text":"<p>Cleaning steps typically include:</p> <ul> <li>HTML boilerplate removal: Strips navigation bars, scripts, ads, and markup artifacts so the model learns from meaningful textual content rather than page structure noise.</li> <li>Unicode normalization: Converts visually or semantically equivalent characters into a canonical form to reduce vocabulary fragmentation and stabilize token statistics.</li> <li>Language-specific token normalization: Applies rules tailored to each language, such as lowercasing, diacritic handling, or script normalization, to improve consistency and learning efficiency.</li> <li>Removal of corrupted or truncated documents</li> </ul> <p>PII Redaction (Privacy):</p> <p>Strictly required for enterprise models to prevent regurgitating private data.</p> <ul> <li>Regex-based: Removing emails, SSNs, phone numbers.</li> <li>Entity Recognition: Replacing proper names with placeholders (e.g., <code>&lt;PERSON&gt;</code>).</li> <li>Memorization Audits: Checking if the model can generate unique sequences from the training set.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#33-exact-and-near-duplicate-removal","title":"3.3 Exact and Near-Duplicate Removal","text":"<p>Duplicates distort the training distribution and waste compute.</p> <p>Techniques:</p> <ul> <li>Exact hashing: SHA-256 matching.</li> <li>MinHash / LSH: Locality-sensitive hashing for near-duplicates.</li> <li>Embedding-based similarity: For semantic duplicates.</li> </ul> <p>Why this matters:</p> <ul> <li>Inflated frequency biases</li> <li>Artificially low validation loss</li> <li>Memorization instead of abstraction</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#34-leakage-and-benchmark-contamination","title":"3.4 Leakage and Benchmark Contamination","text":"<p>Leakage sources:</p> <ul> <li>Public benchmark solutions in web data</li> <li>GitHub repositories with answers</li> <li>Fine-tuning data overlapping with evaluation sets</li> </ul> <p>Consequences:</p> <ul> <li>Inflated benchmark scores</li> <li>Misleading claims of reasoning ability</li> <li>Poor real-world generalization</li> </ul> <p>Mitigation requires proactive filtering (n-gram matching against test sets) and post-hoc auditing.</p>"},{"location":"training_techniques/pre_training/pre_training/#35-data-mixing-and-annealing","title":"3.5 Data Mixing and Annealing","text":"<p>How different data sources are combined determines the model's flavor.</p> <ul> <li>Static Mixing: Pre-assigned weights (e.g., 60% Web, 20% Code, 10% Math).</li> <li>Dynamic Selection (DoReMi): Continuously adjusts the sampling weights of different data sources during training using feedback from a smaller proxy model, prioritizing data that most improves validation loss and downweighting less useful or noisy sources.</li> <li>Annealing: A form of curriculum learning where high-quality data (synthetic, textbooks, math) is upsampled heavily in the final 5-10% of training to \"polish\" the model's skills.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#36-synthetic-data-generation-and-risks","title":"3.6 Synthetic Data Generation and Risks","text":"<p>Synthetic data is increasingly used to:</p> <ul> <li>Fill data gaps</li> <li>Emphasize rare skills</li> <li>Bootstrap reasoning behaviors</li> </ul> <p>However, risks include:</p> <ul> <li>Model Collapse: Reinforcement of model errors leading to distribution narrowing.</li> <li>Reduced diversity and creativity.</li> </ul> <p>Uncontrolled synthetic feedback loops can permanently damage model quality.</p>"},{"location":"training_techniques/pre_training/pre_training/#4-architecture-choices","title":"4. Architecture Choices","text":""},{"location":"training_techniques/pre_training/pre_training/#41-decoder-only-transformers","title":"4.1 Decoder-Only Transformers","text":"<p>Most LLMs use a decoder-only Transformer due to:</p> <ul> <li>Causal attention alignment with autoregressive training</li> <li>Simpler deployment and KV caching</li> <li>Strong scaling behavior</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#42-mixture-of-experts-moe","title":"4.2 Mixture of Experts (MoE)","text":"<p>MoE is the standard for scaling model capacity while keeping inference cheap.</p> <ul> <li>Concept: Feed-Forward Network (FFN) layers are split into multiple \"experts.\"</li> <li>Routing: A learnable gate selects only top-\\(k\\) experts (usually \\(k=2\\)) for each token.</li> <li>Sparse Activation: A model might have huge total parameters (e.g., 8x7B) but low active parameters per token.</li> <li>Trade-offs: High capacity and low latency, but training can be unstable (load balancing) and memory bandwidth heavy.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#43-modern-component-standards-the-no-vanilla-transformer","title":"4.3 Modern Component Standards (The \"No-Vanilla\" Transformer)","text":"<p>Standard Transformers (2017) are rarely used. Modern defaults include:</p> <ul> <li>RMSNorm: Replaces LayerNorm. It is computationally simpler (no mean centering) and numerically stable.</li> <li>Pre-Norm: Normalization is applied before attention/FFN layers (improves gradient flow) rather than after.</li> <li>SwiGLU: An activation function that replaces ReLU/GeLU. It adds a gating mechanism, increasing parameters slightly but improving convergence significantly.</li> <li>Bias-Free Layers: Removing bias terms from Linear layers to improve stability.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#44-tokenization","title":"4.4 Tokenization","text":"<p>Common approaches:</p> <ul> <li>BPE (Byte Pair Encoding)</li> <li>SentencePiece Unigram</li> </ul> <p>Modern Nuances:</p> <ul> <li>Byte-Fallback: Falls back to raw bytes for unknown characters to ensure no <code>&lt;UNK&gt;</code> tokens exist (crucial for code).</li> <li>Digit Splitting: Splitting numbers (e.g., \"2025\" \\(\\to\\) \"2\", \"0\", \"2\", \"5\") rather than grouping them improves arithmetic reasoning.</li> <li>Trade-offs: Larger vocabularies compress text better (faster inference) but increase the embedding layer size and training difficulty.</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#45-positional-embeddings","title":"4.5 Positional Embeddings","text":"<p>Modern models use Rotary Positional Embeddings (RoPE).</p> <p>Advantages:</p> <ul> <li>Implicit relative positioning</li> <li>Better extrapolation to longer contexts</li> <li>Compatible with attention optimizations (FlashAttention)</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#46-attention-variants","title":"4.6 Attention Variants","text":"<p>To reduce memory and compute:</p> <ul> <li>Multi-Query Attention (MQA): All heads share one KV head.</li> <li>Grouped-Query Attention (GQA): Compromise where groups of heads share a KV head (Standard in LLaMA).</li> </ul> <p>Benefits:</p> <ul> <li>Drastically lower KV cache memory usage</li> <li>Faster inference decoding</li> </ul>"},{"location":"training_techniques/pre_training/pre_training/#47-context-length-scaling","title":"4.7 Context Length Scaling","text":"<p>Increasing context length impacts:</p> <ul> <li>Memory quadratically (without FlashAttention/Ring Attention)</li> <li>Training stability (loss spikes)</li> </ul> <p>Common strategies:</p> <ul> <li>Long-context fine-tuning: Pre-train on short context (e.g., 4k), then anneal on long context (e.g., 128k).</li> <li>Ring Attention: For training on sequences longer than single-GPU memory.</li> </ul>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/","title":"Sentence Piece Unigram","text":"<p>The Unigram Language Model is a probabilistic, top-down subword tokenization algorithm. While often associated with the SentencePiece library (the framework for lossless, language-agnostic tokenization), Unigram itself is a distinct algorithm used by models like T5, XLNet, ALBERT, and many modern multilingual LLMs.</p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#1-the-probabilistic-framework","title":"1. The Probabilistic Framework","text":"<p>Unlike BPE, which is a greedy heuristic, Unigram treats tokenization as a statistical inference problem. It assumes that a sequence is generated by independently sampling tokens from a vocabulary.</p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#the-objective-function","title":"The Objective Function","text":"<p>Given a sentence \\(S\\) and a candidate segmentation \\(\\mathbf{x} = \\{t_1, t_2, \\dots, t_k\\}\\), the model calculates the probability of that segmentation as:</p> \\[P(\\mathbf{x}) = \\prod_{i=1}^{k} P(t_i)\\] <p>The goal of the tokenizer is to find the segmentation that maximizes this likelihood:</p> \\[\\mathbf{x}^* = \\arg\\max_{\\mathbf{x} \\in \\mathcal{S}} \\sum_{i=1}^{k} \\log P(t_i)\\] <p>Where \\(\\mathcal{S}\\) is the set of all possible segmentations. This is efficiently solved using Viterbi decoding (Dynamic Programming) in \\(O(N^2)\\) time relative to string length.</p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#2-training-the-em-style-pruning-process","title":"2. Training: The EM-Style Pruning Process","text":"<p>Unigram training is a \"top-down\" approach. Instead of merging small units (like BPE), it starts with a massive vocabulary and prunes it down.</p> <ol> <li>Initialization: Start with a very large vocabulary (e.g., every character in the corpus plus the most frequent substrings).</li> <li>Expectation (E-step): Given the current vocabulary and token probabilities, find the optimal (Viterbi) segmentation for every sentence in the corpus.</li> <li>Maximization (M-step): Update the token probabilities \\(P(t_i)\\) based on their frequency in the newly computed optimal segmentations.</li> <li>Pruning:<ul> <li>For each token, calculate the loss: how much the total corpus likelihood would decrease if that token were removed from the vocabulary.</li> <li>Discard the bottom \\(X\\%\\) of tokens with the lowest loss.</li> <li>Note: To ensure every string remains segmentable, single characters are never pruned.</li> </ul> </li> <li>Iteration: Repeat until the target vocabulary size is reached.</li> </ol>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#3-small-intuitive-example","title":"3. Small Intuitive Example","text":"<p>Assume the vocabulary contains the following tokens with probabilities:</p> Token Probability <code>low</code> 0.30 <code>lower</code> 0.25 <code>er</code> 0.20 <code>l</code> 0.05 <code>o</code> 0.05 <code>w</code> 0.05 <p>Input word: <code>lower</code></p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#possible-segmentations","title":"Possible Segmentations","text":"<ol> <li> <p><code>lower</code> \\(P = 0.25\\)</p> </li> <li> <p><code>low + er</code> \\(P = 0.30 \\times 0.20 = 0.06\\)</p> </li> <li> <p><code>l + o + w + er</code> \\(P = 0.05^3 \\times 0.20 = 0.00025\\)</p> </li> </ol>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#selected-tokenization","title":"Selected Tokenization","text":"<p>The tokenizer selects <code>lower</code> because it maximizes the likelihood.</p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#4-the-sentencepiece-secret-sauce","title":"4. The SentencePiece \"Secret Sauce\"","text":"<p>SentencePiece is the implementation wrapper that adds critical features for modern LLMs:</p> <ul> <li>Lossless Reversibility: It replaces spaces with a meta-symbol (usually <code>_</code>). Because the space is treated as a standard character, the original string can be reconstructed perfectly (detokenized) without complex rules.</li> <li>Byte-Fallback: If the model encounters a character not in its vocabulary, it falls back to UTF-8 bytes. This effectively eliminates the \"Unknown Token\" (<code>UNK</code>) problem.</li> <li>Pre-tokenization Independence: It does not require splitting text by spaces first. This makes it natively compatible with non-segmented languages like Chinese, Japanese, or Thai.</li> </ul>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#5-subword-regularization-sampling","title":"5. Subword Regularization (Sampling)","text":"<p>This is a major advantage for model robustness. During training, instead of always using the \"best\" (Viterbi) segmentation, we sample different segmentations based on their probabilities:</p> \\[P(\\mathbf{x}) = \\frac{P(\\mathbf{x})^\\alpha}{\\sum_{\\mathbf{x}' \\in \\mathcal{S}} P(\\mathbf{x}')^\\alpha}\\] <p>By exposing the model to multiple ways of segmenting the same word (e.g., <code>higher</code> as <code>high + er</code> vs <code>h + igher</code>), the model becomes more robust to spelling variations and noise.</p>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#6-comparison-bpe-vs-unigram","title":"6. Comparison: BPE vs. Unigram","text":"Feature BPE (Byte-Pair Encoding) Unigram (SentencePiece) Logic Bottom-up (Iterative Merging) Top-down (Iterative Pruning) Philosophy Frequency-based heuristic Probabilistic Likelihood Optimality Greedy (not globally optimal) Global Optimum (via Viterbi) Regularization Difficult to implement Natively supports subword sampling Use Case GPT-family, Llama, RoBERTa T5, ALBERT, Multilingual models"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#6-practical-limitations","title":"6. Practical Limitations","text":"<ul> <li>Complexity: Viterbi decoding is more computationally expensive than the greedy merges of BPE.</li> <li>Independence Assumption: It assumes tokens are independent, ignoring the linguistic context in which a subword appears.</li> <li>Training Time: The EM pruning process on a massive corpus is generally slower than BPE's initial merge-counting.</li> </ul>"},{"location":"training_techniques/pre_training/sentence_piece_unigram/#7-summary","title":"7. Summary","text":"<p>SentencePiece Unigram provides a principled, probabilistic objective for tokenization. Its ability to provide globally optimal segmentations, support subword regularization, and maintain lossless reversibility makes it a preferred choice for high-performance, multilingual Large Language Models.</p>"}]}