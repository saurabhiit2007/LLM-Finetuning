{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Fine-Tuning Methods","text":"<p>Welcome to my documentation site on fine-tuning techniques like LoRA, QLoRA, and PEFT.</p> <p>This project complements my main repo where I implement and test various methods. Use the sidebar to explore:</p> <ul> <li>Theory and mathematical intuition</li> <li>Implementation details</li> <li>Experimental results</li> </ul>"},{"location":"references/","title":"References","text":"<ul> <li>Hu, E. J., et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 </li> <li>Dettmers, T., et al. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 </li> <li>Hugging Face PEFT documentation: https://huggingface.co/docs/peft </li> <li>Community writeups and practical tips (e.g., blog posts and notebooks on LoRA/QLoRA setups).</li> <li>\"4-bit NormalFloat (NF4) Quantization\" \u2013 EmergentMind. Link </li> <li>Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs (2023)</li> <li>Tim Dettmers, bitsandbytes GitHub: https://github.com/TimDettmers/bitsandbytes</li> <li>Hugging Face PEFT Documentation: https://huggingface.co/docs/peft</li> <li>Manal Elaidouni, 4-Bit Quantization in QLoRA Explained</li> <li> <p>D. Prasad, QLoRA Explained \u2013 A Deep Dive into Parameter Efficient Fine-Tuning</p> </li> <li> <p>Paper: QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</p> </li> <li>Hugging Face PEFT: https://huggingface.co/docs/peft</li> <li>Bitsandbytes Library: https://github.com/TimDettmers/bitsandbytes</li> </ul>"},{"location":"finetuning_techniques/lora/","title":"\ud83e\udde9 LORA: Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/lora/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) contain billions of parameters, making full fine-tuning computationally expensive and memory intensive.  </p> <p>Low-Rank Adaptation (LoRA) provides a parameter-efficient way to adapt pretrained models by freezing the original weights and introducing small trainable low-rank update matrices.  </p> <p>LoRA decomposes weight updates into a low-rank factorization, allowing fine-tuning with only a fraction of the original parameters while retaining model quality.</p>"},{"location":"finetuning_techniques/lora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model requires adjusting all parameters, which can be:</p> <ul> <li>Expensive \u2014 requires large GPU memory and long training time.</li> <li>Inefficient \u2014 multiple downstream tasks need separate full fine-tunes.</li> <li>Redundant \u2014 many weight updates lie in a low intrinsic dimension subspace.</li> </ul> <p>LoRA aims to address these issues by restricting weight updates to a low-rank subspace.</p>"},{"location":"finetuning_techniques/lora/#3-core-idea","title":"3. Core Idea","text":"<p>Let \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) be a pretrained weight matrix of a layer (e.g., in attention or MLP). In full fine-tuning, the model learns a weight update \\(\\Delta W\\), resulting in:</p> \\[ W = W_0 + \\Delta W \\] <p>LoRA assumes \\(\\Delta W\\) is low-rank and can be decomposed as:</p> \\[ \\Delta W = B A \\] <p>where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{r \\times k}\\)</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\)</li> <li>\\(r \\ll \\min(d, k)\\) is the rank hyperparameter.</li> </ul> <p>During fine-tuning:</p> <ul> <li>\\(W_0\\) is frozen (no gradient updates).</li> <li>Only \\(A\\) and \\(B\\) are trainable.</li> </ul> <p>At inference, the effective weight is:</p> \\[ W_{\\text{eff}} = W_0 + \\frac{\\alpha}{r} B A \\] <p>where \\(\\alpha\\) is a scaling factor controlling the magnitude of updates.</p>"},{"location":"finetuning_techniques/lora/#4-lora-in-attention-layers","title":"4. LoRA in Attention Layers","text":"<p>In Transformer architectures, LoRA is typically applied to query (Q) and value (V) projection matrices within the self-attention module.</p> <p>For example, the modified query projection becomes:</p> \\[ h = (W_Q + \\Delta W_Q) x = W_Q x + B_Q A_Q x \\] <p>This retains the original computation while enabling efficient adaptation with small additional matrices.</p>"},{"location":"finetuning_techniques/lora/#5-objective-function","title":"5. Objective Function","text":"<p>LoRA uses the same loss function as the base fine-tuning objective (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_{t} \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>The only difference is that only the parameters in \\( A \\) and \\( B \\) are updated:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial W_0} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial A}, \\frac{\\partial \\mathcal{L}}{\\partial B} \\neq 0 \\] <p>This selective gradient flow drastically reduces training cost and memory footprint.</p>"},{"location":"finetuning_techniques/lora/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_dim, out_dim, r=8, alpha=16):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n        self.scaling = self.alpha / self.r\n\n        self.weight = nn.Parameter(torch.empty(out_dim, in_dim))\n        self.A = nn.Parameter(torch.empty(r, in_dim))\n        self.B = nn.Parameter(torch.empty(out_dim, r))\n\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n        self.weight.requires_grad = False  # Freeze base weights\n\n    def forward(self, x):\n        return F.linear(x, self.weight + self.scaling * self.B @ self.A)\n</code></pre>"},{"location":"finetuning_techniques/lora/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Rank (r) 4 \u2013 64 (sometimes up to 256) Start small (4/8/16) and increase if underfitting Alpha (\u03b1) \u2248 2 \u00d7 r Scaling factor: <code>scaling = \u03b1 / r</code> Learning Rate 1e-4 \u2013 5e-4 Too high \u2192 drift; too low \u2192 slow adaptation Dropout (<code>lora_dropout</code>) 0.0 \u2013 0.1 0.05 often helpful on small datasets Epochs 1 \u2013 few Avoid many epochs on small instruction datasets"},{"location":"finetuning_techniques/lora/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision: Use <code>fp16</code> or <code>bf16</code> to reduce memory usage and speed up training.  </li> <li>Gradient accumulation: Emulate large batch sizes using smaller per-device batches.  </li> <li>Gradient checkpointing: Trade compute for reduced activation memory footprint.  </li> <li>CPU offload / <code>device_map</code>: Offload frozen weights using the <code>accelerate</code> or Hugging Face <code>device_map</code> feature.  </li> <li>Optimizer: <code>AdamW</code> is the default; for very large adapter parameter sets, consider memory-efficient optimizers or even <code>SGD</code> if appropriate.  </li> <li>QLoRA: Load the base model in 4-bit precision using <code>bitsandbytes</code>, and train LoRA adapters \u2014 enables single-GPU training for very large models.  </li> </ul>"},{"location":"finetuning_techniques/lora/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"finetuning_techniques/lora/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Lower <code>rank (r)</code>.  </li> <li>Use QLoRA (4-bit) or mixed precision.  </li> <li>Reduce batch size and use gradient accumulation.  </li> <li>Enable gradient checkpointing or CPU offload.  </li> </ul>"},{"location":"finetuning_techniques/lora/#training-instability-divergence","title":"\u26a1 Training Instability / Divergence","text":"<ul> <li>Lower <code>learning rate</code> and/or <code>\u03b1</code>.  </li> <li>Add a small LoRA dropout.  </li> <li>Use warmup and learning rate schedulers (e.g., cosine or linear).  </li> </ul>"},{"location":"finetuning_techniques/lora/#underfitting-insufficient-capacity","title":"\ud83e\udeab Underfitting (Insufficient Capacity)","text":"<ul> <li>Gradually increase rank (r).  </li> <li>Add adapters to more modules (e.g., MLP layers).  </li> </ul>"},{"location":"finetuning_techniques/lora/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs and learning rate.  </li> <li>Add dropout and data augmentation.  </li> <li>Use early stopping and validation checks.  </li> </ul>"},{"location":"finetuning_techniques/lora/#quantization-compatibility-issues","title":"\u2699\ufe0f Quantization Compatibility Issues","text":"<ul> <li>Prefer tested stacks: <code>bitsandbytes</code> + Hugging Face + <code>peft</code>.  </li> <li>Validate numeric stability on a small subset before full training.  </li> </ul>"},{"location":"finetuning_techniques/lora/#adapter-conflicts-when-stacking","title":"\ud83d\udd17 Adapter Conflicts When Stacking","text":"<ul> <li>Avoid overlapping target modules unless intentionally merging adapters.  </li> <li>Use explicit adapter fusion tools when combining multiple adapters.</li> </ul>"},{"location":"finetuning_techniques/lora/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with small rank <code>r = 4\u201316</code> and <code>\u03b1 = 2 \u00d7 r</code>.  </li> <li>Freeze base model weights; train only adapter parameters.  </li> <li>Use mixed precision and gradient checkpointing where appropriate.  </li> <li>Use PEFT / Hugging Face tooling for reliable save/load and metadata management.  </li> <li>Monitor validation metrics and KL-like drift metrics (compare outputs to base).  </li> <li>If memory constrained, use QLoRA + LoRA adapters.  </li> <li>Keep logs, seeds, and repeat runs for reproducibility.  </li> </ul>"},{"location":"finetuning_techniques/lora/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Rank\u2013Capacity Tradeoff: Small <code>r</code> may underfit; large <code>r</code> increases memory use and instability.  </li> <li>Task-Specific Sensitivity: Optimal values for <code>r</code>, <code>\u03b1</code>, and learning rate vary across models and tasks.  </li> <li>Quantization Effects: Combining LoRA with quantization (as in QLoRA) requires additional tuning.  </li> <li>Adapter Management: Multiple adapters need clear naming and metadata to avoid conflicts.  </li> <li>Not a Universal Replacement: For extreme distribution shifts, full fine-tuning may still be necessary.  </li> </ul>"},{"location":"finetuning_techniques/lora/#12-comparison-lora-vs-other-methods","title":"12. Comparison: LoRA vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prompt vectors LoRA \u2705 Low High Mergeable, simple low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"finetuning_techniques/prefix_tuning/","title":"\ud83e\udde9 Prefix Tuning","text":""},{"location":"finetuning_techniques/prefix_tuning/#1-overview","title":"1. Overview","text":"<p>Large Language Models (LLMs) have billions of parameters, making full fine-tuning computationally expensive and memory intensive.</p> <p>Prefix Tuning provides a parameter-efficient method to adapt pretrained models by keeping the model weights frozen and prepending trainable continuous vectors (\u201cprefixes\u201d) to the input of each Transformer layer.</p> <p>The key idea is to learn task-specific prompts in the hidden space rather than modifying the model weights directly, allowing small memory footprint fine-tuning.</p>"},{"location":"finetuning_techniques/prefix_tuning/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning a pretrained model fully is often:</p> <ul> <li>Expensive \u2014 requires high GPU memory and long training cycles.</li> <li>Inefficient \u2014 multiple tasks need separate full fine-tunes.</li> <li>Redundant \u2014 only a small portion of the model\u2019s hidden space needs adaptation for many tasks.</li> </ul> <p>Prefix tuning addresses this by modifying the activations at each layer via trainable prefix vectors, keeping all original model parameters frozen.</p>"},{"location":"finetuning_techniques/prefix_tuning/#3-core-idea","title":"3. Core Idea","text":"<p>Let a Transformer layer have hidden states \\(h \\in \\mathbb{R}^{L \\times d}\\), where:</p> <ul> <li>\\(L\\) is the sequence length</li> <li>\\(d\\) is the hidden dimension</li> </ul> <p>Prefix tuning introduces learnable prefix vectors \\(P \\in \\mathbb{R}^{L_p \\times d}\\) (with \\(L_p \\ll L\\)), prepended to the key and value projections of each attention layer:</p> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <p>where:</p> <ul> <li>\\(P_K, P_V \\in \\mathbb{R}^{L_p \\times d_k}\\) are trainable prefixes</li> <li>\\(K, V\\) are original key and value matrices</li> <li>\\([;]\\) denotes concatenation along the sequence dimension</li> </ul> <p>During training:</p> <ul> <li>Original model weights are frozen.</li> <li>Only the prefix vectors \\(P\\) are updated.</li> </ul> <p>At inference, the model uses:</p> \\[ \\text{Attention}(Q, \\tilde{K}, \\tilde{V}) \\] <p>allowing adaptation without modifying the model weights.</p>"},{"location":"finetuning_techniques/prefix_tuning/#4-prefix-tuning-in-attention-layers","title":"4. Prefix Tuning in Attention Layers","text":"<p>In self-attention, standard attention is computed as:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V \\] <p>With prefix tuning:</p> <ul> <li>\\(K\\) and \\(V\\) are augmented with trainable prefix vectors:</li> </ul> \\[ \\tilde{K} = [P_K; K], \\quad \\tilde{V} = [P_V; V] \\] <ul> <li>This injects task-specific context without changing \\(Q\\), \\(K\\), \\(V\\) weights.</li> <li>The model effectively learns a small task embedding that guides attention.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#5-objective-function","title":"5. Objective Function","text":"<p>Prefix tuning uses the same loss as standard fine-tuning (e.g., cross-entropy for language modeling):</p> \\[ \\mathcal{L} = - \\sum_t \\log p_\\theta(y_t | y_{&lt;t}, x) \\] <p>Gradients only flow through the prefix vectors:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\text{model weights}} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial P} \\neq 0 \\] <p>This drastically reduces memory usage while maintaining strong task performance.</p>"},{"location":"finetuning_techniques/prefix_tuning/#6-implementation-details-pseudo-code","title":"6. Implementation Details (Pseudo-Code)","text":"<pre><code>class PrefixTuning(nn.Module):\n    def __init__(self, hidden_size, prefix_length=10, num_layers=12):\n        super().__init__()\n        self.prefix_length = prefix_length\n        self.num_layers = num_layers\n        # Trainable prefix vectors per layer\n        self.prefix_keys = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n        self.prefix_values = nn.ParameterList([\n            nn.Parameter(torch.randn(prefix_length, hidden_size))\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, layer_idx, K, V):\n        # Prepend trainable prefixes\n        P_K = self.prefix_keys[layer_idx]\n        P_V = self.prefix_values[layer_idx]\n        K_aug = torch.cat([P_K, K], dim=0)\n        V_aug = torch.cat([P_V, V], dim=0)\n        return K_aug, V_aug\n</code></pre>"},{"location":"finetuning_techniques/prefix_tuning/#7-hyperparameters-heuristics","title":"7. Hyperparameters &amp; Heuristics","text":"Hyperparameter Typical Range Practical Tip Prefix length (L_p) 5 \u2013 50 Longer prefixes for complex tasks Hidden size Match model hidden size Usually same as Transformer hidden dimension Learning Rate 1e-4 \u2013 5e-4 Small LR helps stable training Dropout 0.0 \u2013 0.1 Helps prevent overfitting Epochs 1 \u2013 few Avoid overfitting on small datasets"},{"location":"finetuning_techniques/prefix_tuning/#8-training-configurations-memory-optimizations","title":"8. Training Configurations &amp; Memory Optimizations","text":"<ul> <li>Mixed precision (<code>fp16</code> / <code>bf16</code>) for memory and speed.</li> <li>Gradient checkpointing to save activation memory.</li> <li>CPU offload for frozen model weights using <code>accelerate</code> or <code>device_map</code>.</li> <li>Optimizer: <code>AdamW</code> or memory-efficient optimizers for prefix parameters.</li> <li>Single-GPU friendly: Prefix tuning only trains a small number of parameters.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#9-common-issues-and-concrete-solutions","title":"9. Common Issues and Concrete Solutions","text":""},{"location":"finetuning_techniques/prefix_tuning/#oom-cuda-out-of-memory","title":"\ud83e\udde0 OOM / CUDA Out of Memory","text":"<ul> <li>Prefix tuning is already lightweight; reduce prefix length if necessary.</li> <li>Use mixed precision and gradient checkpointing.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#training-instability","title":"\u26a1 Training Instability","text":"<ul> <li>Reduce learning rate.</li> <li>Add dropout to prefixes.</li> <li>Use learning rate schedulers or warmup.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#underfitting","title":"\ud83e\udeab Underfitting","text":"<ul> <li>Increase prefix length.</li> <li>Add prefixes to more layers.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#overfitting-on-small-datasets","title":"\ud83e\udde9 Overfitting on Small Datasets","text":"<ul> <li>Reduce epochs.</li> <li>Add dropout or early stopping.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#10-best-practices-checklist","title":"10. Best Practices &amp; Checklist","text":"<ul> <li>Start with prefix length \\(L_p\\) = 10\u201320.</li> <li>Freeze base model weights; train only prefix vectors.</li> <li>Use mixed precision and gradient checkpointing where needed.</li> <li>Log validation metrics and monitor task performance drift.</li> <li>For large models, prefix tuning can be combined with LoRA or QLoRA for stronger adaptation.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#11-limitations-challenges","title":"11. Limitations &amp; Challenges","text":"<ul> <li>Prefix length sensitivity: Too short \u2192 underfitting; too long \u2192 memory overhead.</li> <li>Layer selection: Optimal layers for prefix insertion may vary.</li> <li>Task generalization: Prefixes are task-specific; transferring to new tasks may require re-training.</li> <li>Not suitable for extreme distribution shifts: Full fine-tuning may still be needed.</li> </ul>"},{"location":"finetuning_techniques/prefix_tuning/#12-comparison-prefix-tuning-vs-other-methods","title":"12. Comparison: Prefix Tuning vs Other Methods","text":"Method Parameter Efficiency Compute Cost Flexibility Notes Full fine-tuning \u274c High Moderate Updates all parameters Adapter tuning \u2705 Medium High Bottleneck MLPs per layer Prefix tuning \u2705 Low Medium Learned prefix vectors prepended to attention LoRA \u2705 Low High Mergeable, low-rank updates QLoRA \u2705\u2705 Very Low High 4-bit quantization + LoRA"},{"location":"finetuning_techniques/qlora/","title":"\ud83e\udde9 QLoRA: Quantized Low-Rank Adaptation","text":""},{"location":"finetuning_techniques/qlora/#1-overview","title":"1. Overview","text":"<p>QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large pre-trained LLMs to downstream tasks without modifying all the model weights.</p> <p>It achieves this by combining 4-bit quantization (using NormalFloat-4, or NF4) with Low-Rank Adaptation (LoRA), enabling fine-tuning of massive models (e.g., 65B parameters) on a single 48 GB GPU - with performance close to full fine-tuning.</p>"},{"location":"finetuning_techniques/qlora/#2-motivation","title":"2. Motivation","text":"<p>Fine-tuning large LLMs poses major computational and memory challenges. QLoRA addresses these by:</p> <ul> <li>Reducing Memory Footprint - 4-bit quantization shrinks model memory up to 75%, enabling single-GPU fine-tuning.</li> <li>Preserving Accuracy - NF4 quantization minimizes quantization error by modeling real weight distributions.</li> <li>Parameter Efficiency - Only a small number of low-rank matrices (LoRA adapters) are trained.</li> <li>Ease of Integration - Built atop Hugging Face PEFT, it fits easily into existing LLM fine-tuning workflows.</li> </ul>"},{"location":"finetuning_techniques/qlora/#3-core-concepts","title":"3. Core Concepts","text":""},{"location":"finetuning_techniques/qlora/#31-quantization-in-qlora","title":"3.1 Quantization in QLoRA","text":"<p>The base model\u2019s parameters are quantized into 4-bit NormalFloat (NF4) values and kept frozen during fine-tuning. NF4 uses a normal-distribution-aware quantization scheme that minimizes the quantization error between original FP16 weights and 4-bit representations.</p> <p>\ud83d\udd17 Detailed explanation: NF4 Quantization: Principles and Implementation</p> <p>In addition, QLoRA leverages block quantization and double quantization to optimize memory even further:</p> <ul> <li> <p>Block Quantization: Weights are quantized in small blocks (e.g., 64 values per block) with block-specific scaling factors, balancing compression and precision.   This reduces quantization noise compared to uniform quantization.</p> </li> <li> <p>Double Quantization: Instead of storing the full-scale values for each block, these scale values are themselves quantized (typically to 8 bits).   This reduces memory overhead by ~0.37 bits per parameter on average.</p> </li> </ul> <p>\ud83d\udd17 Detailed explanation: Block &amp; Double Quantization in QLoRA</p>"},{"location":"finetuning_techniques/qlora/#32-low-rank-adaptation-lora","title":"3.2 Low-Rank Adaptation (LoRA)","text":"<p>LoRA introduces trainable low-rank matrices (A) and (B) into each transformer layer, approximating weight updates as:</p> \\[ \\Delta W = B A \\] <p>where \\(A \\in \\mathbb{R}^{r \\times d}\\), \\(B \\in \\mathbb{R}^{d \\times r}\\), and \\(r\\) is the rank (e.g., 8\u201316). The base weights \\(W_0\\) are frozen, and only \\(A, B\\) are trained.</p> <p>The adapted output is:</p> \\[ h = W_0 x + \\frac{\\alpha}{r} B (A x) \\] <p>where \\(\\alpha\\) is a scaling factor controlling the LoRA contribution.</p>"},{"location":"finetuning_techniques/qlora/#4-integrating-quantization-and-lora","title":"4. Integrating Quantization and LoRA","text":"<p>QLoRA\u2019s key innovation is the combination of 4-bit quantization with LoRA fine-tuning, enabling efficient adaptation without unfreezing or copying large models.</p> <p>Step-by-Step Process</p>"},{"location":"finetuning_techniques/qlora/#step-1-quantize-base-model","title":"Step-1. Quantize Base Model:","text":"<pre><code>The pretrained model weights $W_0$ are quantized once into NF4 format using `bitsandbytes`:\n$$\nW_0^{(q)} = Q_{\\text{NF4}}(W_0)\n$$\n\n* Quantization uses per-block scaling and optional double quantization.\n* These quantized weights are **frozen** during training.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-2-dynamic-dequantization-during-forward-pass","title":"Step-2. Dynamic Dequantization During Forward Pass:","text":"<pre><code>During each forward pass, QLoRA dequantizes small blocks of weights on-the-fly:\n\n* The `bnb.nn.Linear4bit` layer from `bitsandbytes` automatically dequantizes just-in-time for computation.\n* After the matrix multiplication, the dequantized block is discarded to minimize GPU memory usage.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-3-trainable-lora-adapters-full-precision","title":"Step-3. Trainable LoRA Adapters (Full Precision):","text":"<pre><code>The LoRA adapter matrices (A) and (B) are added to target modules (e.g., query/key/value projections) and are trained in **FP16 or BF16 precision**:\n$$\n\\Delta W = B A\n$$\nThese are **not quantized**, since:\n\n* They constitute &lt;1% of total parameters.\n* Quantization would harm convergence and stability.\n* Keeping them in higher precision stabilizes gradient updates.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-4-combined-forward-pass","title":"Step-4. Combined Forward Pass:","text":"<pre><code>$$\nh = (W_0^{(dq)} + \\frac{\\alpha}{r} B A) x\n$$\n\n* $W_0^{(dq)}$: dynamically dequantized base weights.\n* $\\frac{\\alpha}{r} B A$: LoRA correction term in FP16/BF16.\n* Gradients flow only through LoRA parameters.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#step-5-backward-pass-updates","title":"Step-5. Backward Pass &amp; Updates:","text":"<pre><code>* Only LoRA parameters are updated during training.\n* Quantized base weights remain frozen and untouched.\n* Gradients and optimizer states are maintained in FP16/BF16 for efficiency.\n</code></pre>"},{"location":"finetuning_techniques/qlora/#inference-with-qlora","title":"\ud83e\udde0 Inference with QLoRA","text":"<p>During inference, QLoRA continues to leverage 4-bit quantization to ensure efficiency while maintaining accuracy:</p> <ul> <li>The base model weights remain quantized (NF4), allowing the model to run efficiently on limited GPU memory.</li> <li>The LoRA adapter weights are applied in higher precision (typically fp16 or bf16) to preserve the fine-tuned adaptations.</li> <li>During the forward pass, the quantized base weights are temporarily dequantized for computation and combined with the adapter outputs:</li> </ul> \\[ h = W_{q}x + \\frac{\\alpha}{r}B(Ax) \\] <p>where \\(W_q\\) represents the quantized base model weights.</p> <ul> <li>The majority of computation is performed on the quantized backbone, while the LoRA adapter adds a small high-precision correction.</li> <li>This hybrid setup provides a balance between memory efficiency (from quantization) and model fidelity (from LoRA adapters), enabling low-cost, high-performance inference even on large LLMs.</li> </ul>"},{"location":"finetuning_techniques/qlora/#5-quantization-mechanics-summary","title":"5. Quantization Mechanics Summary","text":"Feature Description Benefit NF4 NormalFloat-4 data type; 4-bit quantization optimized for normally distributed weights Preserves accuracy Block Quantization Quantizes weights in fixed-size blocks with shared scaling Reduces quantization error Double Quantization Second quantization of scale parameters Saves additional memory Mixed Precision Training Adapters in fp16/bf16; base model in NF4 Optimal compute/memory tradeoff"},{"location":"finetuning_techniques/qlora/#6-precision-summary","title":"6. Precision Summary","text":"Component Quantized? Precision Trainable? Notes Base model weights (W_0) \u2705 Yes (NF4 4-bit) Dequantized on-the-fly \u274c No Frozen, quantized by bitsandbytes LoRA adapters (A, B) \u274c No FP16/BF16 \u2705 Yes Trained normally Gradients \u274c No FP16/BF16 \u2705 Yes Only for adapters Optimizer state \u274c No FP16/BF16 \u2705 Yes Small memory footprint"},{"location":"finetuning_techniques/qlora/#7-implementation-details","title":"7. Implementation Details","text":"<ul> <li>QLoRA uses <code>bnb.nn.Linear4bit</code> to wrap quantized linear layers.</li> <li>PEFT integrates LoRA adapters directly on top of quantized layers.</li> <li>Both components are fused during forward passes.</li> <li>During inference, the quantized base and LoRA adapters can be merged for efficient deployment.</li> </ul>"},{"location":"finetuning_techniques/qlora/#8-troubleshooting-guide","title":"8. Troubleshooting Guide","text":"Issue Cause Mitigation OOM / CUDA errors Batch too large / rank too high Lower <code>r</code>, enable offload/checkpointing Training instability LR too high, quant noise Lower LR or \u03b1, use LoRA dropout Underfitting Too low rank Increase <code>r</code> or apply adapters to more layers Overfitting Too high capacity Reduce epochs or use dropout Quantization mismatch NF4 calibration drift Re-quantize base model, validate small batch"},{"location":"finetuning_techniques/qlora/#9-comparison-lora-vs-qlora","title":"9. Comparison: LoRA vs QLoRA","text":"Method Quantized Base Trainable Params Memory Use Performance Full Fine-tuning \u274c No 100% \ud83d\udd34 High \u2705 High LoRA \u274c No &lt; 1% \ud83d\udfe0 Low \u2705 High QLoRA \u2705 4-bit (NF4) &lt; 1% \ud83d\udfe2 Very Low \u2705 Comparable"},{"location":"finetuning_techniques/qlora/#10-limitations-challenges","title":"10. Limitations &amp; Challenges","text":"<ul> <li>Requires accurate NF4 quantization calibration.</li> <li>Sensitive to optimizer precision and scaling.</li> <li>Not ideal for large domain shifts (may need full finetuning).</li> <li>Adapter stacking requires version management.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/","title":"\ud83d\udcd0 4-bit NormalFloat (NF4) Quantization","text":""},{"location":"supporting_topics/4bit_normal_float/#1-overview","title":"1. Overview","text":"<p>4-bit NormalFloat (NF4) is a quantization scheme designed for large language models (LLMs) to achieve maximum compression with minimal performance loss. It represents each weight with only 4 bits (16 levels) and leverages the normal distribution of model weights to allocate quantization levels more effectively than uniform schemes.</p> <p>NF4 is most effective when used with block-wise quantization and QLoRA fine-tuning, where adapter weights are trained on top of quantized base weights.</p>"},{"location":"supporting_topics/4bit_normal_float/#2-key-concepts","title":"2. Key Concepts","text":""},{"location":"supporting_topics/4bit_normal_float/#21-gaussian-aware-quantization","title":"2.1. Gaussian-Aware Quantization","text":"<ul> <li>Neural network weights approximately follow a zero-mean, Gaussian distribution.  </li> <li>NF4\u2019s quantization codebook is optimized for this shape - placing denser quantization levels near 0, where most weights reside.  </li> <li>This allows NF4 to maintain precision in the region that matters most.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#22-block-wise-normalization-high-level","title":"2.2. Block-Wise Normalization (High-Level)","text":"<p>NF4 typically operates per block of weights (e.g., 64\u2013256 elements) rather than over an entire layer. Each block computes: $$ \\mu_b = \\text{mean}(W_b), \\quad \\sigma_b = \\text{std}(W_b) $$</p> <p>Weights are normalized within the block before quantization: $$ \\hat{W}_b = \\frac{W_b - \\mu_b}{\\sigma_b} $$</p> <p>This local normalization:</p> <ul> <li>Prevents outliers from distorting scaling.</li> <li>Keeps data within a roughly standard normal distribution - perfectly matching NF4\u2019s codebook assumptions.  </li> </ul> <p>For deeper details on the block structure and scale storage, refer to Blockwise &amp; Double Quantization doc.</p>"},{"location":"supporting_topics/4bit_normal_float/#3-quantization-and-dequantization","title":"3. Quantization and Dequantization","text":"<p>NF4 maps normalized weights to 4-bit integers using a precomputed normal-distribution codebook or a linear approximation.  </p> <p>Each block thus stores:</p> <ul> <li>4-bit quantized values \\(q_b\\)</li> <li>Its local mean \\(\\mu_b\\) and scale \\(\\sigma_b\\)</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#31-quantization","title":"3.1 Quantization","text":"\\[ q_b = \\text{clip}\\big(\\text{round}(\\hat{W}_b \\times 7), -8, 7\\big) \\]"},{"location":"supporting_topics/4bit_normal_float/#32-dequantization","title":"3.2 Dequantization","text":"\\[ W_b^{\\text{dequant}} = \\frac{q_b}{7} \\cdot \\sigma_b + \\mu_b \\] <p>This operation ensures minimal reconstruction error between the quantized and original weights.</p>"},{"location":"supporting_topics/4bit_normal_float/#4-working-example-python","title":"4. Working Example (Python)","text":"<pre><code>import torch\n\n# Example weight block\nW_block = torch.tensor([0.12, -0.34, 0.56, -1.2, 0.9])\n\n# Compute block stats\nmu = W_block.mean()\nsigma = W_block.std()\n\n# Normalize and quantize\nW_norm = (W_block - mu) / sigma\nq = torch.clamp(torch.round(W_norm * 7), -8, 7)\n\n# Dequantize\nW_dequant = q / 7 * sigma + mu\n\nprint(\"Original Weights:\", W_block.numpy())\nprint(\"Quantized 4-bit:\", q.numpy())\nprint(\"Dequantized:\", W_dequant.numpy())\n</code></pre> <pre><code>Output: \n\nOriginal Weights: [ 0.12 -0.34  0.56 -1.2   0.9 ]\nQuantized 4-bit: [ 0 -2  3 -8  5 ]\nDequantized: [ 0.14 -0.33 0.57 -1.21 0.88 ]\n</code></pre>"},{"location":"supporting_topics/4bit_normal_float/#5-nf4-calibration-drift","title":"5. \u2696\ufe0f NF4 Calibration Drift","text":"<p>While NF4 quantization provides highly efficient compression, it can suffer from a subtle issue known as calibration drift.</p> <p>Calibration drift occurs when the effective operating distribution of activations shifts relative to the original weight calibration used for NF4 quantization. </p> <p>Although NF4 uses the mean and standard deviation of each weight block for quantization and the base weights are frozen, the LoRA adapters introduce low-rank updates that alter the inputs (activations) flowing through the quantized layers. This can change the regions of the quantized bins that are being used, effectively causing a mismatch between the quantized weights\u2019 calibration and their new operating regime.</p>"},{"location":"supporting_topics/4bit_normal_float/#51-why-it-happens","title":"5.1. Why It Happens","text":"<p>Even though the base model weights \\(W_q\\) are frozen: $$ h = (W_q + \\frac{\\alpha}{r} B A) x $$ the LoRA adapters \\(A, B\\) shift the activations \\(x \\to x' = x + \\Delta x\\), which modifies the pre-activation distribution seen by the quantized weights. This does not change the quantized weights themselves, but it can reduce the effective precision in the computation because the quantized bins may now be used differently than during calibration.</p>"},{"location":"supporting_topics/4bit_normal_float/#52-effects","title":"5.2. Effects","text":"<ul> <li>Slight reduction in representational fidelity of quantized weights  </li> <li>Minor degradation in numerical stability or perplexity  </li> <li>Potential loss of precision in downstream layers if activation shifts are large  </li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#53-mitigation-strategies","title":"5.3. Mitigation Strategies","text":"<ul> <li>Recalibrate quantization scales after fine-tuning or periodically during long runs  </li> <li>Apply SmoothQuant to shift scaling between weights and activations  </li> <li>Use Quantization-Aware Fine-Tuning (QAFT) to make adapters robust to quantization noise  </li> <li>Limit LoRA influence via smaller rank \\(r\\) or scaling factor \\(\\alpha\\)</li> <li>Use larger block sizes (e.g., 128) to reduce sensitivity to local activation shifts  </li> </ul> <p>In practice, QLoRA\u2019s frozen-weight design and low-rank adapters keep drift minimal, but understanding this effect is important for advanced fine-tuning and quantization-aware training workflows.</p>"},{"location":"supporting_topics/4bit_normal_float/#6-practical-notes","title":"6. Practical Notes","text":"<ul> <li>Precision Trade-off: <code>4-bit NF4</code> achieves near-float accuracy while reducing memory up to <code>4x</code>.</li> <li>Block Dependency: <code>NF4</code> inherently requires per-block normalization (mean &amp; std). Without it, a global scale would fail due to outliers.</li> <li>Compatibility: Used in QLoRA, bitsandbytes, and PEFT libraries for efficient 4-bit fine-tuning.</li> <li>Performance: Empirical studies (Dettmers et al., 2023) show NF4 retains &gt;99.5% of FP16 accuracy for LLaMA-like models with up to <code>8.1x</code> faster training throughput.</li> </ul>"},{"location":"supporting_topics/4bit_normal_float/#7-summary","title":"7. Summary","text":"Aspect Description Bit-width 4 bits Quantization type Non-uniform (NormalFloat codebook) Normalization Per block (mean &amp; std) Key benefit Precision around zero preserved Typical use QLoRA / LoRA fine-tuning Dependency Requires block-wise normalization"},{"location":"supporting_topics/accelerate/","title":"\u26a1 Accelerate: Efficient Training for Large Language Models","text":""},{"location":"supporting_topics/accelerate/#1-overview","title":"1. Overview","text":"<p>Accelerate is a lightweight framework by Hugging Face that simplifies distributed and mixed-precision training for large models, including LLMs. It abstracts device placement, process coordination, and backend integration so developers can scale from single GPU to multi-node setups with minimal code changes.</p> <p>Accelerate works as an orchestration layer on top of PyTorch DDP, FSDP, DeepSpeed ZeRO, and TPU/XLA, without introducing new training algorithms.</p>"},{"location":"supporting_topics/accelerate/#key-features","title":"Key Features","text":"<ul> <li>Multi-GPU, multi-node, and TPU training with minimal code changes</li> <li>Mixed precision support (FP16, BF16)</li> <li>Gradient accumulation</li> <li>Integration with FSDP and DeepSpeed ZeRO for memory efficiency</li> <li>Distributed-safe checkpointing and logging</li> </ul>"},{"location":"supporting_topics/accelerate/#2-problem-statement","title":"2. Problem Statement","text":"<p>Training large transformer models introduces key challenges:</p> <ol> <li>Memory limits - Models often exceed single-GPU memory.</li> <li>Distributed complexity - Manual DDP setup is error-prone.</li> <li>Scaling - Efficient multi-GPU or multi-node scaling is non-trivial.</li> <li>Numerical stability - Mixed-precision training requires careful handling.</li> </ol> <p>Accelerate addresses these by providing a unified, backend-agnostic interface for distributed training.</p>"},{"location":"supporting_topics/accelerate/#3-core-components","title":"3. Core Components","text":""},{"location":"supporting_topics/accelerate/#31-accelerator","title":"\ud83e\udde9 3.1. <code>Accelerator</code>","text":"<p>The central abstraction that manages:</p> <ul> <li>Device placement</li> <li>Distributed backend setup</li> <li>Mixed precision</li> <li>Gradient accumulation</li> <li>Process coordination for logging and checkpointing</li> </ul> <p>Initialization: <pre><code>from accelerate import Accelerator\naccelerator = Accelerator()\n\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n</code></pre></p> <p>What Accelerate does automatically:</p> <ul> <li>Moves models and data to the correct device </li> <li>Wraps models with DDP, FSDP, or DeepSpeed </li> <li>Handles mixed-precision context and gradient scaling</li> </ul>"},{"location":"supporting_topics/accelerate/#32-device-management","title":"\u2699\ufe0f 3.2. Device Management","text":"<p>Accelerate auto-detects available hardware and exposes a unified device handle.</p> <ul> <li>Supports CPU, CUDA GPUs, and TPUs</li> <li>Avoids manual .cuda() or rank-specific logic</li> </ul> <pre><code>inputs = inputs.to(accelerator.device)\n</code></pre> <p>Benefit: Prevents CUDA placement errors and maximizes hardware utilization.</p>"},{"location":"supporting_topics/accelerate/#33-distributed-data-parallelism-ddp","title":"\ud83d\udd01 3.3. Distributed Data Parallelism (DDP)","text":"<p>Each device holds a replica of the model and processes a shard of data.</p>"},{"location":"supporting_topics/accelerate/#workflow","title":"\u2699\ufe0f Workflow","text":"<ol> <li>Each GPU computes gradients on its local data shard.  </li> <li>Gradients are averaged across all GPUs.  </li> <li>Parameter updates are synchronized globally.</li> </ol>"},{"location":"supporting_topics/accelerate/#mathematical-representation","title":"\ud83e\uddee Mathematical Representation","text":"\\[ g = \\frac{1}{D} \\sum_{d=1}^{D} g_d \\] <p>Where:</p> <ul> <li>\\( D \\): Number of devices  </li> <li>\\( g_d \\): Gradient computed on device \\( d \\)</li> </ul> <p>Accelerate provides:</p> <ul> <li>Simple configuration for DDP</li> <li>Support for FSDP and DeepSpeed ZeRO</li> <li>Efficient gradient synchronization using PyTorch primitives </li> <li> <p>Gradient bucketing - combining many small gradient updates into a few larger batches before sharing them between GPUs - this reduces communication time and makes training faster.</p> Difference b/w Gradient Accumulation and Bucketing <ul> <li>Gradient Accumulation helps with memory limits \u2014 it adds up gradients over several mini-batches before taking an optimizer step, so you can simulate larger batch sizes on limited GPU memory. </li> <li>Gradient Bucketing helps with communication overhead \u2014 it groups many small gradients together before synchronizing across GPUs, so data exchange between devices is faster and more efficient.</li> </ul> </li> </ul>"},{"location":"supporting_topics/accelerate/#34-gradient-accumulation","title":"\ud83d\udcbe 3.4. Gradient Accumulation","text":"<p>Simulates large batch sizes without exceeding GPU memory limits by accumulating gradients over multiple mini-batches before performing an optimizer step.</p>"},{"location":"supporting_topics/accelerate/#mathematical-formulation","title":"\ud83e\uddee Mathematical Formulation","text":"\\[ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i \\] <p>Where:</p> <ul> <li>\\( N \\): Number of mini-batches accumulated  </li> <li>\\( g_i \\): Gradient from the \\( i^{th} \\) mini-batch</li> </ul>"},{"location":"supporting_topics/accelerate/#implementation-example","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation Example","text":"<p><pre><code>with accelerator.accumulate(model):\n    loss = model(**batch).loss\n    accelerator.backward(loss)\n</code></pre> \ud83d\ude80 Benefits</p> <ul> <li>Enables stable training even on smaller GPUs.</li> <li>Effectively increases batch size without additional memory requirements.</li> </ul>"},{"location":"supporting_topics/accelerate/#35-mixed-precision-training","title":"\ud83e\uddee 3.5. Mixed Precision Training","text":"<p>Accelerate integrates Automatic Mixed Precision (AMP) for performing computations using FP16 or BF16, while maintaining numerical stability and high throughput.</p>"},{"location":"supporting_topics/accelerate/#mechanism","title":"\u2699\ufe0f Mechanism","text":"<ul> <li>Forward Pass: Forward pass uses lower precision FP16  </li> <li>Backward Pass: Applies dynamic loss scaling to prevent gradient underflow (mainly FP16).  </li> <li>Optimizer Step: Performed in FP32 for numerical stability during parameter updates.</li> </ul>"},{"location":"supporting_topics/accelerate/#outcomes","title":"\ud83d\ude80 Outcomes","text":"<ul> <li>2\u00d7 faster training  </li> <li>~50% less GPU memory usage  </li> <li>Comparable accuracy to full FP32 training  </li> </ul>"},{"location":"supporting_topics/accelerate/#36-optimizer-and-scheduler-wrappers","title":"\u26a1 3.6. Optimizer and Scheduler Wrappers","text":"<p>Accelerate automatically scales and synchronizes optimizers and schedulers.</p> <pre><code>optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n</code></pre> <p>Key Functions:</p> <ul> <li>Synchronizes state across distributed workers. </li> <li>Compatibility with sharded optimizers (FSDP, ZeRO)  </li> <li>Works with common optimizers like AdamW and Adafactor</li> </ul>"},{"location":"supporting_topics/accelerate/#37-checkpointing-and-state-management","title":"\ud83e\uddf1 3.7. Checkpointing and State Management","text":"<p>Manages distributed checkpointing with process coordination:</p> <ul> <li>Consolidates multi-GPU state into single checkpoints. </li> <li>Includes model weights, optimizer states, RNG, and scheduler. </li> <li>Compatible with FSDP and ZeRO partitioned states.</li> </ul> <p>Example:</p> <pre><code>accelerator.save_state(output_dir=\"checkpoints/\")\n</code></pre> <p>Benefit: Fault-tolerant and restart-safe training in multi-node clusters.</p>"},{"location":"supporting_topics/accelerate/#38-logging-and-monitoring","title":"\ud83d\udd0d 3.8. Logging and Monitoring","text":"<p>Supports built-in and third-party loggers:</p> <ul> <li>TensorBoard, Weights &amp; Biases, MLflow, or custom. </li> <li>Ensures only the main process logs globally aggregated metrics.</li> <li>Built-in accelerator.print() avoids duplicate console output.</li> </ul> <pre><code>accelerator.log({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n</code></pre>"},{"location":"supporting_topics/accelerate/#39-memory-and-compute-efficiency-tools","title":"\ud83e\udde0 3.9. Memory and Compute Efficiency Tools","text":"<p>Accelerate provides hooks for reducing memory footprint:</p> <ul> <li>Gradient Checkpointing: Recomputes intermediate activations during backprop.</li> <li>Model Parameter Sharding (FSDP/ZeRO): Splits model weights across GPUs.</li> <li>Dynamic Padding: Reduces unnecessary computation on padded tokens.</li> </ul> <p>Useful for long-sequence transformer models where input lengths vary widely.</p>"},{"location":"supporting_topics/accelerate/#310-backend-support","title":"\ud83c\udf10 3.10. Backend Support","text":"<p>Accelerate integrates seamlessly across various distributed backends:</p> Backend Description Typical Use PyTorch DDP Default distributed backend Multi-GPU training FSDP Fully sharded parameter and optimizer state Memory-constrained setups DeepSpeed ZeRO Offloads parameters to CPU/NVMe Ultra-large LLMs (10B\u2013100B+) TPU/XLA TPU support via PyTorch/XLA Cloud TPU pods"},{"location":"supporting_topics/accelerate/#4-accelerate-training-workflow","title":"4. Accelerate Training Workflow","text":"<pre><code>from accelerate import Accelerator\n\n# Initialize Accelerator\naccelerator = Accelerator()\n\n# Prepare model, optimizer, dataloader\nmodel, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n\n# Training Loop\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        with accelerator.accumulate(model):\n            with accelerator.autocast():\n                outputs = model(**batch)\n                loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n\n    # Save checkpoint and log metrics\n    accelerator.save_state(f\"checkpoints/epoch_{epoch}\")\n    accelerator.log({\"epoch\": epoch, \"loss\": loss.item()})\n</code></pre>"},{"location":"supporting_topics/blockwise_kbit_quantization/","title":"\u2699\ufe0f Block-wise k-bit Quantization","text":""},{"location":"supporting_topics/blockwise_kbit_quantization/#1-overview","title":"1. Overview","text":"<p>Block-wise k-bit quantization is a technique that compresses model weights into low-bit representations (e.g., 4-bit, 8-bit) while preserving performance and minimizing quantization error. Instead of quantizing each value independently, block-wise quantization divides the weight matrix into smaller blocks (chunks) and performs quantization relative to local statistics (like scale and zero-point) of each block.</p> <p>This local normalization significantly reduces quantization error caused by outlier values \u2014 a common issue in transformer weights.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#2-motivation","title":"2. Motivation","text":""},{"location":"supporting_topics/blockwise_kbit_quantization/#problem-outliers-in-weight-distributions","title":"\ud83e\udde0 Problem: Outliers in Weight Distributions","text":"<p>Weights in large models (especially in attention layers) often follow heavy-tailed distributions \u2014 a few large values coexist with many small ones. In global quantization, a single scale \\( s_{\\text{global}} = \\frac{\\max(|W|)}{2^{k-1}-1} \\) is used for all weights. Large outliers force the scale up, making most small weights collapse to zero after quantization.</p> <p>Example</p> <p>Consider: $$ W = [0.01, 0.02, -0.03, 0.05, 3.0] $$</p> <p>With 4-bit global quantization: $$ s_{\\text{global}} = \\frac{3.0}{7} \\approx 0.43 $$</p> <p>Quantized weights \u2192 <code>[0, 0, 0, 0, 7]</code> \u2014 almost all small weights vanish due to the single large outlier.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#solution-block-wise-quantization","title":"\u2705 Solution: Block-wise Quantization","text":"<p>Split weights into small blocks (e.g., 64\u2013256 values each), and compute a separate scale per block: $$ s_b = \\frac{\\max(|W_b|)}{2^{k-1}-1} $$ Each block adapts to its local range, preserving fine details while still compressing efficiently.</p> <p>By partitioning weights into blocks and computing scale/offset per block, quantization adapts to local statistics and better preserves precision.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#3-mathematical-formulation","title":"3. Mathematical Formulation","text":"<p>\ud83d\udcd8 Steps for block quantization</p> <p>Let:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\): full-precision weight matrix</li> <li>\\(B_i \\subset W\\): the i-th block of size \\(n_b\\)</li> <li>\\(k\\): number of bits used for quantization (e.g., 4 or 8)</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-1-compute-local-scale-and-zero-point","title":"Step 1: Compute Local Scale and Zero-Point","text":"<p>For each block \\(B_i\\):</p> \\[ s_i = \\frac{\\max(B_i) - \\min(B_i)}{2^k - 1} \\] \\[ z_i = \\text{round}\\left(-\\frac{\\min(B_i)}{s_i}\\right) \\] <p>Where:</p> <ul> <li>\\(s_i\\): scale factor for block \\(i\\)</li> <li>\\(z_i\\): zero-point (offset)</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-2-quantization","title":"Step 2: Quantization","text":"<p>Quantized integer representation:</p> \\[ q_i = \\text{clip}\\left(\\text{round}\\left(\\frac{B_i}{s_i}\\right) + z_i, 0, 2^k - 1\\right) \\]"},{"location":"supporting_topics/blockwise_kbit_quantization/#step-3-dequantization-reconstruction","title":"Step 3: Dequantization (Reconstruction)","text":"\\[ \\hat{B_i} = s_i \\times (q_i - z_i) \\] <p>The final reconstructed weight matrix:</p> \\[ \\hat{W} = \\bigcup_i \\hat{B_i} \\]"},{"location":"supporting_topics/blockwise_kbit_quantization/#4-double-quantization","title":"4. Double Quantization","text":"<p>Double quantization is a secondary compression layer designed to reduce the overhead of storing multiple block-wise scales. Instead of storing each block\u2019s scaling factor \\( s_j \\) as a 16-bit or 32-bit float, these scale values themselves are quantized into a lower precision representation (e.g., 8-bit or 4-bit).</p> <p>\ud83d\udcd8 Details</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#41-concept","title":"4.1. Concept","text":"<p>If there are \\(N\\) blocks, each with a scale \\(s_j\\):</p> \\[ \\tilde{s_j} = \\text{quantize}(s_j, s_{\\text{meta}}, q_{\\min}, q_{\\max}) \\] <p>Here, \\(s_{\\text{meta}}\\) is a higher-level scale shared across a group of block-scales.</p> <p>At dequantization:</p> \\[ s_j = s_{\\text{meta}} \\cdot \\tilde{s_j} \\] \\[ \\hat{x_i} = s_j \\cdot q_i \\] <p>This approach can yield 20\u201330% memory savings, especially when using small block sizes where the number of stored scales is large.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#42-example","title":"4.2. Example","text":"<p>Consider a model layer with 10,000 blocks of weights. Each block has one scale \\( s_i \\).</p> Parameter Value Number of blocks 10,000 Scale per block (FP16) 2 bytes Memory (without double quantization) 20 KB Quantized scale (8-bit) 1 byte Memory (with double quantization) 10 KB <p>So double quantization reduces metadata memory by 50% with negligible degradation (typically &lt; 0.1% accuracy loss).</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#43-implementation-in-bitsandbytes","title":"4.3. Implementation in Bitsandbytes","text":"<p>In bitsandbytes 0.39+, both block-wise quantization and double quantization are implemented jointly:</p> <ul> <li>Each weight block is quantized in NF4 format.</li> <li>Each block\u2019s scale value is quantized using 8-bit quantization.</li> <li>The quantized scales are stored alongside the 4-bit codes.</li> <li>Dequantization happens transparently during forward passes.</li> </ul> <p>This enables models like LLaMA-2 70B to be fine-tuned on single 48GB GPUs.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#44-key-notes","title":"4.4 Key Notes","text":"<ul> <li>Double quantization is orthogonal but complementary to block-wise quantization.  </li> <li>It primarily targets metadata compression, not model accuracy.  </li> <li>Used in QLoRA to compress per-block scales efficiently.</li> </ul> Aspect Effect Memory Efficiency Up to 2\u00d7 reduction in metadata storage Accuracy Impact Negligible (&lt; 0.1% degradation) Computation Overhead Minimal (scales dequantized once per block) Compatibility Fully supported in <code>bitsandbytes</code> &amp; <code>QLoRA</code> stack"},{"location":"supporting_topics/blockwise_kbit_quantization/#5-implementation-details-pseudo-code","title":"5. Implementation Details (Pseudo-Code)","text":"<pre><code>def blockwise_quantize(weights, block_size=64, num_bits=4):\n    q_blocks, scales, zeros = [], [], []\n    n = len(weights)\n    for i in range(0, n, block_size):\n        block = weights[i:i+block_size]\n        min_val, max_val = block.min(), block.max()\n        scale = (max_val - min_val) / (2 ** num_bits - 1)\n        zero_point = -min_val / scale\n        q_block = np.round(block / scale + zero_point).clip(0, 2 ** num_bits - 1)\n        q_blocks.append(q_block)\n        scales.append(scale)\n        zeros.append(zero_point)\n    return q_blocks, scales, zeros\n</code></pre>"},{"location":"supporting_topics/blockwise_kbit_quantization/#6-example-4-bit-quantization","title":"6. Example (4-bit Quantization)","text":"<p>\ud83d\udcd8 Working example</p> <p>Consider a block of weights:</p> \\[ B_i = [-0.9, -0.3, 0.2, 0.5, 1.0] \\] <p>For \\(k = 4\\) bits:</p> <ul> <li>\\(\\min = -0.9, \\max = 1.0\\)</li> <li>\\(s_i = (1.0 - (-0.9)) / 15 = 0.1267\\)</li> <li>\\(z_i = -(-0.9) / 0.1267 = 7.1 \\approx 7\\)</li> </ul> <p>Quantized values:</p> \\[ q_i = \\text{round}(B_i / s_i + z_i) = [0, 5, 9, 11, 15] \\] <p>Dequantized:</p> \\[ \\hat{B_i} = s_i \\times (q_i - 7) = [-0.9, -0.26, 0.32, 0.51, 1.01] \\] <p>The reconstruction closely approximates the original block.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#7-advantages","title":"7. Advantages","text":"Aspect Benefit Local scaling Reduces sensitivity to outliers Memory Lower storage cost (e.g., 4-bit = 8\u00d7 compression) Compute Enables efficient GPU matrix-multiplication with custom kernels Accuracy Closer performance to full precision"},{"location":"supporting_topics/blockwise_kbit_quantization/#8-hardware-implementation","title":"8. Hardware Implementation","text":"<ul> <li>Most modern inference frameworks (e.g., bitsandbytes, TensorRT) store the scale and zero-point per block.</li> <li>For 4-bit quantization, typical block sizes: 32, 64, or 128.</li> <li>Scales are stored in FP16 to balance precision and storage.</li> </ul>"},{"location":"supporting_topics/blockwise_kbit_quantization/#9-visualization","title":"9. Visualization","text":"<p>A conceptual diagram of block-wise quantization:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Weight Matrix      \u2502\n\u2502  [w\u2081, w\u2082, \u2026, w\u2099]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2193 Split into Blocks\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Block 1      \u2502 Block 2      \u2502 ...\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193                 \u2193\nCompute s\u2081,z\u2081      Compute s\u2082,z\u2082\n     \u2193                 \u2193\nQuantize each block separately\n     \u2193                 \u2193\nStore q\u2081,s\u2081,z\u2081,...,q\u2099,s\u2099,z\u2099\n</code></pre> <p>Each block retains its own quantization scale and offset, enabling more accurate low-bit representation.</p>"},{"location":"supporting_topics/blockwise_kbit_quantization/#10-relationship-to-qlora","title":"10. Relationship to QLoRA","text":"<p>QLoRA uses 4-bit NormalFloat (NF4) quantization with block-wise statistics:</p> <ul> <li>Each block (typically 64 elements) uses local mean and std for normalization.</li> <li>NF4 values are quantized into [-1, 1] with learned scales.</li> <li>This approach allows fine-tuning large LLMs on a single GPU without significant accuracy loss.</li> </ul>"},{"location":"supporting_topics/bp16/","title":"\ud83e\uddee BF16 (BFloat16): Mixed Precision for Stable LLM Training","text":""},{"location":"supporting_topics/bp16/#1-overview","title":"1. Overview","text":"<p>BF16 (Brain Floating Point 16) is a 16-bit floating-point format designed specifically for numerically stable neural network training.</p> <p>The core idea behind BF16 is simple:</p> <p>Preserve the numerical range required for training, while reducing memory and compute cost.</p> <p>This makes BF16 especially suitable for training large models such as LLMs, where values can vary widely during forward and backward passes.</p>"},{"location":"supporting_topics/bp16/#2-motivation-what-goes-wrong-with-lower-precision","title":"2. Motivation: What Goes Wrong with Lower Precision","text":"<p>Training deep neural networks involves: - Very large values in activations and gradients - Very small gradient updates - Accumulation of numerical error over long runs</p> <p>Standard FP16 improves speed and memory usage but introduces a major issue: - Its limited exponent range causes gradient underflow and overflow</p> <p>BF16 was introduced to solve these stability issues without reverting to FP32.</p>"},{"location":"supporting_topics/bp16/#3-bf16-vs-fp16-vs-fp32","title":"3. BF16 vs FP16 vs FP32","text":"Format Bits Exponent Bits Mantissa Bits Dynamic Range Precision FP32 32 8 23 High High FP16 16 5 10 Low Medium BF16 16 8 7 High (same as FP32) Lower <p>Justification - Training stability depends more on range than fine-grained precision - Small rounding errors are usually tolerable - Underflow and overflow are not</p> <p>BF16 keeps the FP32 exponent, which directly addresses instability.</p>"},{"location":"supporting_topics/bp16/#4-fp16-vs-bf16-precision-vs-range-through-examples","title":"4. FP16 vs BF16: Precision vs Range Through Examples","text":"<p>These examples illustrate the core tradeoff between FP16 and BF16: FP16 has higher precision, while BF16 has larger numerical range.</p> <p>Example 1: Small but representable value</p> <p>Original value: <code>0.0001</code></p> <ul> <li>FP16: <code>0.00010001659393</code>   (10-bit mantissa, 5-bit exponent)</li> <li>BF16: <code>0.00010013580322</code>   (7-bit mantissa, 8-bit exponent)</li> </ul> <p>Explanation</p> <p>Both formats can represent this value, but FP16 is closer to the original because it has more mantissa bits. This shows FP16\u2019s advantage in precision when the value lies within its representable range.</p> <p>Example 2: Very small value</p> <p>Original value: <code>1e-08</code></p> <ul> <li>FP16: <code>0.0</code>   (underflow)</li> <li>BF16: <code>0.00000001001172</code></li> </ul> <p>Explanation</p> <p>FP16 cannot represent this value because its binary exponent range is too small, even though it has 10 mantissa bits. The number of decimal zeros is irrelevant \u2014 FP16\u2019s minimum normalized positive number is about <code>6.1e-5</code>, and numbers smaller than this either underflow or rely on very low-precision subnormals. BF16 succeeds because it has the same exponent range as FP32, allowing much smaller numbers to be represented reliably.</p> <p>This is a key reason BF16 is more stable during training, especially for gradients that can be extremely small.</p> <p>Example 3: Large value</p> <p>Original value: <code>100000.00001</code></p> <ul> <li>FP16: <code>inf</code>   (overflow)</li> <li>BF16: <code>99840.0</code></li> </ul> <p>Explanation</p> <p>FP16 overflows because all exponent bits are exhausted. BF16 can still represent the value, though with reduced precision, because it has more exponent bits.</p> <p>Key takeaway</p> <ul> <li>FP16 represents values more accurately within its limited range  </li> <li>BF16 represents values more reliably across a wide range  </li> <li>Training prefers range over precision, which is why BF16 is often safer for large models</li> </ul>"},{"location":"supporting_topics/bp16/#5-why-bf16-is-stable-during-training","title":"5. Why BF16 Is Stable During Training","text":"<p>During backpropagation: - Gradients can span many orders of magnitude - Values that fall outside representable range collapse to zero or NaN</p> <p>Because BF16 has the same exponent range as FP32: - Gradients rarely underflow - Overflow is significantly reduced</p> <p>This leads to more predictable and stable optimization behavior.</p>"},{"location":"supporting_topics/bp16/#6-loss-scaling-and-why-bf16-usually-does-not-need-it","title":"6. Loss Scaling and Why BF16 Usually Does Not Need It","text":"<p>Loss scaling multiplies the loss to push gradients into a representable range.</p> <ul> <li>FP16 requires loss scaling because its exponent range is small</li> <li>BF16 usually does not, because the range is already sufficient</li> </ul> <p>Implication - BF16 simplifies training pipelines - Fewer tuning knobs - Fewer silent numerical failures</p> Loss Scaling (click to expand) In mixed-precision training with FP16, gradients can become too small to be represented and underflow to zero, which stops learning.  Loss scaling prevents this by temporarily increasing gradient magnitudes without changing the true optimization objective.   How it works 1. Multiply the loss by a scale factor   2. Backpropagate using the scaled loss   3. Divide gradients by the same factor before the optimizer step   **Example** A true gradient:  $$ g = 1 \\times 10^{-9} $$  In FP16, this value underflows to zero. With a scale factor \\( S = 1024 \\):  $$ g' = S \\cdot g = 1.024 \\times 10^{-6} $$  This value is representable in FP16. Before the optimizer update, gradients are divided by \\( S \\), preserving the correct update.  Key point - FP16 requires loss scaling due to limited exponent range   - BF16 usually does not, because it preserves FP32\u2019s range"},{"location":"supporting_topics/bp16/#7-performance-and-memory-characteristics","title":"7. Performance and Memory Characteristics","text":"<p>BF16 reduces memory usage by half compared to FP32, which: - Allows larger batch sizes - Reduces memory bandwidth pressure</p> <p>On supported hardware, BF16: - Uses tensor cores - Achieves near-FP16 throughput - Converges similarly to FP32 in practice</p> <p>The slight loss in mantissa precision rarely affects convergence for deep models.</p>"},{"location":"supporting_topics/bp16/#8-bf16-in-practice-pytorch","title":"8. BF16 in Practice (PyTorch)","text":"<p>```python import torch</p> <p>model = model.to(dtype=torch.bfloat16)</p> <p>with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):     outputs = model(inputs)     loss = outputs.loss</p>"},{"location":"supporting_topics/decoding_strategies/","title":"Decoding Strategies","text":""},{"location":"supporting_topics/decoding_strategies/#decoding-strategies","title":"\ud83d\udce6 Decoding Strategies","text":""},{"location":"supporting_topics/decoding_strategies/#1-overview","title":"1. Overview","text":"<p>Large Language Models output a probability distribution over the vocabulary at each decoding step. A decoding strategy defines how the next token is selected from this distribution.</p> <p>This page covers five commonly used decoding strategies:</p> <ol> <li>Greedy decoding  </li> <li>Beam search  </li> <li>Temperature sampling  </li> <li>Top-k sampling  </li> <li>Top-p sampling (nucleus sampling)</li> </ol>"},{"location":"supporting_topics/decoding_strategies/#2-decoding-strategies-explained-with-examples","title":"2. Decoding Strategies Explained with examples","text":"<p>Toy probability distribution used in examples. Assume the model predicts the next token after:</p> <pre><code>**\"The cat sat on the\"**\n</code></pre> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02"},{"location":"supporting_topics/decoding_strategies/#21-greedy-decoding","title":"2.1. Greedy Decoding","text":""},{"location":"supporting_topics/decoding_strategies/#idea","title":"Idea","text":"<p>Always select the token with the highest probability.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm","title":"Algorithm","text":"<pre><code>next_token = argmax(probabilities)\nHighest probability token is `mat`.\nOutput: The cat sat on the mat\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#edge-case","title":"Edge Case","text":"<pre><code>If probabilities are very close: A: 0.31, B: 0.30, C: 0.29\nGreedy decoding always selects `A`, even when the model is uncertain.\n\nThis often leads to repetitive or dull outputs.\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#when-to-use","title":"When to use","text":"<ul> <li>Debugging</li> <li>Baselines</li> <li>Deterministic generation</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example","title":"Python example","text":"<pre><code>import torch\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\nnext_token = torch.argmax(probs)\nprint(next_token.item())\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#22-beam-search","title":"2.2 Beam Search","text":""},{"location":"supporting_topics/decoding_strategies/#idea_1","title":"Idea","text":"<p>Beam search keeps multiple candidate sequences at each decoding step instead of a single one. It selects the sequence with the highest overall probability, not just the best local choice.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_1","title":"Algorithm","text":"<ol> <li>Maintain B beams, where B is the beam width  </li> <li>At each step, expand every beam with all possible next tokens  </li> <li>Compute cumulative log probability for each expanded sequence  </li> <li>Keep the top B sequences  </li> <li>Repeat until an end condition is met</li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example","title":"Example","text":"<p>Assume the next-token probabilities after:</p> <p>\"The cat sat on the\"</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 <p>Beam width = 2</p> <p>Step 1</p> <ul> <li>Beam 1: <code>\"mat\"</code> score = log(0.40)</li> <li>Beam 2: <code>\"floor\"</code> score = log(0.25)</li> </ul> <p>Step 2</p> <ul> <li><code>\"mat \u2192 quietly\"</code> score = log(0.40) + log(0.30)</li> <li><code>\"floor \u2192 loudly\"</code> score = log(0.25) + log(0.50)</li> </ul> <p>Even if <code>\"quietly\"</code> was locally better, <code>\"floor \u2192 loudly\"</code> may win due to higher cumulative probability.</p> <p>Final output is the sequence with the highest total score.</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_1","title":"Edge Case","text":"<p>Beam search tends to favor safe, high-probability continuations, which can reduce diversity. This behavior becomes obvious in conversational or creative tasks.</p> <p>Assume the model is generating the next phrase after:</p> <p>\"I think that\"</p> <p>At a certain step, the model assigns probabilities like:</p> Token Probability the 0.35 we 0.30 this 0.15 pizza 0.10 unicorn 0.10 <p>With beam width = 3:</p> <p>All beams will keep continuations starting with: <code>\"the\"</code> <code>\"we\"</code> <code>\"this\"</code></p> <p>Tokens like <code>\"pizza\"</code> and <code>\"unicorn\"</code> are discarded early because their probabilities are lower.</p> <p>As decoding continues, beams converge to similar phrases:</p> <ul> <li>I think that the best way to...</li> <li>I think that we should...</li> <li>I think that this is...</li> </ul> <p>All beams are grammatically correct but nearly identical.</p> <p>If top-p sampling is used instead:</p> <ul> <li>Tokens like <code>\"pizza\"</code> or <code>\"unicorn\"</code> may occasionally be sampled</li> <li> <p>Outputs become more diverse:</p> <ul> <li>I think that pizza could solve this</li> <li>I think that unicorn stories are fun</li> </ul> </li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-to-use-beam-search","title":"When to use beam search","text":"<ul> <li>Machine translation  </li> <li>Speech recognition  </li> <li>Structured text generation  </li> <li>Tasks where correctness matters more than diversity</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-not-to-use-beam-search","title":"When not to use beam search","text":"<ul> <li>Chatbots  </li> <li>Story generation  </li> <li>Creative writing  </li> <li>Conversational agents</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example-simplified","title":"Python example (simplified)","text":"<pre><code>from heapq import nlargest\nimport math\n\ndef beam_search_step(beams, probs, beam_width):\n    new_beams = []\n    for seq, score in beams:\n        for i, p in enumerate(probs):\n            new_seq = seq + [i]\n            new_score = score + math.log(p)\n            new_beams.append((new_seq, new_score))\n    return nlargest(beam_width, new_beams, key=lambda x: x[1])\n\n# Initial beam\nbeams = [([], 0.0)]\nprobs = [0.40, 0.25, 0.15]\n\nbeams = beam_search_step(beams, probs, beam_width=2)\nprint(beams)\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#23-temperature-sampling","title":"2.3 Temperature Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#idea_2","title":"Idea","text":"<p>Temperature controls how random the next-token selection is by scaling the model logits before applying softmax.</p> <p>It does not change which tokens are possible. It changes how strongly the model prefers high-probability tokens.</p>"},{"location":"supporting_topics/decoding_strategies/#formula","title":"Formula","text":"\\[p_i = \\text{softmax}(\\text{logits}_i / T)\\] <p>Where:</p> <ul> <li><code>T</code> is the temperature</li> <li>lower <code>T</code> sharpens the distribution</li> <li>higher <code>T</code> flattens the distribution</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#effect-of-temperature","title":"Effect of temperature","text":"Temperature Behavior T &lt; 1 More deterministic T = 1 Original distribution T &gt; 1 More random"},{"location":"supporting_topics/decoding_strategies/#example_1","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Low temperature (T = 0.3)</p> <ul> <li>Distribution becomes very sharp</li> <li><code>mat</code> dominates even more</li> </ul> <p>Output: The cat sat on the mat</p> <p>This behaves almost like greedy decoding.</p> <p>High temperature (T = 1.5)</p> <ul> <li>Distribution becomes flatter</li> <li>Low-probability tokens become more likely</li> </ul> <p>Possible output: The cat sat on the moon</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_2","title":"Edge Case","text":"<p>With very high temperature:</p> Token Probability mat 0.18 floor 0.17 sofa 0.16 bed 0.15 roof 0.14 moon 0.10 pizza 0.10 <p>The model loses strong preferences and may generate incoherent text:</p> <pre><code>The cat sat on pizza quantum sky\n</code></pre>"},{"location":"supporting_topics/decoding_strategies/#when-temperature-helps","title":"When temperature helps","text":"<ul> <li>Creative writing</li> <li>Brainstorming</li> <li>Open-ended dialogue</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-temperature-hurts","title":"When temperature hurts","text":"<ul> <li>Factual tasks</li> <li>Code generation</li> <li>Structured outputs</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_1","title":"Python example","text":"<pre><code>import torch\n\nlogits = torch.log(torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]))\ntemperature = 1.2\n\nscaled_logits = logits / temperature\nprobs = torch.softmax(scaled_logits, dim=0)\n\nnext_token = torch.multinomial(probs, 1)\nprint(next_token.item())\n</code></pre> <p>Note: Temperature controls randomness, not feasibility. It is usually combined with top-p or top-k sampling to avoid incoherent outputs.</p>"},{"location":"supporting_topics/decoding_strategies/#24-top-k-sampling","title":"2.4 Top-k Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#idea_3","title":"Idea","text":"<p>Top-k sampling restricts the model to sample only from the K most probable tokens at each decoding step. This prevents extremely unlikely tokens from being selected while still allowing randomness.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_2","title":"Algorithm","text":"<ol> <li>Sort all tokens by probability  </li> <li>Keep only the top K tokens  </li> <li>Renormalize their probabilities  </li> <li>Sample one token  </li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example_2","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-k with k = 3</p> <p>Tokens kept:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> </ul> <p>Tokens removed:</p> <ul> <li>bed, roof, moon, pizza</li> </ul> <p>Possible output: The cat sat on the sofa</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case_3","title":"Edge Case","text":"<p>Flat probability distribution</p> <p>Assume: A: 0.11, B: 0.10, C: 0.10, D: 0.10, E: 0.10, F: 0.10, G: 0.10</p> <p>With <code>k = 3</code>:</p> <ul> <li>Only A, B, C are considered</li> <li>D, E, F, G are removed despite being equally likely</li> </ul> <p>This makes top-k sensitive to the choice of K and blind to the shape of the distribution.</p>"},{"location":"supporting_topics/decoding_strategies/#when-top-k-works-well","title":"When top-k works well","text":"<ul> <li>Moderate creativity with controlled randomness</li> <li>General text generation</li> <li>Chat systems with fixed diversity constraints</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-top-k-works-poorly","title":"When top-k works poorly","text":"<ul> <li>Highly uncertain distributions</li> <li>Long-form creative writing</li> <li>Prompts with many equally valid continuations</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_2","title":"Python example","text":"<pre><code>import torch\n\ndef top_k_sampling(probs, k):\n    topk_probs, topk_idx = torch.topk(probs, k)\n    topk_probs = topk_probs / topk_probs.sum()\n    sampled = torch.multinomial(topk_probs, 1)\n    return topk_idx[sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_k_sampling(probs, k=3)\nprint(token.item())\n</code></pre> <p>Note: Top-k sampling fixes the number of candidate tokens regardless of model confidence. This makes it simpler than top-p but less adaptive in practice.</p>"},{"location":"supporting_topics/decoding_strategies/#25-top-p-sampling-nucleus-sampling","title":"2.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"supporting_topics/decoding_strategies/#idea_4","title":"Idea","text":"<p>Top-p sampling selects the smallest possible set of tokens whose cumulative probability is at least <code>p</code>, then samples from that set. Unlike top-k, the number of candidate tokens changes dynamically based on model confidence.</p>"},{"location":"supporting_topics/decoding_strategies/#algorithm_3","title":"Algorithm","text":"<ol> <li>Sort tokens by probability in descending order  </li> <li>Add tokens until cumulative probability \u2265 <code>p</code> </li> <li>Renormalize probabilities within this set  </li> <li>Sample one token  </li> </ol>"},{"location":"supporting_topics/decoding_strategies/#example_3","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-p with p = 0.9</p> <p>Cumulative probability:</p> <ul> <li>mat \u2192 0.40  </li> <li>floor \u2192 0.65  </li> <li>sofa \u2192 0.80  </li> <li>bed \u2192 0.90  </li> </ul> <p>Tokens selected:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> <li>bed</li> </ul> <p>Possible output: The cat sat on the bed</p>"},{"location":"supporting_topics/decoding_strategies/#edge-case-key-difference-from-top-k","title":"Edge Case (Key Difference from Top-k)","text":"<p>Highly confident model</p> <p>Assume: A: 0.85, B: 0.07, C: 0.03, D: 0.03, E: 0.02</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B  </li> <li>Effective K = 2</li> </ul> <p>With top-k (k = 5):</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> </ul> <p>Top-p automatically reduces randomness when the model is confident.</p>"},{"location":"supporting_topics/decoding_strategies/#another-edge-case","title":"Another Edge Case","text":"<p>Uncertain model</p> <p>Assume: A: 0.20, B: 0.20, C: 0.20, D: 0.20, E: 0.20</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> <li>Effective K = 5</li> </ul> <p>Top-p expands the candidate set when uncertainty is high.</p>"},{"location":"supporting_topics/decoding_strategies/#when-top-p-works-well","title":"When top-p works well","text":"<ul> <li>Conversational agents</li> <li>Long-form text generation</li> <li>Creative writing with coherence</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#when-top-p-works-poorly","title":"When top-p works poorly","text":"<ul> <li>Strictly deterministic tasks</li> <li>Code generation with exact formatting requirements</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#python-example_3","title":"Python example","text":"<pre><code>import torch\n\ndef top_p_sampling(probs, p):\n    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n    cumulative = torch.cumsum(sorted_probs, dim=0)\n\n    cutoff_mask = cumulative &lt;= p\n    cutoff_mask[cutoff_mask.sum()] = True\n\n    filtered_probs = sorted_probs[cutoff_mask]\n    filtered_probs = filtered_probs / filtered_probs.sum()\n\n    sampled = torch.multinomial(filtered_probs, 1)\n    return sorted_idx[cutoff_mask][sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_p_sampling(probs, p=0.9)\nprint(token.item())\n</code></pre> <p>Note : Top-p sampling adapts to the probability distribution shape, making it more robust than top-k for real-world language generation.</p>"},{"location":"supporting_topics/decoding_strategies/#3-pros-and-cons-of-decoding-strategies-in-large-language-models","title":"3. Pros and Cons of Decoding Strategies in Large Language Models","text":""},{"location":"supporting_topics/decoding_strategies/#31-greedy-decoding","title":"3.1 Greedy Decoding","text":""},{"location":"supporting_topics/decoding_strategies/#pros","title":"Pros","text":"<ul> <li>Extremely fast and simple</li> <li>Fully deterministic and reproducible</li> <li>Easy to debug and analyze</li> <li>Works well when the model is very confident</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons","title":"Cons","text":"<ul> <li>No diversity at all</li> <li>Easily gets stuck in repetitive loops</li> <li>Early mistakes cannot be corrected</li> <li>Often produces dull or incomplete responses</li> <li>Poor performance for long or open-ended generation</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#32-beam-search","title":"3.2 Beam Search","text":""},{"location":"supporting_topics/decoding_strategies/#pros_1","title":"Pros","text":"<ul> <li>Optimizes global sequence likelihood</li> <li>Reduces early local decision errors</li> <li>Produces fluent and grammatically correct text</li> <li>Effective for tasks with a single correct output</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_1","title":"Cons","text":"<ul> <li>Computationally expensive</li> <li>Produces generic and safe outputs</li> <li>Very low diversity</li> <li>All beams often converge to similar sequences</li> <li>Performs poorly for dialogue and creative tasks</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#33-temperature-sampling","title":"3.3 Temperature Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#pros_2","title":"Pros","text":"<ul> <li>Simple and intuitive control over randomness</li> <li>Enables creative and diverse outputs</li> <li>Easy to combine with other sampling methods</li> <li>Useful for brainstorming and storytelling</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_2","title":"Cons","text":"<ul> <li>High temperature can cause incoherent text</li> <li>Low temperature collapses to greedy behavior</li> <li>Does not prevent sampling of very unlikely tokens</li> <li>Sensitive to temperature tuning</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#34-top-k-sampling","title":"3.4 Top-k Sampling","text":""},{"location":"supporting_topics/decoding_strategies/#pros_3","title":"Pros","text":"<ul> <li>Prevents extremely low-probability tokens</li> <li>Provides controlled randomness</li> <li>Simple to implement</li> <li>More diverse than greedy and beam search</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_3","title":"Cons","text":"<ul> <li>Fixed K ignores distribution shape</li> <li>Sensitive to the choice of K</li> <li>Removes valid tokens in flat distributions</li> <li>Not adaptive to model confidence</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#35-top-p-sampling-nucleus-sampling","title":"3.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"supporting_topics/decoding_strategies/#pros_4","title":"Pros","text":"<ul> <li>Adapts automatically to model confidence</li> <li>Better diversity-quality tradeoff than top-k</li> <li>Stable across different prompts</li> <li>Widely used in modern chat models</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#cons_4","title":"Cons","text":"<ul> <li>Slightly more complex than top-k</li> <li>Still stochastic and non-deterministic</li> <li>Can include many tokens in very flat distributions</li> <li>Less suitable for strictly deterministic tasks</li> </ul>"},{"location":"supporting_topics/decoding_strategies/#4-high-level-comparison","title":"4. High-level Comparison","text":"Strategy Diversity Determinism Adaptivity Typical Usage Greedy Very low High No Baselines, debugging Beam Search Low Medium No Translation, ASR Temperature Medium to high Low Partial Creative text Top-k Medium Low No General generation Top-p Medium to high Low Yes Chat and dialogue"},{"location":"supporting_topics/flash_attention/","title":"Flash Attention","text":""},{"location":"supporting_topics/flash_attention/#1-overview","title":"1. Overview","text":"<p>FlashAttention is a fast and memory-efficient implementation of the attention mechanism used in Transformer models. This repository explains what FlashAttention is, why it is faster than standard attention, and how it works under the hood, with a focus on interview preparation and practical understanding.</p>"},{"location":"supporting_topics/flash_attention/#2-motivation","title":"2. Motivation","text":"<p>Attention is the core operation behind Transformers, but standard attention becomes a major bottleneck for long sequences. The main problem is not only compute, but memory movement, which is often the true limiter on modern GPUs.</p> <p>FlashAttention was introduced to:</p> <ul> <li>Reduce memory usage  </li> <li>Minimize expensive GPU memory reads and writes  </li> <li>Scale efficiently to long sequences  </li> </ul>"},{"location":"supporting_topics/flash_attention/#3-standard-attention-and-its-limitations","title":"3. Standard Attention and Its Limitations","text":"<p>Given query, key, and value matrices:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V \\] <p>While simple and elegant, this formulation has serious performance and memory issues for long sequences.</p>"},{"location":"supporting_topics/flash_attention/#31-quadratic-memory-growth","title":"3.1 Quadratic Memory Growth","text":"<p>Assume:</p> <ul> <li>Sequence length \\(N = 16{,}384\\) </li> <li>FP16 precision (2 bytes per element)  </li> </ul> <p>The attention score matrix \\(QK^T\\) has:</p> \\[ N^2 = 16{,}384^2 \\approx 268 \\text{ million elements} \\] <p>Memory required just for the attention matrix:</p> \\[ 268\\text{M} \\times 2 \\text{ bytes} \\approx 512 \\text{ MB} \\] <p>This does not include the softmax output, gradients during training, or activations from other layers, which can easily exceed GPU memory limits.</p>"},{"location":"supporting_topics/flash_attention/#32-excessive-memory-traffic","title":"3.2 Excessive Memory Traffic","text":"<p>Standard attention performs multiple memory-heavy steps:</p> <ol> <li>Compute \\(QK^T\\) and write to GPU global memory  </li> <li>Read \\(QK^T\\) back to apply softmax  </li> <li>Write softmax output back to memory  </li> <li>Read softmax output again to compute weighted sum with \\(V\\) </li> </ol> <p>Even with fast compute, repeated global memory reads and writes dominate runtime, making GPUs often memory-bound rather than compute-bound.</p>"},{"location":"supporting_topics/flash_attention/#33-inefficient-for-long-sequences-code-example","title":"3.3 Inefficient for Long Sequences (Code Example)","text":"<p>A simplified PyTorch-style implementation:</p> <pre><code>import torch\nimport math\n\n# Q, K, V shape: (batch, seq_len, num_heads, head_dim)\nscores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d)\nattn = torch.softmax(scores, dim=-1)\noutput = torch.matmul(attn, V)\n</code></pre> <p>What happens internally:</p> <ul> <li>scores materializes a full \\(N\u00d7N\\) tensor</li> <li>attn creates another \\(N\u00d7N\\) tensor</li> <li>Both tensors live in global memory</li> </ul> <p>As N grows, memory usage and latency grow quadratically.</p>"},{"location":"supporting_topics/flash_attention/#34-numerical-issues-with-low-precision","title":"3.4 Numerical Issues with Low Precision","text":"<p>With FP16 or BF16:</p> <ul> <li>Large dot products in \\(QK^T\\) can overflow</li> <li>Small values can underflow to zero</li> </ul> <p>Standard attention often requires casting to FP32 for stability, which further increases memory usage and slows execution.</p>"},{"location":"supporting_topics/flash_attention/#4-what-is-flashattention-and-how-it-works","title":"4. What Is FlashAttention and How It Works","text":"<p>FlashAttention is an exact, memory-efficient attention algorithm. It computes the same result as standard attention but avoids materializing the full \\(N \\times N\\) attention matrix. This makes it much faster and reduces GPU memory usage, especially for long sequences.</p> <p>Key advantages:</p> <ul> <li>Handles long sequences efficiently (e.g., 4k+ tokens)  </li> <li>Works in FP16 and BF16 without numerical issues  </li> <li>Reduces memory bandwidth usage with minimal extra compute  </li> </ul> <p>FlashAttention achieves this through three main ideas: tiling, fused computation, and single-pass attention with online softmax.</p>"},{"location":"supporting_topics/flash_attention/#41-tiling","title":"4.1 Tiling","text":"<p>Instead of computing attention for the full sequence at once, FlashAttention splits the query, key, and value matrices into small tiles that fit into GPU shared memory.</p> <p>Example:</p> <ul> <li>Sequence length: \\(N = 16{,}384\\)</li> <li>Tile size: \\(B = 128\\) </li> </ul> <p>Memory usage for a tile: \\(128 \\times 128 = 16{,}384\\) elements (much smaller than \\((16{,}384)^2\\))  </p> <p>Code-style intuition:</p> <pre><code># pseudo-code for tiling\nfor q_tile in Q_tiles:\n    for k_tile, v_tile in zip(K_tiles, V_tiles):\n        partial_scores = q_tile @ k_tile.T\n        # accumulate results incrementally\n</code></pre> <p>Benefit: Only a small block is in memory at a time, reducing GPU memory footprint dramatically.</p>"},{"location":"supporting_topics/flash_attention/#42-fused-computation","title":"4.2 Fused Computation","text":"<p>FlashAttention fuses multiple steps into a single kernel:</p> <ol> <li>Matrix multiplication \\((Q \\cdot K^T)\\) </li> <li>Scaling by \\((1/\\sqrt{d})\\) </li> <li>Softmax computation  </li> <li>Weighted sum with \\((V)\\) </li> </ol> <p>Why this matters: </p> <ul> <li>Standard attention performs each step separately, writing intermediate results to global memory.  </li> <li>FlashAttention keeps all intermediate computations in shared memory, avoiding costly reads/writes.</li> </ul> <p>Example intuition:</p> <pre><code># pseudo-code for fused attention\noutput_tile = flash_attention(q_tile, k_tile, v_tile)\n</code></pre> <p>Here, flash_attention does all four steps at once, producing the final output for that tile.</p>"},{"location":"supporting_topics/flash_attention/#43-single-pass-attention-and-online-softmax","title":"4.3 Single-Pass Attention and Online Softmax","text":"<p>FlashAttention computes attention in one streaming pass:</p> <ul> <li>Compute partial scores for each tile</li> <li>Update running maximum and normalization term for softmax</li> <li>Accumulate output incrementally</li> </ul> <p>This allows numerically stable softmax in FP16/BF16 without ever storing the full attention matrix.</p> <p>Example numerical intuition:</p> <ul> <li>Tile 1 contributes scores [0.1, 0.5, 0.3]</li> <li>Tile 2 contributes [0.2, 0.4, 0.1]</li> <li>Running softmax computes the final normalized weights across tiles incrementally</li> </ul> <p>Benefit:</p> <ul> <li>Exact same result as full attention</li> <li>Avoids overflow/underflow in low precision</li> <li>Reduces memory reads/writes drastically</li> </ul>"},{"location":"supporting_topics/flash_attention/#44-practical-impact","title":"4.4 Practical Impact","text":"<ul> <li>Memory complexity reduced from \\(O(N^2) \u2192 O(N\u22c5B)\\) where \\(B\\) is tile size</li> <li>Enables training with longer sequences or larger batch sizes</li> <li>Provides 2\u20134x speedups for long sequences on modern GPUs</li> </ul> <p>Code example using PyTorch API: <pre><code>from flash_attn import flash_attn_func\n\n# q, k, v shape: (batch, seq_len, num_heads, head_dim)\noutput = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n</code></pre></p> <p>This produces exact attention results while being faster and more memory-efficient than standard attention.</p>"},{"location":"supporting_topics/flash_attention/#5-when-flashattention-helps-and-when-it-does-not","title":"5. When FlashAttention Helps (and When It Does Not)","text":"<p>Works best when:</p> <ul> <li>Sequence length is large (typically 2k tokens or more)</li> <li>Using FP16 or BF16</li> <li>Running on modern NVIDIA GPUs with fast shared memory</li> </ul> <p>Less useful when:</p> <ul> <li>Sequence length is very short</li> <li>CPU-based inference</li> <li>Custom attention patterns not supported by FlashAttention kernels</li> </ul>"},{"location":"supporting_topics/flash_attention/#6-why-is-online-softmax-needed","title":"6. Why is online softmax needed?","text":""},{"location":"supporting_topics/flash_attention/#61-numerical-stability-problem","title":"6.1. Numerical Stability Problem","text":"<p>Standard softmax is computed as:</p> \\[ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\] <p>Issue in FP16/BF16:</p> <ul> <li>FP16 has limited precision (~3\u20134 decimal digits) and a small exponent range.  </li> <li>Large values of \\((x_i)\\) (e.g., 50) cause \\(e^{x_i}\\) to overflow.  </li> <li>Very negative values of \\((x_i)\\) (e.g., -50) cause \\((e^{x_i})\\) to underflow to zero.  </li> <li>Long sequences exacerbate the problem because summing hundreds or thousands of exponentials increases the risk of overflow/underflow.  </li> </ul> <p>Without precautions, computing softmax in FP16 can produce NaNs or zeros, breaking both training and inference.</p>"},{"location":"supporting_topics/flash_attention/#62-why-online-softmax-helps","title":"6.2. Why \u201cOnline\u201d Softmax Helps","text":"<p>FlashAttention computes attention tile by tile, so it cannot store the full \\(N \\times N\\) attention matrix. To compute softmax correctly across the entire sequence in FP16/BF16, it uses online softmax.</p>"},{"location":"supporting_topics/flash_attention/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Maintain a running maximum \\(m\\) across tiles.</p> <ul> <li>Shift scores before exponentiating: \\(e^{x_i - m}\\)</li> <li>Prevents overflow in exponential.</li> </ul> </li> <li> <p>Maintain a running sum of exponentials across tiles.</p> <ul> <li>Partial sums from each tile are combined incrementally.  </li> <li>Ensures correct normalization for the softmax over the full sequence.</li> </ul> </li> <li> <p>Compute the weighted sum with \\(V\\) incrementally.</p> <ul> <li>No full softmax matrix is stored in memory.  </li> <li>Output is accumulated as each tile is processed.</li> </ul> </li> </ol>"},{"location":"supporting_topics/flash_attention/#example","title":"Example","text":"<p>Suppose we have 2 tiles with attention scores:</p> <ul> <li>Tile 1: <code>[0.1, 0.5, 0.3]</code> </li> <li>Tile 2: <code>[0.2, 0.4, 0.1]</code></li> </ul> <p>Standard softmax (if we could store all scores):</p> \\[ \\text{softmax}([0.1, 0.5, 0.3, 0.2, 0.4, 0.1]) \\] <p>Online softmax computation:</p> <ol> <li> <p>Tile 1 </p> <ul> <li>Running max \\(m = 0.5\\) </li> <li>Compute shifted exponentials: <code>[exp(0.1-0.5), exp(0.5-0.5), exp(0.3-0.5)] \u2248 [0.67, 1.0, 0.82]</code> </li> <li>Running sum \\(s = 0.67 + 1.0 + 0.82 = 2.49\\) </li> <li>Partial weighted sum with \\(V\\) stored in output</li> </ul> </li> <li> <p>Tile 2 </p> <ul> <li>New max \\(m = \\max(0.5, 0.4) = 0.5\\) (same in this case)  </li> <li>Shifted exponentials: <code>[exp(0.2-0.5), exp(0.4-0.5), exp(0.1-0.5)] \u2248 [0.74, 0.90, 0.61]</code> </li> <li>Update running sum: \\(s = 2.49 + 0.74 + 0.90 + 0.61 = 4.74\\) </li> <li>Accumulate weighted sum with \\(V\\)</li> </ul> </li> <li> <p>Normalization </p> <ul> <li>Each accumulated output is divided by the final sum \\(s = 4.74\\) </li> <li>Produces exact same softmax result as computing on the full sequence</li> </ul> </li> </ol>"},{"location":"supporting_topics/flash_attention/#key-benefits","title":"Key Benefits","text":"<ul> <li>Computes exact attention even in FP16/BF16  </li> <li>Works efficiently with long sequences and large tiles </li> <li>Avoids storing huge intermediate matrices  </li> <li>Reduces GPU memory usage and memory bandwidth overhead</li> </ul> <p>In short: Online softmax allows FlashAttention to compute attention tile by tile while staying numerically stable and memory-efficient.</p>"},{"location":"supporting_topics/flash_attention/#7-end-to-end-flashattention-example","title":"7. End-to-End FlashAttention Example","text":"<p>Suppose we have:</p> <ul> <li>Sequence length \\(N = 8\\) (small for simplicity)  </li> <li>Head dimension \\(d = 2\\) </li> <li>Tile size \\(B = 4\\) </li> </ul> <p>We want to compute attention for a single head:</p> \\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V \\]"},{"location":"supporting_topics/flash_attention/#step-1-prepare-q-k-v","title":"Step 1: Prepare Q, K, V","text":"<pre><code>import torch\nimport math\n\nQ = torch.tensor([[0.1, 0.2],\n                  [0.3, 0.1],\n                  [0.0, 0.4],\n                  [0.5, 0.2],\n                  [0.3, 0.3],\n                  [0.1, 0.5],\n                  [0.4, 0.0],\n                  [0.2, 0.1]])  # shape: (8, 2)\n\nK = Q.clone()  # for simplicity\nV = torch.arange(8*2).reshape(8,2).float()  # dummy value matrix\n</code></pre>"},{"location":"supporting_topics/flash_attention/#step-2-split-into-tiles","title":"Step 2: Split into Tiles","text":"<p>To reduce memory usage, FlashAttention splits the sequence into smaller tiles that fit into GPU shared memory.</p> <ul> <li>Tile size \\(B=4\\) \u2192 2 tiles along the sequence  </li> </ul> <pre><code># Split Q, K, V into tiles\nQ_tiles = [Q[:4], Q[4:]]  # tile 1 and tile 2\nK_tiles = [K[:4], K[4:]]\nV_tiles = [V[:4], V[4:]]\n</code></pre> <p>Benefit: Only a small portion of the sequence is in memory at a time, avoiding the need to materialize the full attention matrix.</p>"},{"location":"supporting_topics/flash_attention/#step-3-process-tile-1","title":"Step 3: Process Tile 1","text":"<ol> <li> <p>Compute partial scores in shared memory:</p> \\[ \\text{scores} = Q_\\text{tile1} \\cdot K_\\text{tile1}^T / \\sqrt{d} \\] <pre><code>scores_tile1 = Q_tiles[0] @ K_tiles[0].T / math.sqrt(2)\n</code></pre> </li> <li> <p>Apply online softmax:</p> <ul> <li>Compute max of scores: m = scores_tile1.max(dim=1)</li> <li>Shift and exponentiate: exp_scores = torch.exp(scores_tile1 - m)</li> <li>Running sum: s = exp_scores.sum(dim=1)</li> <li>Partial weighted sum with V: output_tile1 = (exp_scores @ V_tiles[0]) / s</li> </ul> </li> </ol> <p>Memory benefit: only a 4\u00d74 matrix exists at a time.</p>"},{"location":"supporting_topics/flash_attention/#step-4-process-tile-2-incrementally","title":"Step 4: Process Tile 2 Incrementally","text":"<ul> <li>Compute partial scores of Q_tile1 \u00d7 K_tile2^T</li> <li>Update running max and running sum for online softmax</li> <li>Accumulate weighted outputs with V_tile2</li> <li>Repeat for Q_tile2 \u00d7 K_tile1^T and Q_tile2 \u00d7 K_tile2^T</li> </ul> <p>No full 8\u00d78 attention matrix is ever materialized.</p>"},{"location":"supporting_topics/flash_attention/#step-5-accumulate-output","title":"Step 5: Accumulate Output","text":"<ul> <li>Incrementally compute the weighted sum across all tiles</li> <li>Resulting output shape (8, 2) matches standard attention</li> <li>Softmax computed exactly using online normalization</li> </ul>"},{"location":"supporting_topics/kv_caching/","title":"KV Caching","text":""},{"location":"supporting_topics/kv_caching/#1-self-attention-recap","title":"\ud83d\udce61. Self Attention Recap","text":"<p>Given hidden states \\(X \\in \\mathbb{R}^{T \\times d}\\):</p> \\[ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V \\] <p>Per head attention:</p> \\[ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V \\] <p>Autoregressive decoding generates one token at a time with causal masking.</p>"},{"location":"supporting_topics/kv_caching/#2-why-kv-cache-is-needed","title":"\ud83d\udce62. Why KV Cache Is Needed","text":"<p>At decoding step \\(t\\), keys and values for tokens \\(1 \\ldots t-1\\) are unchanged but would be recomputed without caching.</p> <p>This repeated computation dominates inference latency and wastes FLOPs.</p>"},{"location":"supporting_topics/kv_caching/#3-kv-cache-mechanism","title":"\ud83d\udce63. KV Cache Mechanism","text":"<p>For each transformer layer \\(\\ell\\):</p> \\[ \\text{KVCache}_\\ell = \\{K_\\ell^{1:t}, V_\\ell^{1:t}\\} \\] <p>At decoding step \\(t\\):</p> <ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to the cache</li> <li>Attend over all cached keys and values</li> </ul> \\[ \\text{Attn}_t = \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Only the current token requires new computation.</p>"},{"location":"supporting_topics/kv_caching/#4-toy-example","title":"\ud83d\udce64. Toy Example","text":"<p>Prompt: \"I like neural\"</p> <p>Step 1: generate <code>\"networks\"</code></p> <ul> <li>Compute and cache keys and values for the prompt</li> <li>Attend to all cached tokens</li> </ul> <p>Step 2: generate <code>\"models\"</code></p> <ul> <li>Reuse cached keys and values</li> <li>Compute keys and values only for <code>\"networks\"</code></li> </ul> <p>Previously generated tokens are never recomputed.</p>"},{"location":"supporting_topics/kv_caching/#5-complexity-analysis","title":"\ud83d\udce65. Complexity Analysis","text":""},{"location":"supporting_topics/kv_caching/#notation","title":"Notation","text":"<ul> <li>\\(T\\): number of generated tokens</li> <li>\\(L\\): number of transformer layers</li> <li>\\(H\\): number of attention heads</li> <li>\\(d_h\\): head dimension</li> </ul>"},{"location":"supporting_topics/kv_caching/#without-kv-cache","title":"Without KV Cache","text":"<p>At each decoding step, attention is recomputed for all previous tokens:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^3) \\]"},{"location":"supporting_topics/kv_caching/#with-kv-cache","title":"With KV Cache","text":"<p>Only attention against cached keys and values is computed:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^2) \\] <p>KV caching removes one full factor of \\(T\\) from decoding complexity.</p>"},{"location":"supporting_topics/kv_caching/#6-memory-cost","title":"\ud83d\udce66. Memory Cost","text":"<p>Each layer stores:</p> \\[ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} \\] <p>Total KV cache memory across all layers:</p> \\[ O(L \\cdot H \\cdot T \\cdot d_h) \\] <p>For long context inference, KV cache memory is often the dominant bottleneck.</p>"},{"location":"supporting_topics/kv_caching/#7-inference-vs-training-usage","title":"\ud83d\udce67. Inference v/s Training Usage","text":""},{"location":"supporting_topics/kv_caching/#71-during-inference","title":"7.1 During Inference","text":"<p>This is the most common and important usage.</p> <p>Inference Workflow</p> <ul> <li>Encode prompt</li> <li>Initialize empty KV cache per layer</li> <li>For each generated token:<ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to cache</li> </ul> </li> <li>Compute attention using cached tensors</li> </ul> <p>Practical Benefits</p> <ul> <li>Faster decoding</li> <li>Lower FLOPs</li> <li>Enables long context generation</li> <li>Essential for streaming and chat systems</li> </ul>"},{"location":"supporting_topics/kv_caching/#72-during-training","title":"7.2 During Training","text":"<p>KV caching is not used in standard full sequence training.</p> <p>Why?</p> <ul> <li>Training processes full sequences in parallel</li> <li>All tokens attend to each other simultaneously</li> <li>No repeated computation across steps</li> </ul>"},{"location":"supporting_topics/kv_caching/#8-scaling-kv-cache-for-long-context","title":"\ud83d\udce68. Scaling KV Cache for Long Context","text":"<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>"},{"location":"supporting_topics/kv_caching/#81-sliding-window-attention","title":"8.1 Sliding Window Attention","text":"<p>Only retain keys and values for the most recent \\(W\\) tokens:</p> \\[ K_{t-W:t}, V_{t-W:t} \\] <p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>"},{"location":"supporting_topics/kv_caching/#82-kv-cache-quantization","title":"8.2 KV Cache Quantization","text":"<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>"},{"location":"supporting_topics/kv_caching/#what-gets-quantized","title":"What Gets Quantized","text":"<p>Both keys and values can be quantized, but they have different sensitivity:</p> <ul> <li>Keys (K) directly affect attention scores \\(QK^T\\)</li> <li>Values (V) affect the weighted sum after softmax</li> </ul> <p>As a result:</p> <ul> <li>Keys usually require higher precision</li> <li>Values tolerate more aggressive quantization</li> </ul>"},{"location":"supporting_topics/kv_caching/#common-quantization-schemes","title":"Common Quantization Schemes","text":"Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios <p>Mixed precision KV cache is widely used in practice.</p>"},{"location":"supporting_topics/kv_caching/#quantization-granularity","title":"Quantization Granularity","text":"<p>KV cache quantization can be applied at different levels:</p> <ul> <li>Per tensor: One scale for entire K or V tensor</li> <li>Per head: Separate scale per attention head</li> <li>Per channel: Separate scale per head dimension</li> </ul> <p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>"},{"location":"supporting_topics/kv_caching/#dequantization-during-attention","title":"Dequantization During Attention","text":"<p>At decoding step \\(t\\):</p> <ol> <li>Load quantized \\(K, V\\) from cache</li> <li>Dequantize to FP16 or BF16</li> <li>Compute attention normally:</li> </ol> \\[ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Dequantization cost is small compared to memory bandwidth savings.</p>"},{"location":"supporting_topics/kv_caching/#impact-on-performance","title":"Impact on Performance","text":"<p>Benefits:</p> <ul> <li>2x to 4x KV memory reduction</li> <li>Higher batch size and longer context</li> <li>Improved inference throughput due to reduced memory traffic</li> </ul> <p>Tradeoffs:</p> <ul> <li>Slight loss in generation quality</li> <li>Additional dequantization overhead</li> </ul> <p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>"},{"location":"supporting_topics/kv_caching/#interaction-with-other-optimizations","title":"Interaction with Other Optimizations","text":"<ul> <li>GQA further reduces KV cache size and works well with quantization</li> <li>Paged KV cache benefits from smaller KV blocks</li> <li>FlashAttention amortizes dequantization overhead inside fused kernels</li> </ul>"},{"location":"supporting_topics/kv_caching/#83-prefix-caching","title":"8.3 Prefix Caching","text":"<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>"},{"location":"supporting_topics/kv_caching/#84-paged-kv-cache","title":"8.4 Paged KV Cache","text":"<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>"},{"location":"supporting_topics/kv_caching/#9-grouped-query-attention-gqa","title":"\ud83d\udce69. Grouped Query Attention (GQA)","text":"<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>"},{"location":"supporting_topics/kv_caching/#91-head-configuration","title":"9.1 Head Configuration","text":"\\[ H_q &gt; H_k = H_v \\] <p>Example:</p> <ul> <li>Query heads \\(H_q = 32\\)</li> <li>Key value heads \\(H_k = 8\\)</li> </ul> <p>This reduces KV cache memory by a factor of \\(H_q / H_k\\).</p>"},{"location":"supporting_topics/kv_caching/#92-qk-computation-with-mismatched-heads","title":"9.2 QK Computation with Mismatched Heads","text":"<p>Each key value head is shared by a fixed group of query heads.</p> <p>Let:</p> \\[ g = \\frac{H_q}{H_k} \\] <p>Each key value head serves \\(g\\) query heads.</p> <p>For query head \\(i\\), the corresponding key value head index is:</p> \\[ \\left\\lfloor \\frac{i}{g} \\right\\rfloor \\] <p>The attention computation becomes:</p> \\[ \\text{Attn}_i = \\text{softmax}\\left(\\frac{Q_i K_{\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} \\] <p>Keys and values are reused directly without additional projection or averaging.</p>"},{"location":"supporting_topics/kv_caching/#93-why-gqa-is-effective","title":"9.3 Why GQA Is Effective","text":"<ul> <li>Query heads retain expressive power</li> <li>Keys and values capture shared context</li> <li>KV cache size and memory bandwidth are significantly reduced</li> </ul> <p>GQA is widely used in production LLMs.</p>"},{"location":"supporting_topics/kv_caching/#10-other-common-optimizations","title":"\ud83d\udce610. Other Common Optimizations","text":""},{"location":"supporting_topics/kv_caching/#flashattention","title":"FlashAttention","text":"<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>"},{"location":"supporting_topics/kv_caching/#chunked-prefill","title":"Chunked Prefill","text":"<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>"},{"location":"supporting_topics/kv_caching/#speculative-decoding","title":"Speculative Decoding","text":"<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p>"},{"location":"supporting_topics/paged_optimizers/","title":"Paged optimizers","text":""},{"location":"supporting_topics/paged_optimizers/#5-paged-optimizers","title":"\ud83d\udce6 5. Paged Optimizers","text":""},{"location":"supporting_topics/paged_optimizers/#overview","title":"Overview","text":"<p>Paged optimizers are designed to manage memory more efficiently during training by dynamically moving optimizer states between CPU and GPU memory. This approach is particularly useful when dealing with large models that exceed GPU memory capacity.</p>"},{"location":"supporting_topics/paged_optimizers/#mechanism","title":"Mechanism","text":"<ul> <li>Dynamic Memory Management: Optimizer states are stored in CPU memory and paged into GPU memory as needed.</li> <li>Efficient Data Transfer: Minimizes data transfer overhead by batching optimizer state updates.</li> </ul>"},{"location":"supporting_topics/paged_optimizers/#advantages","title":"Advantages","text":"<ul> <li>Reduced GPU Memory Usage: Allows training of larger models on GPUs with limited memory.</li> <li>Scalability: Facilitates scaling to models with billions of parameters.</li> </ul>"},{"location":"supporting_topics/prefix_caching/","title":"Prefix caching","text":""},{"location":"supporting_topics/prefix_caching/#paged-attention","title":"Paged Attention","text":""},{"location":"supporting_topics/speculative_decoding/","title":"Speculative Decoding & Medusa","text":""},{"location":"supporting_topics/speculative_decoding/#1-speculative-decoding-overview","title":"\ud83d\udce61. Speculative Decoding: Overview","text":"<p>Speculative decoding reduces inference latency in decoder-only LLMs while preserving the exact output distribution of a large target model.</p>"},{"location":"supporting_topics/speculative_decoding/#11-why-standard-decoding-is-slow","title":"1.1 Why standard decoding is slow","text":"<p>In standard autoregressive decoding:</p> <ul> <li>The target model generates one token per forward pass</li> <li>The model is large and expensive</li> <li>Latency grows linearly with output length</li> </ul> <p>KV cache reduces computation but does not remove the sequential bottleneck.</p>"},{"location":"supporting_topics/speculative_decoding/#12-core-idea","title":"1.2 Core idea","text":"<p>Speculative decoding separates token proposal from token verification:</p> <ul> <li>A draft model proposes multiple tokens cheaply</li> <li>A target model verifies them efficiently</li> </ul> <p>If most draft tokens are accepted, multiple tokens are generated per expensive target model forward pass.</p>"},{"location":"supporting_topics/speculative_decoding/#2-background-how-decoding-works-in-decoder-only-llms","title":"\ud83d\udce62. Background: How Decoding Works in Decoder-Only LLMs","text":"<p>Before speculative decoding, it is critical to understand standard autoregressive decoding.</p>"},{"location":"supporting_topics/speculative_decoding/#21-autoregressive-modeling-assumption","title":"2.1 Autoregressive modeling assumption","text":"<p>A decoder-only LLM models a sequence of tokens using the following factorization:</p> \\[ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\dots, x_{t-1}) \\] <p>Key points:</p> <ul> <li>Tokens are generated left to right</li> <li>Each new token depends on all previous tokens</li> <li>There is no notion of predicting multiple future tokens independently</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#22-what-happens-in-one-forward-pass","title":"2.2 What happens in one forward pass","text":"<p>Assume the current sequence length is \\(T\\).</p>"},{"location":"supporting_topics/speculative_decoding/#step-1-embedding","title":"Step 1: Embedding","text":"<p>Each token \\(x_t\\) is mapped to a vector representation:</p> \\[ \\mathbf{e}_t = \\text{TokenEmbed}(x_t) + \\text{PosEmbed}(t) \\]"},{"location":"supporting_topics/speculative_decoding/#step-2-masked-self-attention","title":"Step 2: Masked self-attention","text":"<p>For each token position \\(t\\):</p> \\[ \\mathbf{q}_t = \\mathbf{e}_t W_Q,\\quad \\mathbf{k}_t = \\mathbf{e}_t W_K,\\quad \\mathbf{v}_t = \\mathbf{e}_t W_V \\] <p>Attention scores are computed as:</p> \\[ \\alpha_{t,i} = \\frac{\\mathbf{q}_t \\cdot \\mathbf{k}_i}{\\sqrt{d_k}} \\] <p>A causal mask ensures token \\(t\\) can only attend to tokens \\(i \\le t\\).</p> <p>The attended representation is:</p> \\[ \\mathbf{a}_t = \\sum_{i=1}^{t} \\text{softmax}(\\alpha_{t,i}) \\mathbf{v}_i \\] <p>This is followed by a linear projection:</p> \\[ \\mathbf{o}_t = \\mathbf{a}_t W_O \\]"},{"location":"supporting_topics/speculative_decoding/#step-3-feed-forward-network-ffn","title":"Step 3: Feed Forward Network (FFN)","text":"<p>Each token is processed independently:</p> \\[ \\mathbf{h}_t = \\text{FFN}(\\mathbf{o}_t) \\] <p>Residual connections and layer normalization are applied around both attention and FFN blocks.</p> <p>This completes one decoder layer. The same process repeats across multiple stacked layers.</p>"},{"location":"supporting_topics/speculative_decoding/#23-computing-logits-and-selecting-the-next-token","title":"2.3 Computing logits and selecting the next token","text":"<p>After the final decoder layer, each token position has a hidden state \\(\\mathbf{h}_t\\).</p> <p>These are projected to vocabulary logits:</p> \\[ \\mathbf{z}_t = \\mathbf{h}_t W_{\\text{vocab}} \\quad \\text{where } \\mathbf{z}_t \\in \\mathbb{R}^{|V|} \\] <p>Important clarification:</p> <ul> <li>Logits are computed for every token position</li> <li>Softmax is applied over the vocabulary</li> <li>During decoding, only the last position is used</li> </ul> \\[ P(x_{T+1} \\mid x_{\\le T}) = \\text{softmax}(\\mathbf{z}_T) \\] <p>A token is selected using greedy decoding or sampling and appended to the sequence.</p>"},{"location":"supporting_topics/speculative_decoding/#24-autoregressive-decoding-loop","title":"2.4 Autoregressive decoding loop","text":"<p>For each generated token:</p> <ol> <li>Run the Transformer forward pass</li> <li>Take logits from the last token position</li> <li>Apply softmax over the vocabulary</li> <li>Select one token</li> <li>Append it to the sequence</li> <li>Repeat</li> </ol> <p>This process continues until an end-of-sequence token is produced or a maximum length is reached.</p>"},{"location":"supporting_topics/speculative_decoding/#25-key-limitation-of-standard-decoding","title":"2.5 Key limitation of standard decoding","text":"<ul> <li>Each generated token requires a new forward pass of the model</li> <li>Latency grows linearly with output l</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#3-step-by-step-algorithm-for-speculative-decoding","title":"\ud83d\udce63. Step-by-Step Algorithm for Speculative Decoding","text":"<p>This section describes the speculative decoding algorithm precisely, step by step, focusing on what each model does and why it is needed.</p> <p>Assume:</p> <ul> <li>Prompt tokens: \\(x\\)</li> <li>Draft model: \\(q\\)</li> <li>Target model: \\(p\\)</li> <li>Draft length: \\(k\\)</li> <li>Drafted tokens: \\(y_1, y_2, \\dots, y_k\\)</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-1-draft-model-proposes-tokens","title":"Step 1: Draft model proposes tokens","text":"<p>The draft model generates tokens autoregressively, starting from the prompt:</p> \\[ y_i \\sim q(\\cdot \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1 \\dots k \\] <p>Key points:</p> <ul> <li>This step is fast because the draft model is small</li> <li>Tokens are sampled, not greedily selected</li> <li>The draft model also records the probability of each sampled token</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-2-target-model-verifies-the-draft","title":"Step 2: Target model verifies the draft","text":"<p>The target model is run once on the combined sequence: \\([x, y1, y2, ..., yk]\\)</p> <p>This produces target model probabilities:</p> \\[ p(y_i \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1 \\dots k \\] <p>Important clarification:</p> <ul> <li>The target model naturally computes logits for all positions</li> <li>Only logits corresponding to the drafted tokens are used</li> <li>Logits for the prompt tokens are ignored</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#step-3-acceptance-test","title":"Step 3: Acceptance test","text":"<p>Each drafted token is accepted independently using:</p> \\[ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{&lt;i})}{q(y_i \\mid x, y_{&lt;i})}\\right) \\] <p>Procedure:</p> <ol> <li>Sample \\(u \\sim \\text{Uniform}(0, 1)\\)</li> <li>Accept token \\(y_i\\) if \\(u \\le \\alpha_i\\)</li> <li>Stop at the first rejected token</li> </ol>"},{"location":"supporting_topics/speculative_decoding/#step-4-rejection-handling-and-fallback","title":"Step 4: Rejection handling and fallback","text":"<p>If token \\(y_j\\) is rejected:</p> <ul> <li>Tokens \\(y_j, y_{j+1}, \\dots, y_k\\) are discarded</li> <li>The next token is sampled directly from the target model:   $$   x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{&lt;j})   $$</li> <li>Speculative decoding restarts from the new prefix</li> </ul> <p>If no token is rejected, all \\(k\\) draft tokens are accepted.</p>"},{"location":"supporting_topics/speculative_decoding/#why-this-preserves-correctness","title":"Why this preserves correctness","text":"<ul> <li>The acceptance rule implements rejection sampling</li> <li>Bias introduced by the draft model is corrected</li> <li>The final output distribution exactly matches the target model</li> </ul> <p>This guarantees that speculative decoding is statistically equivalent to standard decoding with the target model.</p>"},{"location":"supporting_topics/speculative_decoding/#4-why-speculative-decoding-is-faster","title":"\ud83d\udce64. Why Speculative Decoding Is Faster","text":"<p>Speculative decoding reduces inference latency by decreasing how often the expensive target model must be executed.</p>"},{"location":"supporting_topics/speculative_decoding/#41-standard-decoding-vs-speculative-decoding","title":"4.1 Standard decoding vs speculative decoding","text":"<p>Standard decoding</p> <ul> <li>One target model forward pass per generated token</li> <li>For \\(N\\) tokens, \\(N\\) forward passes are required</li> </ul> <p>Speculative decoding</p> <ul> <li>The draft model proposes \\(k\\) tokens cheaply</li> <li>A single target model forward pass verifies up to \\(k\\) tokens</li> <li>Multiple tokens can be generated per target model invocation</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#42-source-of-the-speedup","title":"4.2 Source of the speedup","text":"<p>The speedup comes from two properties:</p> <ul> <li>The draft model is significantly cheaper than the target model</li> <li>The target model can evaluate multiple draft tokens in parallel</li> </ul> <p>If the acceptance rate is high, the target model is called much less frequently.</p>"},{"location":"supporting_topics/speculative_decoding/#43-what-speculative-decoding-does-not-optimize","title":"4.3 What speculative decoding does not optimize","text":"<p>Speculative decoding does not reduce:</p> <ul> <li>The total number of FLOPs in the target model</li> <li>The per-token computation inside the Transformer</li> </ul> <p>It primarily reduces latency, not theoretical compute.</p>"},{"location":"supporting_topics/speculative_decoding/#44-practical-speedups","title":"4.4 Practical speedups","text":"<p>In practice, speculative decoding often achieves:</p> <ul> <li>1.5x to 3x latency improvement</li> <li>Higher gains when draft and target distributions are close</li> </ul> <p>Actual speedup depends on model sizes, hardware, and acceptance rate.</p>"},{"location":"supporting_topics/speculative_decoding/#5-relationship-to-logits-for-all-tokens","title":"5. Relationship to Logits for All Tokens","text":"<p>Speculative decoding does not introduce a new requirement to compute logits for all tokens.</p>"},{"location":"supporting_topics/speculative_decoding/#51-standard-transformer-behavior","title":"5.1 Standard Transformer behavior","text":"<p>A Transformer forward pass naturally produces:</p> <ul> <li>One hidden state per token position</li> <li>One vocabulary logits vector per token position</li> </ul> <p>This is true during both training and inference.</p>"},{"location":"supporting_topics/speculative_decoding/#52-how-logits-are-used-in-speculative-decoding","title":"5.2 How logits are used in speculative decoding","text":"<p>During verification:</p> <ul> <li>The target model is run once on the prompt plus draft tokens</li> <li>Logits for prompt tokens are ignored</li> <li>Only logits corresponding to the draft token positions are used</li> </ul> <p>Speculative decoding simply reuses standard per-position logits.</p>"},{"location":"supporting_topics/speculative_decoding/#53-what-speculative-decoding-does-not-do","title":"5.3 What speculative decoding does not do","text":"<p>Speculative decoding does not:</p> <ul> <li>Perform softmax over token positions</li> <li>Predict future tokens independently</li> <li>Generate tokens in parallel from the target model</li> </ul> <p>The target model still defines an autoregressive distribution.</p>"},{"location":"supporting_topics/speculative_decoding/#6-interaction-with-kv-cache","title":"6. Interaction with KV Cache","text":"<p>KV cache improves performance in speculative decoding but does not change the algorithm.</p>"},{"location":"supporting_topics/speculative_decoding/#61-kv-cache-in-the-draft-model","title":"6.1 KV cache in the draft model","text":"<ul> <li>The draft model maintains its own KV cache</li> <li>Draft tokens are generated autoregressively</li> <li>KV cache allows fast token proposal</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#62-kv-cache-in-the-target-model","title":"6.2 KV cache in the target model","text":"<ul> <li>The target model computes KV cache for the entire speculative window</li> <li>KV states corresponding to accepted tokens are reused</li> <li>KV states for rejected tokens are discarded</li> </ul>"},{"location":"supporting_topics/speculative_decoding/#63-why-kv-cache-matters","title":"6.3 Why KV cache matters","text":"<p>KV cache:</p> <ul> <li>Avoids recomputing attention for previously processed tokens</li> <li>Reduces per-step computation</li> <li>Improves practical throughput</li> </ul> <p>KV cache affects efficiency only, not correctness.</p>"},{"location":"training_techniques/foundation/","title":"Foundation","text":""},{"location":"training_techniques/foundation/#1-what-is-an-llm-optimizing","title":"1 What is an LLM Optimizing?","text":"<p>At its core, a Large Language Model (LLM) is a probabilistic system designed to model the distribution of natural language. Despite emergent reasoning and planning behaviors, the training objective itself is simple: reduce uncertainty about the next token given prior context.</p>"},{"location":"training_techniques/foundation/#11-the-autoregressive-objective","title":"1.1 The Autoregressive Objective","text":"<p>Most LLMs are trained using an Autoregressive (AR) or Causal Language Modeling (CLM) objective. The joint probability of a token sequence is factorized as a product of conditional probabilities:</p> \\[ P(x_1, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{&lt;t}) \\] <p>This formulation assumes:</p> <ul> <li>Tokens are conditionally independent given their history</li> <li>Knowledge is captured implicitly through long context dependencies</li> <li>Tokenization defines the atomic units of prediction</li> </ul>"},{"location":"training_techniques/foundation/#12-the-loss-function-negative-log-likelihood","title":"1.2 The Loss Function: Negative Log-Likelihood","text":"<p>Training minimizes the Negative Log-Likelihood (NLL) of the observed data, which is equivalent to Cross-Entropy Loss:</p> \\[ \\mathcal{L}(\\theta) = - \\mathbb{E}_{(x_1,\\dots,x_T) \\sim D} \\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{&lt;t}) \\] <p>Where: - \\(\\theta\\) are the model parameters - \\(x_{&lt;t}\\) is the context window - \\(P_\\theta\\) is the predicted probability distribution produced by a Softmax layer</p>"},{"location":"training_techniques/foundation/#13-cross-entropy-kl-divergence-and-learning","title":"1.3 Cross-Entropy, KL Divergence, and Learning","text":"<p>Minimizing cross-entropy implicitly minimizes the KL divergence between the true data distribution \\(P_{data}\\) and the model distribution \\(P_\\theta\\):</p> \\[ H(P_{data}, P_\\theta) = H(P_{data}) + D_{KL}(P_{data} \\| P_\\theta) \\] <p>Since \\(H(P_{data})\\) is fixed, training pushes the model distribution closer to the real language distribution. This connects LLM training directly to information theory and compression.</p>"},{"location":"training_techniques/foundation/#14-why-next-token-prediction-works-the-compression-hypothesis","title":"1.4 Why Next-Token Prediction Works (The Compression Hypothesis)","text":"<p>Predicting the next token forces the model to internalize structure across many domains.</p> <ul> <li>Compression implies abstraction: To compress diverse text, the model must learn syntax, semantics, facts, and procedures.</li> <li>Softmax competition: Increasing probability mass for the correct token necessarily decreases mass for alternatives, encouraging fine-grained representations.</li> <li>Generalization pressure: Predicting well across domains requires reusable internal features, which later appear as reasoning, translation, or coding skills.</li> </ul>"},{"location":"training_techniques/foundation/#2-language-modeling-and-likelihood-minimization","title":"2 Language Modeling and Likelihood Minimization","text":""},{"location":"training_techniques/foundation/#21-maximum-likelihood-estimation-mle","title":"2.1 Maximum Likelihood Estimation (MLE)","text":"<p>LLM training is an instance of Maximum Likelihood Estimation, where we seek parameters \\(\\theta^*\\) that maximize the likelihood of observed text:</p> \\[ \\theta^* = \\arg\\max_\\theta \\mathbb{E}_{x \\sim D} [\\log P_\\theta(x)] \\] <p>MLE provides a stable and scalable objective but does not encode preferences such as helpfulness, safety, or instruction following.</p>"},{"location":"training_techniques/foundation/#22-teacher-forcing","title":"2.2 Teacher Forcing","text":"<p>Teacher forcing is a training strategy where the model is conditioned on the ground-truth previous token rather than its own prediction when computing the next-token loss. For a sequence \\(x_1, \\dots, x_T\\), the model always receives \\(x_{t-1}\\) as input when predicting \\(x_t\\), even if it would have predicted a different token at step \\(t-1\\).</p>"},{"location":"training_techniques/foundation/#teacher-forcing-small-practical-example","title":"Teacher Forcing: Small Practical Example","text":"<p>Task: Predict the sentence  </p> <p>\u201cThe cat sat on the mat\u201d</p>"},{"location":"training_techniques/foundation/#step-1-training-data","title":"Step 1: Training data","text":"<p>Tokens: <code>[The, cat, sat, on, the, mat]</code></p>"},{"location":"training_techniques/foundation/#step-2-training-with-teacher-forcing","title":"Step 2: Training with teacher forcing","text":"<p>At each step, the model is conditioned on the ground-truth previous token.</p> <ol> <li> <p>Input: <code>The</code> Target: <code>cat</code></p> </li> <li> <p>Input: <code>The cat</code> Target: <code>sat</code></p> </li> <li> <p>Input: <code>The cat sat</code> Target: <code>on</code></p> </li> <li> <p>Input: <code>The cat sat on</code> Target: <code>the</code></p> </li> <li> <p>Input: <code>The cat sat on the</code> Target: <code>mat</code></p> </li> </ol> <p>Even if the model predicts an incorrect token at any step, the next input still uses the true token from the dataset.</p>"},{"location":"training_techniques/foundation/#step-3-inference-time-behavior","title":"Step 3: Inference time behavior","text":"<p>At inference, the model must condition on its own predictions.</p> <ol> <li> <p>Input: <code>The</code> Prediction: <code>dog</code></p> </li> <li> <p>Next input: <code>The dog</code>    Errors now propagate forward</p> </li> </ol> <p>This train\u2013test mismatch is known as exposure bias and is a key limitation of teacher forcing.</p> <p>This allows:</p> <ul> <li>Full parallelization of loss computation across all positions in a Transformer</li> <li>Stable gradients, since errors do not compound during training</li> </ul> <p>However, teacher forcing introduces exposure bias: during inference, the model must condition on its own past predictions, a distribution shift that can lead to error accumulation. This gap motivates post-training techniques such as supervised fine-tuning and reinforcement learning based alignment.</p> <p>Advantages:</p> <ul> <li>Enables parallel computation in Transformers</li> <li>Stabilizes training and accelerates convergence</li> </ul> <p>Limitation: Exposure Bias</p> <ul> <li>At inference time, the model conditions on its own predictions</li> <li>Errors can compound, leading to drift</li> <li>This motivates post-training methods like SFT, RLHF, and DPO</li> </ul>"},{"location":"training_techniques/foundation/#23-perplexity-as-an-evaluation-metric","title":"2.3 Perplexity as an Evaluation Metric","text":"<p>Perplexity (PPL) is defined as:</p> \\[ \\text{PPL} = \\exp(\\mathcal{L}) \\] <p>Interpretation:</p> <ul> <li>PPL approximates the average branching factor</li> <li>Lower PPL means lower uncertainty about future tokens</li> </ul> <p>Limitations:</p> <ul> <li>PPL correlates weakly with reasoning, factuality, or alignment</li> <li>Two models with similar PPL can differ significantly in downstream capability</li> </ul>"},{"location":"training_techniques/foundation/#13-why-scaling-works-and-where-it-breaks","title":"1.3 Why Scaling Works and Where it Breaks","text":""},{"location":"training_techniques/foundation/#empirical-scaling-laws","title":"Empirical Scaling Laws","text":"<p>Performance improves predictably as a power-law function of: - Model parameters - Training tokens - Compute budget</p> <p>This empirical behavior explains the rapid gains from larger models.</p>"},{"location":"training_techniques/foundation/#kaplan-scaling-laws-2020","title":"Kaplan Scaling Laws (2020)","text":"<p>Early results suggested scaling model size was the dominant factor.</p> <ul> <li>Loss scales roughly as a power-law in parameter count</li> <li>Data was treated as effectively unlimited</li> </ul>"},{"location":"training_techniques/foundation/#chinchilla-scaling-laws-2022","title":"Chinchilla Scaling Laws (2022)","text":"<p>Later work showed most large models were undertrained.</p> <p>Key findings: - Optimal performance requires balancing parameters and tokens - Roughly 20 training tokens per parameter is compute optimal - Smaller models trained on more data can outperform larger undertrained models</p> <p>This led to data-centric model design such as LLaMA and Mistral.</p>"},{"location":"training_techniques/foundation/#where-scaling-breaks","title":"Where Scaling Breaks","text":""},{"location":"training_techniques/foundation/#1-data-scarcity-and-synthetic-feedback-loops","title":"1. Data Scarcity and Synthetic Feedback Loops","text":"<ul> <li>High-quality human text is limited</li> <li>Synthetic data risks reducing diversity</li> <li>Repeated self-training can lead to model collapse</li> </ul>"},{"location":"training_techniques/foundation/#2-capability-saturation","title":"2. Capability Saturation","text":"<ul> <li>Loss improves smoothly, but abilities emerge discontinuously</li> <li>Reasoning, planning, and tool use do not scale linearly with perplexity</li> <li>Small loss gains can hide large behavioral differences</li> </ul>"},{"location":"training_techniques/foundation/#3-inference-cost-and-latency","title":"3. Inference Cost and Latency","text":"<ul> <li>Larger models increase memory, latency, and cost</li> <li>This motivates inference-efficient designs</li> </ul>"},{"location":"training_techniques/foundation/#4-test-time-scaling","title":"4. Test-Time Scaling","text":"<ul> <li>Recent systems scale inference compute rather than parameters</li> <li>Models generate longer internal reasoning traces</li> <li>This shifts scaling from training time to inference time</li> </ul> <p>Examples include OpenAI o1 and DeepSeek-R1.</p>"},{"location":"training_techniques/foundation/#14-interview-cheatsheet","title":"1.4 Interview Cheatsheet","text":"Concept Explanation Interview Signal Cross-Entropy Optimizes token probability matching Core training objective KL Divergence Distance to true language distribution Shows theory depth Teacher Forcing Parallelizable training strategy Leads to exposure bias Chinchilla Optimality Tokens proportional to parameters Compute awareness Emergence Capabilities not directly trained Explains scaling rationale Context Window Maximum visible history Limits memory and reasoning"},{"location":"training_techniques/foundation/#key-intuition-resource","title":"Key Intuition Resource","text":"<p>Generative AI explained</p> <p>This visualization explains why next-token prediction is sufficient to model complex distributions across text, images, and audio.</p>"},{"location":"training_techniques/foundation/#next-section","title":"Next Section","text":"<p>2. Architecture and Attention Mechanisms Multi-Head Attention, positional encodings, KV caching, and why Transformers scale.</p>"}]}