The **Unigram Language Model** is a probabilistic, top-down subword tokenization algorithm. While often associated with the **SentencePiece** library (the framework for lossless, language-agnostic tokenization), Unigram itself is a distinct algorithm used by models like **T5, XLNet, ALBERT, and many modern multilingual LLMs**.

---

## 1. The Probabilistic Framework

Unlike BPE, which is a greedy heuristic, Unigram treats tokenization as a statistical inference problem. It assumes that a sequence is generated by independently sampling tokens from a vocabulary.

### The Objective Function
Given a sentence $S$ and a candidate segmentation $\mathbf{x} = \{t_1, t_2, \dots, t_k\}$, the model calculates the probability of that segmentation as:

$$P(\mathbf{x}) = \prod_{i=1}^{k} P(t_i)$$

The goal of the tokenizer is to find the segmentation that **maximizes this likelihood**:

$$\mathbf{x}^* = \arg\max_{\mathbf{x} \in \mathcal{S}} \sum_{i=1}^{k} \log P(t_i)$$

Where $\mathcal{S}$ is the set of all possible segmentations. This is efficiently solved using **Viterbi decoding** (Dynamic Programming) in $O(N^2)$ time relative to string length.

---

## 2. Training: The EM-Style Pruning Process

Unigram training is a "top-down" approach. Instead of merging small units (like BPE), it starts with a massive vocabulary and prunes it down.

1.  **Initialization**: Start with a very large vocabulary (e.g., every character in the corpus plus the most frequent substrings).
2.  **Expectation (E-step)**: Given the current vocabulary and token probabilities, find the optimal (Viterbi) segmentation for every sentence in the corpus.
3.  **Maximization (M-step)**: Update the token probabilities $P(t_i)$ based on their frequency in the newly computed optimal segmentations.
4.  **Pruning**:
    * For each token, calculate the **loss**: how much the total corpus likelihood would decrease if that token were removed from the vocabulary.
    * Discard the bottom $X\%$ of tokens with the lowest loss.
    * *Note: To ensure every string remains segmentable, single characters are never pruned.*
5.  **Iteration**: Repeat until the target vocabulary size is reached.

---

## 3. Small Intuitive Example

Assume the vocabulary contains the following tokens with probabilities:

| Token | Probability |
|-----|-------------|
| `low` | 0.30 |
| `lower` | 0.25 |
| `er` | 0.20 |
| `l` | 0.05 |
| `o` | 0.05 |
| `w` | 0.05 |

Input word: `lower`

### Possible Segmentations

1. `lower`  
    $P = 0.25$
   

2. `low + er`
   $P = 0.30 \times 0.20 = 0.06$
   

3. `l + o + w + er`  
   $P = 0.05^3 \times 0.20 = 0.00025$

### Selected Tokenization

The tokenizer selects `lower` because it maximizes the likelihood.

---

## 4. The SentencePiece "Secret Sauce"

SentencePiece is the implementation wrapper that adds critical features for modern LLMs:

* **Lossless Reversibility**: It replaces spaces with a meta-symbol (usually `_`). Because the space is treated as a standard character, the original string can be reconstructed perfectly (detokenized) without complex rules.
* **Byte-Fallback**: If the model encounters a character not in its vocabulary, it falls back to **UTF-8 bytes**. This effectively eliminates the "Unknown Token" (`UNK`) problem.
* **Pre-tokenization Independence**: It does not require splitting text by spaces first. This makes it natively compatible with non-segmented languages like Chinese, Japanese, or Thai.

---

## 5. Subword Regularization (Sampling)

This is a major advantage for model robustness. During training, instead of always using the "best" (Viterbi) segmentation, we sample different segmentations based on their probabilities:

$$P(\mathbf{x}) = \frac{P(\mathbf{x})^\alpha}{\sum_{\mathbf{x}' \in \mathcal{S}} P(\mathbf{x}')^\alpha}$$

By exposing the model to multiple ways of segmenting the same word (e.g., `higher` as `high + er` vs `h + igher`), the model becomes more robust to spelling variations and noise.

---

## 6. Comparison: BPE vs. Unigram

| Feature | BPE (Byte-Pair Encoding) | Unigram (SentencePiece) |
| :--- | :--- | :--- |
| **Logic** | Bottom-up (Iterative Merging) | Top-down (Iterative Pruning) |
| **Philosophy** | Frequency-based heuristic | Probabilistic Likelihood |
| **Optimality** | Greedy (not globally optimal) | Global Optimum (via Viterbi) |
| **Regularization** | Difficult to implement | Natively supports subword sampling |
| **Use Case** | GPT-family, Llama, RoBERTa | T5, ALBERT, Multilingual models |

---

## 6. Practical Limitations

* **Complexity**: Viterbi decoding is more computationally expensive than the greedy merges of BPE.
* **Independence Assumption**: It assumes tokens are independent, ignoring the linguistic context in which a subword appears.
* **Training Time**: The EM pruning process on a massive corpus is generally slower than BPE's initial merge-counting.

---

## 7. Summary
SentencePiece Unigram provides a **principled, probabilistic objective** for tokenization. Its ability to provide **globally optimal segmentations**, support **subword regularization**, and maintain **lossless reversibility** makes it a preferred choice for high-performance, multilingual Large Language Models.
