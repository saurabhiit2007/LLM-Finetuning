
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../../phases/sft/">
      
      
        <link rel="next" href="../fsdp_deepspeed/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Distributed Training System - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-data-parallelism-dp" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Distributed Training System
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Phases
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Phases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../phases/pre_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pre-Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../phases/mid_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mid-Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../phases/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Post-Training
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Distributed Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Distributed Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Distributed Training System
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Distributed Training System
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-data-parallelism-dp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Data Parallelism (DP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Data Parallelism (DP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Core Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-distributed-data-parallel-ddp-details" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch Distributed Data Parallel (DDP) Details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch Distributed Data Parallel (DDP) Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-bucketing" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Bucketing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#asynchronous-gradient-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      Asynchronous Gradient Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#synchronization-timing" class="md-nav__link">
    <span class="md-ellipsis">
      Synchronization Timing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-tensor-parallelism-tp" class="md-nav__link">
    <span class="md-ellipsis">
      2. Tensor Parallelism (TP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Tensor Parallelism (TP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-core-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Core Mechanism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Core Mechanism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#column-parallelism-output-dimension-split" class="md-nav__link">
    <span class="md-ellipsis">
      Column Parallelism (Output Dimension Split)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#row-parallelism-input-dimension-split" class="md-nav__link">
    <span class="md-ellipsis">
      Row Parallelism (Input Dimension Split)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-two-parallelization-schemes-exist" class="md-nav__link">
    <span class="md-ellipsis">
      Why Two Parallelization Schemes Exist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-communication-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Communication Characteristics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#insights" class="md-nav__link">
    <span class="md-ellipsis">
      Insights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-pipeline-parallelism-pp" class="md-nav__link">
    <span class="md-ellipsis">
      3. Pipeline Parallelism (PP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Pipeline Parallelism (PP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-mechanism_1" class="md-nav__link">
    <span class="md-ellipsis">
      Core Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-pipeline-bubble" class="md-nav__link">
    <span class="md-ellipsis">
      The Pipeline Bubble
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#micro-batching" class="md-nav__link">
    <span class="md-ellipsis">
      Micro Batching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#insights_1" class="md-nav__link">
    <span class="md-ellipsis">
      Insights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-zero-zero-redundancy-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      4. ZeRO (Zero Redundancy Optimizer)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. ZeRO (Zero Redundancy Optimizer)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-stages" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO Stages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-insight" class="md-nav__link">
    <span class="md-ellipsis">
      Key Insight
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-offload" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO Offload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Patterns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-communication-and-memory-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      5. Communication and Memory Trade-offs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Communication and Memory Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-the-memorycomputecommunication-triangle" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 The Memory–Compute–Communication Triangle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-memory-accounting-with-adam-and-fp16" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Memory Accounting with Adam and FP16
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Memory Accounting with Adam and FP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Storage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Storage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-states" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer States
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#total-training-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Total Training Memory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concrete-example" class="md-nav__link">
    <span class="md-ellipsis">
      Concrete Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-why-communication-becomes-the-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Why Communication Becomes the Bottleneck
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-memory-reduction-techniques-and-their-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Memory Reduction Techniques and Their Trade-offs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.4 Memory Reduction Techniques and Their Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#activation-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-sharding" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Sharding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-checkpointing-and-fault-tolerance" class="md-nav__link">
    <span class="md-ellipsis">
      6. Checkpointing and Fault Tolerance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Checkpointing and Fault Tolerance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-checkpointing-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Checkpointing Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-fault-tolerance" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 Fault Tolerance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-advanced-parallelism-and-optimization-topics" class="md-nav__link">
    <span class="md-ellipsis">
      7. Advanced Parallelism and Optimization Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Advanced Parallelism and Optimization Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-mixture-of-experts-moe" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Mixture of Experts (MoE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-sequence-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 Sequence Parallelism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.3 Sequence Parallelism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-idea" class="md-nav__link">
    <span class="md-ellipsis">
      Core Idea
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-is-useful" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Is Useful
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-pattern" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Pattern
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relationship-with-tensor-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Relationship with Tensor Parallelism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-low-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      7.4 Low Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-additional-common-interview-topics" class="md-nav__link">
    <span class="md-ellipsis">
      8. Additional Common Interview Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Additional Common Interview Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-hybrid-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Hybrid Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-throughput-vs-latency" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Throughput vs Latency
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.2 Throughput vs Latency">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-throughput-oriented" class="md-nav__link">
    <span class="md-ellipsis">
      Training: Throughput Oriented
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference-latency-oriented" class="md-nav__link">
    <span class="md-ellipsis">
      Inference: Latency Oriented
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-parallelism-strategies-differ" class="md-nav__link">
    <span class="md-ellipsis">
      Why Parallelism Strategies Differ
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Gradient Accumulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.3 Gradient Accumulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-idea_1" class="md-nav__link">
    <span class="md-ellipsis">
      Core Idea
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-saves-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Saves Memory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-benefits-in-distributed-training" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Benefits in Distributed Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-confusion-with-data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Common Confusion with Data Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fsdp_deepspeed/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FSDP & DeepSpeed
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../parameter_efficient_fine_tuning/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../parameter_efficient_fine_tuning/qlora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../parameter_efficient_fine_tuning/adaptors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adaptors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../parameter_efficient_fine_tuning/prefix_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prefix Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/4bit_normal_float/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4-bit NormalFloat (NF4)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/blockwise_kbit_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Block-wise k-bit Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/accelerate.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerate Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/bp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BP16
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/memory_considerations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Consideration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/paged_adam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paged Adam Optimizer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/training_optimization_and_stability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Optimization and Stability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/prefix_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prefix Caching
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Training Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/foundation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Foundation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_techniques/thinking_llms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Reasoning & Thinking Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-data-parallelism-dp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Data Parallelism (DP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Data Parallelism (DP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Core Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-distributed-data-parallel-ddp-details" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch Distributed Data Parallel (DDP) Details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PyTorch Distributed Data Parallel (DDP) Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-bucketing" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Bucketing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#asynchronous-gradient-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      Asynchronous Gradient Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#synchronization-timing" class="md-nav__link">
    <span class="md-ellipsis">
      Synchronization Timing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-tensor-parallelism-tp" class="md-nav__link">
    <span class="md-ellipsis">
      2. Tensor Parallelism (TP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Tensor Parallelism (TP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-core-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Core Mechanism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Core Mechanism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#column-parallelism-output-dimension-split" class="md-nav__link">
    <span class="md-ellipsis">
      Column Parallelism (Output Dimension Split)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#row-parallelism-input-dimension-split" class="md-nav__link">
    <span class="md-ellipsis">
      Row Parallelism (Input Dimension Split)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-two-parallelization-schemes-exist" class="md-nav__link">
    <span class="md-ellipsis">
      Why Two Parallelization Schemes Exist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-communication-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Communication Characteristics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#insights" class="md-nav__link">
    <span class="md-ellipsis">
      Insights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-pipeline-parallelism-pp" class="md-nav__link">
    <span class="md-ellipsis">
      3. Pipeline Parallelism (PP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Pipeline Parallelism (PP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-mechanism_1" class="md-nav__link">
    <span class="md-ellipsis">
      Core Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-pipeline-bubble" class="md-nav__link">
    <span class="md-ellipsis">
      The Pipeline Bubble
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#micro-batching" class="md-nav__link">
    <span class="md-ellipsis">
      Micro Batching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#insights_1" class="md-nav__link">
    <span class="md-ellipsis">
      Insights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-zero-zero-redundancy-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      4. ZeRO (Zero Redundancy Optimizer)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. ZeRO (Zero Redundancy Optimizer)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-stages" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO Stages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-insight" class="md-nav__link">
    <span class="md-ellipsis">
      Key Insight
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-offload" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO Offload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Patterns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-communication-and-memory-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      5. Communication and Memory Trade-offs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Communication and Memory Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-the-memorycomputecommunication-triangle" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 The Memory–Compute–Communication Triangle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-memory-accounting-with-adam-and-fp16" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Memory Accounting with Adam and FP16
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Memory Accounting with Adam and FP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Storage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-storage" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Storage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-states" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer States
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#total-training-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Total Training Memory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concrete-example" class="md-nav__link">
    <span class="md-ellipsis">
      Concrete Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-why-communication-becomes-the-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Why Communication Becomes the Bottleneck
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-memory-reduction-techniques-and-their-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Memory Reduction Techniques and Their Trade-offs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.4 Memory Reduction Techniques and Their Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#activation-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-sharding" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Sharding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-checkpointing-and-fault-tolerance" class="md-nav__link">
    <span class="md-ellipsis">
      6. Checkpointing and Fault Tolerance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Checkpointing and Fault Tolerance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-checkpointing-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Checkpointing Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-fault-tolerance" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 Fault Tolerance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-advanced-parallelism-and-optimization-topics" class="md-nav__link">
    <span class="md-ellipsis">
      7. Advanced Parallelism and Optimization Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Advanced Parallelism and Optimization Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-mixture-of-experts-moe" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Mixture of Experts (MoE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-sequence-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 Sequence Parallelism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.3 Sequence Parallelism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-idea" class="md-nav__link">
    <span class="md-ellipsis">
      Core Idea
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-is-useful" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Is Useful
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-pattern" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Pattern
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relationship-with-tensor-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Relationship with Tensor Parallelism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-low-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      7.4 Low Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-additional-common-interview-topics" class="md-nav__link">
    <span class="md-ellipsis">
      8. Additional Common Interview Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Additional Common Interview Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-hybrid-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Hybrid Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-throughput-vs-latency" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Throughput vs Latency
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.2 Throughput vs Latency">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-throughput-oriented" class="md-nav__link">
    <span class="md-ellipsis">
      Training: Throughput Oriented
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference-latency-oriented" class="md-nav__link">
    <span class="md-ellipsis">
      Inference: Latency Oriented
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-parallelism-strategies-differ" class="md-nav__link">
    <span class="md-ellipsis">
      Why Parallelism Strategies Differ
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Gradient Accumulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.3 Gradient Accumulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-idea_1" class="md-nav__link">
    <span class="md-ellipsis">
      Core Idea
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-saves-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Why It Saves Memory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-benefits-in-distributed-training" class="md-nav__link">
    <span class="md-ellipsis">
      Communication Benefits in Distributed Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-confusion-with-data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Common Confusion with Data Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Distributed Training System</h1>

<p>This page covers the system architecture, parallelism strategies, and engineering trade offs required to train large language models at scale.</p>
<hr />
<h2 id="1-data-parallelism-dp">1. Data Parallelism (DP)<a class="headerlink" href="#1-data-parallelism-dp" title="Permanent link">&para;</a></h2>
<p>The most widely used baseline parallelism strategy where the full model is replicated across <span class="arithmatex">\(N\)</span> workers and each worker processes a different shard of the input batch.</p>
<h3 id="core-mechanism">Core Mechanism<a class="headerlink" href="#core-mechanism" title="Permanent link">&para;</a></h3>
<ul>
<li>Each GPU performs forward and backward passes on its local mini batch.</li>
<li>Gradients are synchronized across all workers using All Reduce.</li>
<li>After synchronization, every replica applies the same optimizer update, keeping parameters identical.</li>
</ul>
<h3 id="pytorch-distributed-data-parallel-ddp-details">PyTorch Distributed Data Parallel (DDP) Details<a class="headerlink" href="#pytorch-distributed-data-parallel-ddp-details" title="Permanent link">&para;</a></h3>
<h4 id="gradient-bucketing">Gradient Bucketing<a class="headerlink" href="#gradient-bucketing" title="Permanent link">&para;</a></h4>
<ul>
<li>Model parameters are grouped into <strong>buckets</strong> based on size (The default bucket size is about 25 MB, configurable via bucket_cap_mb).</li>
<li>Instead of waiting for all gradients to be computed, DDP starts communication as soon as a bucket is ready.</li>
<li>This reduces idle time by avoiding one large synchronization at the end of backward pass.</li>
</ul>
<p><strong>Why this matters:</strong><br />
Smaller, incremental communication keeps GPUs busy and reduces the impact of network latency.</p>
<h4 id="asynchronous-gradient-reduction">Asynchronous Gradient Reduction<a class="headerlink" href="#asynchronous-gradient-reduction" title="Permanent link">&para;</a></h4>
<ul>
<li>When gradients for a bucket are computed, DDP launches an <strong>asynchronous All Reduce</strong> operation.</li>
<li>While communication is happening, the backward pass continues computing gradients for later layers.</li>
<li>This overlaps computation and communication.</li>
</ul>
<p><strong>Key insight:</strong><br />
Ideally, by the time backward computation finishes, most gradient communication is already done.</p>
<h4 id="synchronization-timing">Synchronization Timing<a class="headerlink" href="#synchronization-timing" title="Permanent link">&para;</a></h4>
<ul>
<li>Gradients are synchronized <strong>once per training iteration</strong>, not per layer.</li>
<li>Each parameter’s gradient is reduced exactly once after it is computed.</li>
<li>The optimizer step is performed only after all gradient reductions complete.</li>
</ul>
<p><strong>Common misconception:</strong><br />
DDP does not pause after every layer to synchronize. Synchronization is event driven and overlaps with backpropagation.</p>
<h3 id="limitations"><strong>Limitations</strong><a class="headerlink" href="#limitations" title="Permanent link">&para;</a></h3>
<ul>
<li>Model parameters, gradients, and optimizer states must fit on a single GPU.</li>
<li>Scaling is limited by global batch size and optimizer stability.</li>
<li>Communication cost grows linearly with the number of GPUs.</li>
</ul>
<hr />
<h2 id="2-tensor-parallelism-tp">2. Tensor Parallelism (TP)<a class="headerlink" href="#2-tensor-parallelism-tp" title="Permanent link">&para;</a></h2>
<p>Tensor Parallelism splits individual layers across devices so that a single layer computation is distributed.</p>
<h3 id="21-core-mechanism">2.1 Core Mechanism<a class="headerlink" href="#21-core-mechanism" title="Permanent link">&para;</a></h3>
<p>Large weight matrices are partitioned across GPUs.</p>
<p>Consider a linear layer:</p>
<p>Y = XW</p>
<p>where:</p>
<ul>
<li>X has shape <code>[batch, hidden_in]</code></li>
<li>W has shape <code>[hidden_in, hidden_out]</code></li>
<li>Y has shape <code>[batch, hidden_out]</code></li>
<li>Column parallel splits <code>W</code> by output dimension.</li>
<li>Row parallel splits <code>W</code> by input dimension.</li>
<li>Each GPU computes a partial result which is combined using collective communication.</li>
</ul>
<hr />
<h4 id="column-parallelism-output-dimension-split">Column Parallelism (Output Dimension Split)<a class="headerlink" href="#column-parallelism-output-dimension-split" title="Permanent link">&para;</a></h4>
<p>In column parallelism, the weight matrix W is split <strong>by columns</strong>:</p>
<p>W = [W₁ | W₂ | ... | Wₖ]</p>
<p>Each GPU holds:</p>
<ul>
<li>Wᵢ with shape <code>[hidden_in, hidden_out / k]</code></li>
</ul>
<p>Each GPU computes:</p>
<ul>
<li>Yᵢ = X · Wᵢ</li>
</ul>
<p>At this point:</p>
<ul>
<li>Each Yᵢ is only a <strong>partial output</strong>.</li>
<li>The full output Y is formed by concatenating all Yᵢ along the feature dimension.</li>
</ul>
<p><strong>Communication Pattern</strong></p>
<ul>
<li>An All Gather is used to assemble the full Y across GPUs.</li>
<li>This happens during the forward pass.</li>
<li>During backpropagation, gradients w.r.t. X require an All Reduce.</li>
</ul>
<p><strong>Key Insight</strong></p>
<p>Column parallelism parallelizes the <strong>output features</strong> and is commonly used in feed forward layers.</p>
<hr />
<h4 id="row-parallelism-input-dimension-split">Row Parallelism (Input Dimension Split)<a class="headerlink" href="#row-parallelism-input-dimension-split" title="Permanent link">&para;</a></h4>
<p>In row parallelism, the weight matrix W is split <strong>by rows</strong>:</p>
<p>W = [ W₁
      W₂
      ...
      Wₖ ]</p>
<p>Each GPU holds:</p>
<ul>
<li>Wᵢ with shape <code>[hidden_in / k, hidden_out]</code></li>
</ul>
<p>The input X is also split:</p>
<ul>
<li>X = [X₁ | X₂ | ... | Xₖ]</li>
</ul>
<p>Each GPU computes:</p>
<ul>
<li>Yᵢ = Xᵢ · Wᵢ</li>
</ul>
<p>Now:</p>
<ul>
<li>Each Yᵢ is a <strong>partial sum</strong> of the final output.</li>
</ul>
<p><strong>Communication Pattern</strong></p>
<ul>
<li>An All Reduce is required to sum Yᵢ across GPUs.</li>
<li>The final Y is identical on all GPUs after reduction.</li>
<li>Backward pass mirrors this communication pattern.</li>
</ul>
<p><strong>Key Insight</strong></p>
<p>Row parallelism parallelizes the <strong>input features</strong> and avoids an All Gather in the forward pass.</p>
<hr />
<h4 id="why-two-parallelization-schemes-exist">Why Two Parallelization Schemes Exist<a class="headerlink" href="#why-two-parallelization-schemes-exist" title="Permanent link">&para;</a></h4>
<p>Column and row parallelism are complementary:
- Column parallelism produces partial outputs that must be gathered.
- Row parallelism produces partial sums that must be reduced.</p>
<p>Modern transformer implementations alternate between them:
- Column parallel for linear projections.
- Row parallel for output projections.</p>
<p>This design minimizes total communication while keeping memory balanced.</p>
<hr />
<h3 id="22-communication-characteristics">2.2 Communication Characteristics<a class="headerlink" href="#22-communication-characteristics" title="Permanent link">&para;</a></h3>
<ul>
<li>Requires All Reduce or All Gather inside the forward and backward pass.</li>
<li>Communication is frequent and latency sensitive.</li>
<li>Performance depends heavily on interconnect bandwidth.</li>
</ul>
<h3 id="insights">Insights<a class="headerlink" href="#insights" title="Permanent link">&para;</a></h3>
<p><strong>Strengths</strong></p>
<ul>
<li>Enables training when individual layers do not fit on a single GPU.</li>
<li>Reduces per GPU activation and parameter memory.</li>
</ul>
<p><strong>Constraints</strong></p>
<ul>
<li>Communication overhead is high.</li>
<li>Usually restricted within a node due to bandwidth requirements.</li>
<li>Sensitive to imbalance across tensor shards.</li>
</ul>
<p><strong>Practical Usage</strong></p>
<ul>
<li>Often combined with Data Parallelism.</li>
<li>Popularized by Megatron LM style architectures.</li>
</ul>
<hr />
<h2 id="3-pipeline-parallelism-pp">3. Pipeline Parallelism (PP)<a class="headerlink" href="#3-pipeline-parallelism-pp" title="Permanent link">&para;</a></h2>
<p>Pipeline Parallelism splits the model by layer depth across devices.</p>
<h3 id="core-mechanism_1">Core Mechanism<a class="headerlink" href="#core-mechanism_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Each GPU owns a contiguous block of layers.</li>
<li>Activations flow forward through the pipeline.</li>
<li>Gradients flow backward in reverse order.</li>
</ul>
<h3 id="the-pipeline-bubble">The Pipeline Bubble<a class="headerlink" href="#the-pipeline-bubble" title="Permanent link">&para;</a></h3>
<ul>
<li>Without micro batching, only one stage is active at a time.</li>
<li>GPUs sit idle waiting for inputs or gradients.</li>
</ul>
<h3 id="micro-batching">Micro Batching<a class="headerlink" href="#micro-batching" title="Permanent link">&para;</a></h3>
<ul>
<li>The global batch is split into micro batches.</li>
<li>Multiple micro batches are in flight simultaneously.</li>
<li>Improves utilization at the cost of activation memory.</li>
</ul>
<h3 id="insights_1">Insights<a class="headerlink" href="#insights_1" title="Permanent link">&para;</a></h3>
<p><strong>Strengths</strong></p>
<ul>
<li>Reduces per GPU parameter memory.</li>
<li>Enables training extremely deep models.</li>
</ul>
<p><strong>Trade-offs</strong></p>
<ul>
<li>Increased activation memory footprint.</li>
<li>Pipeline schedule complexity.</li>
<li>Backward pass latency increases.</li>
</ul>
<p><strong>Systems Insight</strong></p>
<p>Pipeline parallelism improves memory scaling but hurts latency sensitive workloads.</p>
<hr />
<h2 id="4-zero-zero-redundancy-optimizer">4. ZeRO (Zero Redundancy Optimizer)<a class="headerlink" href="#4-zero-zero-redundancy-optimizer" title="Permanent link">&para;</a></h2>
<p>ZeRO is designed to <strong>eliminate memory redundancy</strong> in Data Parallel training. In standard DP, every GPU stores:</p>
<ol>
<li>Model parameters</li>
<li>Gradients</li>
<li>Optimizer states (e.g., Adam’s momentum and variance)</li>
</ol>
<p>This redundancy quickly becomes a bottleneck for very large models. ZeRO partitions these states across GPUs so that <strong>each GPU only stores a fraction of the total memory</strong>, enabling training of models that would otherwise not fit.</p>
<hr />
<h3 id="zero-stages">ZeRO Stages<a class="headerlink" href="#zero-stages" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Partitioned States</th>
<th>Memory Savings</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stage 1</td>
<td>Optimizer states</td>
<td>~4×</td>
<td>Each GPU keeps a shard of the optimizer states instead of a full copy. Gradients and parameters are still replicated.</td>
</tr>
<tr>
<td>Stage 2</td>
<td>Gradients</td>
<td>~8×</td>
<td>Gradients are also sharded. Each GPU contributes its shard to global All Reduce during backprop.</td>
</tr>
<tr>
<td>Stage 3</td>
<td>Parameters</td>
<td>N× (number of GPUs)</td>
<td>Even the model parameters are partitioned. Each GPU only holds a subset and fetches other shards when needed for computation.</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong><br />
Suppose you have a 10B parameter model across 8 GPUs:
- Stage 1: Each GPU holds 1/1 of parameters but 1/8 of optimizer states.<br />
- Stage 2: Each GPU holds 1/8 of gradients.<br />
- Stage 3: Each GPU holds 1/8 of parameters, gradients, and optimizer states.  </p>
<hr />
<h3 id="key-insight">Key Insight<a class="headerlink" href="#key-insight" title="Permanent link">&para;</a></h3>
<ul>
<li>Stage 1 and 2 mainly reduce memory replication.</li>
<li>Stage 3 reduces the largest memory cost: the <strong>parameters themselves</strong>.</li>
<li>At higher stages, computation must <strong>fetch shards of parameters or gradients from other GPUs</strong> on the fly.</li>
<li>Communication becomes the primary bottleneck, requiring overlap with computation for efficiency.</li>
</ul>
<hr />
<h3 id="zero-offload">ZeRO Offload<a class="headerlink" href="#zero-offload" title="Permanent link">&para;</a></h3>
<p>ZeRO can also offload states to CPU RAM or even NVMe storage to free GPU memory:</p>
<ul>
<li>Only the portion of optimizer states or parameters <strong>actively needed</strong> resides on the GPU.</li>
<li>Trades GPU memory pressure for <strong>PCIe / NVMe bandwidth</strong>.</li>
<li>Makes training extremely large models possible even with limited GPU memory (e.g., 10B+ parameters on 4×A100 40GB).</li>
</ul>
<p><strong>Practical Note:</strong><br />
Offloading is especially useful when the model is too big for Stage 3 alone or when using lower-end GPU clusters.</p>
<hr />
<h3 id="communication-patterns">Communication Patterns<a class="headerlink" href="#communication-patterns" title="Permanent link">&para;</a></h3>
<ul>
<li>Stage 1: Minimal communication; only optimizer state shards are reduced.</li>
<li>Stage 2: Requires All Reduce for gradient shards.</li>
<li>Stage 3: Each forward/backward pass may require fetching remote parameter shards.</li>
<li>Communication-computation overlap is critical to avoid GPU idling.</li>
</ul>
<hr />
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<ul>
<li>ZeRO scales memory linearly with the number of GPUs.</li>
<li>Stage 3 maximizes memory saving but <strong>communication overhead increases</strong>.</li>
<li>Poor network bandwidth or high latency can dominate runtime.</li>
<li>Elastic / hybrid parallelism often combines ZeRO with DP, TP, or PP for large-scale training.</li>
</ul>
<hr />
<h2 id="5-communication-and-memory-trade-offs">5. Communication and Memory Trade-offs<a class="headerlink" href="#5-communication-and-memory-trade-offs" title="Permanent link">&para;</a></h2>
<p>Distributed training is fundamentally constrained by a three way trade off between <strong>memory capacity</strong>, <strong>compute throughput</strong>, and <strong>communication bandwidth</strong>. Improving one dimension often degrades another, and real systems are designed to balance all three.</p>
<hr />
<h3 id="51-the-memorycomputecommunication-triangle">5.1 The Memory–Compute–Communication Triangle<a class="headerlink" href="#51-the-memorycomputecommunication-triangle" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Memory</strong> limits how large a model and batch can fit on a single GPU.</li>
<li><strong>Compute</strong> determines how fast forward and backward passes can be executed.</li>
<li><strong>Communication</strong> determines how quickly GPUs can synchronize parameters, gradients, or activations.</li>
</ul>
<p>For large models, memory is usually the first bottleneck. Techniques that reduce memory usage often increase either compute or communication cost.</p>
<h3 id="52-memory-accounting-with-adam-and-fp16">5.2 Memory Accounting with Adam and FP16<a class="headerlink" href="#52-memory-accounting-with-adam-and-fp16" title="Permanent link">&para;</a></h3>
<p>Assume a model with <code>Ψ</code> parameters trained using Adam and mixed precision.</p>
<h4 id="parameter-storage">Parameter Storage<a class="headerlink" href="#parameter-storage" title="Permanent link">&para;</a></h4>
<ul>
<li>Model weights stored in FP16 or BF16: <code>2Ψ</code> bytes</li>
</ul>
<h4 id="gradient-storage">Gradient Storage<a class="headerlink" href="#gradient-storage" title="Permanent link">&para;</a></h4>
<ul>
<li>Gradients stored in FP16 or BF16 during backpropagation: <code>2Ψ</code> bytes</li>
</ul>
<h4 id="optimizer-states">Optimizer States<a class="headerlink" href="#optimizer-states" title="Permanent link">&para;</a></h4>
<p>Adam maintains:</p>
<ul>
<li>FP32 master weights: <code>4Ψ</code> bytes  </li>
<li>First moment (momentum): <code>4Ψ</code> bytes  </li>
<li>Second moment (variance): <code>4Ψ</code> bytes  </li>
</ul>
<p>Total optimizer memory:<br />
<code>12Ψ</code> bytes</p>
<h4 id="total-training-memory">Total Training Memory<a class="headerlink" href="#total-training-memory" title="Permanent link">&para;</a></h4>
<p>Adding all components:</p>
<ul>
<li>Parameters: <code>2Ψ</code></li>
<li>Gradients: <code>2Ψ</code></li>
<li>Optimizer states: <code>12Ψ</code></li>
</ul>
<p><strong>Total:</strong> approximately <code>16Ψ</code> bytes per parameter</p>
<hr />
<h4 id="concrete-example">Concrete Example<a class="headerlink" href="#concrete-example" title="Permanent link">&para;</a></h4>
<p>For a 7B parameter model:</p>
<ul>
<li>7B × 16 bytes ≈ <strong>112 GB</strong></li>
</ul>
<p>This number excludes:</p>
<ul>
<li>Activation memory</li>
<li>Temporary buffers</li>
<li>Communication workspaces</li>
</ul>
<p>This explains why even a single training replica of a 7B model cannot fit on an 80 GB GPU without memory optimization techniques.</p>
<hr />
<h3 id="53-why-communication-becomes-the-bottleneck">5.3 Why Communication Becomes the Bottleneck<a class="headerlink" href="#53-why-communication-becomes-the-bottleneck" title="Permanent link">&para;</a></h3>
<p>As memory saving techniques reduce per GPU state:</p>
<ul>
<li>More parameter shards must be fetched remotely.</li>
<li>Gradients must be synchronized more frequently.</li>
<li>Communication moves from being infrequent and bulk to frequent and latency sensitive.</li>
</ul>
<p>High bandwidth interconnects and overlap with compute become critical.</p>
<hr />
<h3 id="54-memory-reduction-techniques-and-their-trade-offs">5.4 Memory Reduction Techniques and Their Trade-offs<a class="headerlink" href="#54-memory-reduction-techniques-and-their-trade-offs" title="Permanent link">&para;</a></h3>
<h4 id="activation-checkpointing">Activation Checkpointing<a class="headerlink" href="#activation-checkpointing" title="Permanent link">&para;</a></h4>
<ul>
<li>Saves memory by discarding activations during forward pass.</li>
<li>Recomputes activations during backward pass.</li>
<li>Trades memory for additional compute, typically 20 to 40 percent overhead.</li>
</ul>
<p><strong>When to use:</strong>  </p>
<p>Memory constrained training where compute is not the bottleneck.</p>
<hr />
<h4 id="mixed-precision-training">Mixed Precision Training<a class="headerlink" href="#mixed-precision-training" title="Permanent link">&para;</a></h4>
<ul>
<li>Uses FP16 or BF16 for forward and backward computation.</li>
<li>Keeps optimizer states in FP32 for numerical stability.</li>
<li>Reduces memory usage and communication bandwidth.</li>
</ul>
<p><strong>Key benefit:</strong>  </p>
<p>Improves both memory efficiency and throughput with minimal accuracy loss.</p>
<hr />
<h4 id="parameter-sharding">Parameter Sharding<a class="headerlink" href="#parameter-sharding" title="Permanent link">&para;</a></h4>
<ul>
<li>Splits parameters, gradients, or optimizer states across GPUs.</li>
<li>Removes redundancy present in Data Parallelism.</li>
<li>Increases communication during forward and backward passes.</li>
</ul>
<p><strong>Typical examples:</strong>  </p>
<p>ZeRO Stage 2 and Stage 3.</p>
<blockquote>
<p>Takeaway: Large scale training is constrained by a memory–compute–communication trade off. Techniques like mixed precision, activation checkpointing, and parameter sharding reduce memory pressure but introduce additional compute or communication costs. Efficient systems overlap communication with computation to avoid performance collapse.</p>
</blockquote>
<hr />
<h2 id="6-checkpointing-and-fault-tolerance">6. Checkpointing and Fault Tolerance<a class="headerlink" href="#6-checkpointing-and-fault-tolerance" title="Permanent link">&para;</a></h2>
<p>At large cluster scale, hardware and network failures are expected. Training systems must be designed to recover quickly with minimal loss of progress.</p>
<hr />
<h3 id="61-checkpointing-strategies">6.1 Checkpointing Strategies<a class="headerlink" href="#61-checkpointing-strategies" title="Permanent link">&para;</a></h3>
<p><strong>Full Checkpoints</strong></p>
<ul>
<li>Store model parameters, optimizer states, and RNG state.</li>
<li>Enable exact training resumption.</li>
<li>Expensive in terms of storage size and write time.</li>
</ul>
<details>

<summary> RNG State (Random Number Generator State) </summary>

1. Why RNG State Matters

Randomness is used in multiple parts of training:

- Weight initialization
- Dropout masks
- Data shuffling
- Data augmentation
- Stochastic layers or kernels

If training is resumed **without restoring RNG state**:

- Dropout patterns change
- Data order may differ
- Gradient noise changes

As a result, training can diverge from the original run, making debugging and reproducibility difficult.

2. What Is Typically Included

A full checkpoint usually stores RNG states for:

- Python `random`
- NumPy RNG
- PyTorch CPU RNG
- PyTorch CUDA RNG (per GPU)

In distributed training, each worker maintains its own RNG state, which must be saved and restored independently.

</details>

<p><strong>Sharded Checkpoints</strong></p>
<ul>
<li>Each worker writes only its shard of parameters or optimizer state.</li>
<li>Reduces checkpoint time and I/O contention.</li>
<li>Requires coordinated restore logic.</li>
</ul>
<p><strong>Asynchronous Checkpointing</strong></p>
<ul>
<li>Checkpointing runs in the background while training continues.</li>
<li>Avoids blocking GPUs on slow storage.</li>
<li>Slightly increases complexity and memory usage.</li>
</ul>
<hr />
<h3 id="62-fault-tolerance">6.2 Fault Tolerance<a class="headerlink" href="#62-fault-tolerance" title="Permanent link">&para;</a></h3>
<p><strong>Elastic Training</strong></p>
<ul>
<li>Allows workers to join or leave during training.</li>
<li>Automatically rebalances data and workloads.</li>
<li>Commonly implemented using PyTorch Elastic or TorchRun.</li>
</ul>
<p><strong>Health Checks</strong></p>
<ul>
<li>Detect hung, slow, or non communicating workers.</li>
<li>Prevents a single faulty GPU from stalling the entire job.</li>
</ul>
<p><strong>Straggler Detection</strong></p>
<ul>
<li>Identifies workers that are consistently slower.</li>
<li>Helps avoid synchronization delays in collective operations.</li>
</ul>
<hr />
<blockquote>
<p>Takeaway: Checkpointing is fundamentally an I/O and systems problem. Efficient training requires minimizing checkpoint overhead while ensuring fast and correct recovery from failures.</p>
</blockquote>
<hr />
<h2 id="7-advanced-parallelism-and-optimization-topics">7. Advanced Parallelism and Optimization Topics<a class="headerlink" href="#7-advanced-parallelism-and-optimization-topics" title="Permanent link">&para;</a></h2>
<p>These topics often differentiate strong systems candidates.</p>
<h3 id="71-flashattention">7.1 FlashAttention<a class="headerlink" href="#71-flashattention" title="Permanent link">&para;</a></h3>
<ul>
<li>Computes attention in tiles to avoid materializing full attention matrices.</li>
<li>Reduces memory from quadratic to linear in sequence length.</li>
<li>Improves both speed and memory usage.</li>
</ul>
<hr />
<h3 id="72-mixture-of-experts-moe">7.2 Mixture of Experts (MoE)<a class="headerlink" href="#72-mixture-of-experts-moe" title="Permanent link">&para;</a></h3>
<ul>
<li>Sparse activation where only a subset of parameters are used per token.</li>
<li>Requires expert parallelism and routing strategies.</li>
<li>Trades compute efficiency for model capacity.</li>
</ul>
<hr />
<h3 id="73-sequence-parallelism">7.3 Sequence Parallelism<a class="headerlink" href="#73-sequence-parallelism" title="Permanent link">&para;</a></h3>
<p>Sequence parallelism splits the <strong>sequence length dimension</strong> of the input across multiple devices, rather than splitting model parameters or batch elements.</p>
<h4 id="core-idea">Core Idea<a class="headerlink" href="#core-idea" title="Permanent link">&para;</a></h4>
<p>For an input tensor with shape:</p>
<p><code>[batch, sequence_length, hidden_dim]</code></p>
<ul>
<li>The sequence length dimension is partitioned across GPUs.</li>
<li>Each GPU processes a contiguous chunk of tokens from the sequence.</li>
<li>This reduces per GPU activation memory, which scales linearly with sequence length.</li>
</ul>
<h4 id="why-it-is-useful">Why It Is Useful<a class="headerlink" href="#why-it-is-useful" title="Permanent link">&para;</a></h4>
<ul>
<li>Attention and activation memory grow with sequence length.</li>
<li>Very long context models (e.g., 32k, 64k, or 128k tokens) quickly exceed GPU memory limits.</li>
<li>Sequence parallelism allows long sequences to fit by distributing token level computation.</li>
</ul>
<h4 id="communication-pattern">Communication Pattern<a class="headerlink" href="#communication-pattern" title="Permanent link">&para;</a></h4>
<ul>
<li>Certain operations, such as attention and layer normalization, require communication across sequence shards.</li>
<li>Communication typically uses All Gather or All Reduce.</li>
<li>Efficient overlap with computation is necessary to avoid performance loss.</li>
</ul>
<h4 id="relationship-with-tensor-parallelism">Relationship with Tensor Parallelism<a class="headerlink" href="#relationship-with-tensor-parallelism" title="Permanent link">&para;</a></h4>
<ul>
<li>Sequence parallelism is often combined with tensor parallelism.</li>
<li>Tensor parallelism splits hidden dimensions, while sequence parallelism splits tokens.</li>
<li>This combination balances memory usage and communication overhead.</li>
</ul>
<blockquote>
<p>Takeaway: Sequence parallelism distributes tokens across GPUs to reduce activation memory for long context models, trading additional communication for the ability to train with very large sequence lengths.</p>
</blockquote>
<hr />
<h3 id="74-low-precision-training">7.4 Low Precision Training<a class="headerlink" href="#74-low-precision-training" title="Permanent link">&para;</a></h3>
<ul>
<li>FP8 reduces bandwidth and memory.</li>
<li>Requires careful scaling and error management.</li>
<li>Hardware dependent and increasingly common on newer accelerators.</li>
</ul>
<hr />
<h2 id="8-additional-common-interview-topics">8. Additional Common Interview Topics<a class="headerlink" href="#8-additional-common-interview-topics" title="Permanent link">&para;</a></h2>
<p>These are frequently asked in senior level ML systems interviews.</p>
<h3 id="81-hybrid-parallelism">8.1 Hybrid Parallelism<a class="headerlink" href="#81-hybrid-parallelism" title="Permanent link">&para;</a></h3>
<ul>
<li>Real systems combine DP, TP, PP, and ZeRO.</li>
<li>Interviewers often ask how to scale from 1 GPU to hundreds or thousands.</li>
<li>A strong answer mentions hierarchical parallelism.</li>
</ul>
<hr />
<h3 id="82-throughput-vs-latency">8.2 Throughput vs Latency<a class="headerlink" href="#82-throughput-vs-latency" title="Permanent link">&para;</a></h3>
<p>Throughput and latency represent two different optimization goals, and distributed systems make very different design choices depending on which one is prioritized.</p>
<hr />
<h4 id="training-throughput-oriented">Training: Throughput Oriented<a class="headerlink" href="#training-throughput-oriented" title="Permanent link">&para;</a></h4>
<ul>
<li>The primary goal in training is <strong>maximum tokens processed per second</strong>.</li>
<li>Large batch sizes are used to fully utilize GPUs.</li>
<li>Parallelism strategies favor efficiency even if individual requests are slow.</li>
</ul>
<p><strong>Common choices in training</strong></p>
<ul>
<li>Data parallelism with large global batches.</li>
<li>Pipeline parallelism with micro batching.</li>
<li>Aggressive overlap of communication and computation.</li>
<li>Higher tolerance for end to end latency per batch.</li>
</ul>
<hr />
<h4 id="inference-latency-oriented">Inference: Latency Oriented<a class="headerlink" href="#inference-latency-oriented" title="Permanent link">&para;</a></h4>
<ul>
<li>The primary goal in inference is <strong>fast response time per request</strong>.</li>
<li>Batch sizes are often small or dynamic.</li>
<li>Minimizing synchronization and communication is critical.</li>
</ul>
<p><strong>Common choices in inference</strong></p>
<ul>
<li>Replication rather than sharding for small models.</li>
<li>Limited or no pipeline parallelism due to bubble overhead.</li>
<li>Kernel fusion and caching over global synchronization.</li>
</ul>
<hr />
<h4 id="why-parallelism-strategies-differ">Why Parallelism Strategies Differ<a class="headerlink" href="#why-parallelism-strategies-differ" title="Permanent link">&para;</a></h4>
<ul>
<li>Training can amortize communication over large batches.</li>
<li>Inference cannot hide communication latency as easily.</li>
<li>Techniques like tensor or pipeline parallelism may improve throughput but often increase per request latency.</li>
</ul>
<blockquote>
<p>Takeaway: Training systems optimize for throughput using large batches and heavy parallelism, while inference systems optimize for low latency by minimizing synchronization and communication, leading to fundamentally different parallelism strategies.</p>
</blockquote>
<hr />
<h3 id="83-gradient-accumulation">8.3 Gradient Accumulation<a class="headerlink" href="#83-gradient-accumulation" title="Permanent link">&para;</a></h3>
<p>Gradient accumulation is a technique used to simulate a larger batch size by accumulating gradients over multiple forward and backward passes before performing an optimizer update.</p>
<h4 id="core-idea_1">Core Idea<a class="headerlink" href="#core-idea_1" title="Permanent link">&para;</a></h4>
<ul>
<li>Instead of updating model parameters after every mini batch, gradients are accumulated over <code>K</code> steps.</li>
<li>The optimizer step is executed only once after all <code>K</code> gradients are accumulated.</li>
<li>This creates an <strong>effective batch size</strong> of <code>K × mini_batch_size</code>.</li>
</ul>
<hr />
<h4 id="why-it-saves-memory">Why It Saves Memory<a class="headerlink" href="#why-it-saves-memory" title="Permanent link">&para;</a></h4>
<ul>
<li>Each step processes a small mini batch that fits in GPU memory.</li>
<li>Activations are freed after each backward pass.</li>
<li>Only gradients are accumulated, avoiding the need to store a large batch at once.</li>
</ul>
<hr />
<h4 id="communication-benefits-in-distributed-training">Communication Benefits in Distributed Training<a class="headerlink" href="#communication-benefits-in-distributed-training" title="Permanent link">&para;</a></h4>
<ul>
<li>In Data Parallel training, gradient synchronization normally happens every step.</li>
<li>With gradient accumulation, synchronization happens only once every <code>K</code> steps.</li>
<li>This reduces communication frequency and improves scalability.</li>
</ul>
<hr />
<h4 id="common-confusion-with-data-parallelism">Common Confusion with Data Parallelism<a class="headerlink" href="#common-confusion-with-data-parallelism" title="Permanent link">&para;</a></h4>
<ul>
<li>Data parallelism distributes different data samples across GPUs.</li>
<li>Gradient accumulation repeats multiple steps <strong>on the same GPU</strong> before synchronization.</li>
<li>The two techniques are complementary and often used together.</li>
</ul>
<hr />
<h4 id="practical-considerations">Practical Considerations<a class="headerlink" href="#practical-considerations" title="Permanent link">&para;</a></h4>
<ul>
<li>Learning rate schedules often need adjustment for large effective batch sizes.</li>
<li>Loss values are usually scaled to avoid gradient magnitude changes.</li>
<li>Very large accumulation steps can slow convergence.</li>
</ul>
<blockquote>
<p>Takeaway: Gradient accumulation increases effective batch size by accumulating gradients across multiple steps, reducing memory usage and communication frequency, and is often combined with data parallelism to scale training efficiently.</p>
</blockquote>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>