
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="..">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>LoRA - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lora-low-rank-adaptation-a-practical-guide-for-fine-tuning-llms" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LoRA
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fine-Tuning Techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Fine-Tuning Techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-quick-summary-tldr" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quick summary / TL;DR
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-lora-the-idea-and-math" class="md-nav__link">
    <span class="md-ellipsis">
      3. LoRA: the idea and math
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-where-to-apply-lora-in-a-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      4. Where to apply LoRA in a Transformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-implementation-recipes" class="md-nav__link">
    <span class="md-ellipsis">
      5. Implementation recipes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Implementation recipes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-minimal-pytorch-style-lora-layer-pseudocode" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Minimal PyTorch-style LoRA layer (pseudocode)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-hugging-face-peft-recommended" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Hugging Face + PEFT (recommended)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-hyperparameters-heuristics" class="md-nav__link">
    <span class="md-ellipsis">
      6. Hyperparameters &amp; heuristics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-practical-training-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      7. Practical training configurations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-common-issues-and-concrete-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      8. Common issues and concrete solutions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Common issues and concrete solutions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#issue-gpu-oom-during-training" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: GPU OOM during training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-choosing-r-incorrectly-underoverfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Choosing r incorrectly (under/overfitting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-divergence-instability-in-training" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Divergence / instability in training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-quantization-compatibility-lora-with-8-bit-4-bit" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Quantization compatibility (LoRA with 8-bit / 4-bit)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-overfitting-on-small-static-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Overfitting on small static datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-adapter-conflicts-when-stacking-multiple-lora-modules" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Adapter conflicts when stacking multiple LoRA modules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-savingloading-mismatches" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Saving/loading mismatches
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-best-practices-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      9. Best practices &amp; checklist
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-faq-answers-summarised" class="md-nav__link">
    <span class="md-ellipsis">
      10. FAQ (answers summarised)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-references-further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      12. References &amp; further reading
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qlora.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PEFT Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adapters.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adapter Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Experiments
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Experiments
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../experiments/lora_experiments.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA Experiments
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../experiments/qlora_experiments.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QLoRA Experiments
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../references.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-quick-summary-tldr" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quick summary / TL;DR
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-lora-the-idea-and-math" class="md-nav__link">
    <span class="md-ellipsis">
      3. LoRA: the idea and math
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-where-to-apply-lora-in-a-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      4. Where to apply LoRA in a Transformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-implementation-recipes" class="md-nav__link">
    <span class="md-ellipsis">
      5. Implementation recipes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Implementation recipes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-minimal-pytorch-style-lora-layer-pseudocode" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Minimal PyTorch-style LoRA layer (pseudocode)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-hugging-face-peft-recommended" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Hugging Face + PEFT (recommended)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-hyperparameters-heuristics" class="md-nav__link">
    <span class="md-ellipsis">
      6. Hyperparameters &amp; heuristics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-practical-training-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      7. Practical training configurations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-common-issues-and-concrete-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      8. Common issues and concrete solutions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Common issues and concrete solutions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#issue-gpu-oom-during-training" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: GPU OOM during training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-choosing-r-incorrectly-underoverfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Choosing r incorrectly (under/overfitting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-divergence-instability-in-training" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Divergence / instability in training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-quantization-compatibility-lora-with-8-bit-4-bit" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Quantization compatibility (LoRA with 8-bit / 4-bit)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-overfitting-on-small-static-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Overfitting on small static datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-adapter-conflicts-when-stacking-multiple-lora-modules" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Adapter conflicts when stacking multiple LoRA modules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-savingloading-mismatches" class="md-nav__link">
    <span class="md-ellipsis">
      Issue: Saving/loading mismatches
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-best-practices-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      9. Best practices &amp; checklist
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-faq-answers-summarised" class="md-nav__link">
    <span class="md-ellipsis">
      10. FAQ (answers summarised)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-references-further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      12. References &amp; further reading
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="lora-low-rank-adaptation-a-practical-guide-for-fine-tuning-llms">LoRA (Low-Rank Adaptation) — A Practical Guide for Fine-tuning LLMs<a class="headerlink" href="#lora-low-rank-adaptation-a-practical-guide-for-fine-tuning-llms" title="Permanent link">&para;</a></h1>
<blockquote>
<p>A practical, hands-on guide for using LoRA adapters to fine-tune large language models (LLMs). This document compiles conceptual background, implementation recipes, hyperparameter heuristics, common pitfalls and solutions, and answers to frequently asked questions.</p>
</blockquote>
<hr />
<h2 id="table-of-contents">Table of contents<a class="headerlink" href="#table-of-contents" title="Permanent link">&para;</a></h2>
<ol>
<li>Introduction</li>
<li>Quick summary / TL;DR</li>
<li>LoRA: the idea and math</li>
<li>Where to apply LoRA in a Transformer</li>
<li>Implementation recipes</li>
<li>Minimal PyTorch pseudocode</li>
<li>Hugging Face + PEFT example</li>
<li>Working with quantized models (QLoRA)</li>
<li>Hyperparameters &amp; heuristics (rank <code>r</code>, alpha, learning rate)</li>
<li>Practical training configurations (memory, precision, optimizers)</li>
<li>Common issues and concrete solutions</li>
<li>Best practices &amp; checklist</li>
<li>FAQ (ten targeted questions)
1References &amp; further reading</li>
</ol>
<hr />
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>LoRA (Low-Rank Adaptation) is a parameter-efficient way to fine-tune large pretrained models by injecting small low-rank learnable matrices into existing weight projections instead of updating the entire model. LoRA reduces trainable parameters dramatically while preserving the pretrained weights — making it ideal when GPU memory or compute is constrained.</p>
<p>This guide combines broadly useful LoRA knowledge with practical insights and caveats distilled from hundreds of experiments (notably, the "Practical Tips for Finetuning LLMs Using LoRA" writeup by Sebastian Raschka).</p>
<hr />
<h2 id="2-quick-summary-tldr">2. Quick summary / TL;DR<a class="headerlink" href="#2-quick-summary-tldr" title="Permanent link">&para;</a></h2>
<ul>
<li>LoRA replaces full-weight updates ΔW by a low-rank decomposition: ΔW = A · B, where <code>A</code> and <code>B</code> are small trainable matrices and the base weight <code>W</code> remains frozen.</li>
<li>Typical ranks <code>r</code> are small (e.g., 4–64) for most use cases; increasing <code>r</code> increases capacity and memory usage.</li>
<li>A commonly used heuristic is <code>alpha ≈ 2 × r</code> (scaling factor), but it's worth tuning for large <code>r</code> values.</li>
<li>QLoRA (quantized LoRA) reduces memory usage further (e.g., ~33%) at the cost of additional runtime (~39% slower in reported experiments) and more complex setup.</li>
<li>Avoid naïve multi-epoch training on small static instruction datasets — this often hurts performance (likely overfitting).</li>
</ul>
<hr />
<h2 id="3-lora-the-idea-and-math">3. LoRA: the idea and math<a class="headerlink" href="#3-lora-the-idea-and-math" title="Permanent link">&para;</a></h2>
<p>Suppose a pretrained linear layer has weights <code>W ∈ R^{d_out×d_in}</code>. Standard fine-tuning updates <code>W</code> directly (ΔW). LoRA parametrizes the update as:</p>
<div class="highlight"><pre><span></span><code>ΔW = A @ B
</code></pre></div>
<p>where <code>A ∈ R^{d_out×r}</code>, <code>B ∈ R^{r×d_in}</code>, and <code>r &lt;&lt; min(d_out, d_in)</code>.</p>
<p>During forward pass we compute:</p>
<div class="highlight"><pre><span></span><code>y = W x + scaling * (A (B x))
</code></pre></div>
<p><code>scaling</code> is commonly <code>alpha / r</code>.</p>
<p>This reduces stored trainable parameters from <code>d_out×d_in</code> to <code>(d_out + d_in)×r</code> and avoids storing large optimizer states for the frozen base model.</p>
<hr />
<h2 id="4-where-to-apply-lora-in-a-transformer">4. Where to apply LoRA in a Transformer<a class="headerlink" href="#4-where-to-apply-lora-in-a-transformer" title="Permanent link">&para;</a></h2>
<p>Most practitioners apply LoRA to the attention projection matrices because adapting attention often yields high returns:</p>
<ul>
<li>Query (<code>W_q</code>)</li>
<li>Key (<code>W_k</code>)</li>
<li>Value (<code>W_v</code>)</li>
<li>Output projection (<code>W_o</code>)</li>
</ul>
<p>You can also enable LoRA on intermediate feed-forward (MLP) projections or projection layers between attention blocks. Enabling LoRA for <em>more</em> layers increases capacity (and memory) and frequently improves performance — e.g., enabling LoRA across all transformer layers may multiply the number of trainable parameters by ~5× compared to only Q &amp; V.</p>
<hr />
<h2 id="5-implementation-recipes">5. Implementation recipes<a class="headerlink" href="#5-implementation-recipes" title="Permanent link">&para;</a></h2>
<h3 id="51-minimal-pytorch-style-lora-layer-pseudocode">5.1 Minimal PyTorch-style LoRA layer (pseudocode)<a class="headerlink" href="#51-minimal-pytorch-style-lora-layer-pseudocode" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LoRA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">orig_linear</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">orig</span> <span class="o">=</span> <span class="n">orig_linear</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">r</span>
        <span class="c1"># LoRA: A and B (we follow convention B then A in many libs)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">orig_linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">orig_linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
        <span class="c1"># init: small values</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">base</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">orig</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">lora_update</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">T</span>
        <span class="k">return</span> <span class="n">base</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="n">lora_update</span>
</code></pre></div>
<blockquote>
<p>Notes: this is illustrative. For batching and shapes ensure correct broadcasting. Many libraries (PEFT, LoRA implementations) implement optimizations to avoid extra copies and to fuse operations.</p>
</blockquote>
<h3 id="52-hugging-face-peft-recommended">5.2 Hugging Face + PEFT (recommended)<a class="headerlink" href="#52-hugging-face-peft-recommended" title="Permanent link">&para;</a></h3>
<p><code>PEFT</code> (Parameter-Efficient Fine-Tuning) by Hugging Face is the de-facto standard for LoRA integration with Transformers. It abstracts away wiring adapters and saving/loading.</p>
<p>Example outline (pseudo-commands):</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">],</span>  <span class="c1"># example names depending on model</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="c1"># Then use Trainer or custom loop with model.parameters() (only LoRA params are trainable)</span>
</code></pre></div>
<p>PEFT handles saving and loading adapters via <code>model.save_pretrained(path)</code> and <code>from_pretrained</code> utilities.</p>
<hr />
<h2 id="6-hyperparameters-heuristics">6. Hyperparameters &amp; heuristics<a class="headerlink" href="#6-hyperparameters-heuristics" title="Permanent link">&para;</a></h2>
<p><strong>Rank <code>r</code></strong></p>
<ul>
<li>Typical starting values: <code>r = 4, 8, 16</code> for small/medium tasks.</li>
<li>For more complex adaptation or domain shift, try <code>r</code> up to <code>64</code> or <code>256</code> (if memory allows). In experiments, <code>r=256</code> sometimes outperforms smaller <code>r</code> for certain tasks, but be mindful of memory.</li>
</ul>
<p><strong>Alpha (<code>α</code>) / scaling</strong></p>
<ul>
<li>Heuristic: <code>alpha = 2 × r</code> is a stable starting point.</li>
<li>Compute <code>scaling = alpha / r</code>. If <code>alpha</code> is large relative to <code>r</code>, LoRA updates have a larger influence on outputs.</li>
<li>Empirical note: <code>alpha = 2×r</code> is a good default, but at large <code>r</code> it may be worth trying smaller relative alpha (e.g., in experiments <code>r=256</code> with <code>alpha=128</code> gave strong results).</li>
</ul>
<p><strong>Learning rate</strong></p>
<ul>
<li>LoRA often works well with modest learning rates: <code>1e-4</code> to <code>5e-4</code> for many setups, but tune per-task.</li>
<li>Smaller lr helps avoid overfitting / catastrophic drift from pretrained behavior.</li>
</ul>
<p><strong>Dropout (lora_dropout)</strong></p>
<ul>
<li>Small dropout (0.05) can help regularize LoRA weights on smaller datasets.</li>
</ul>
<hr />
<h2 id="7-practical-training-configurations">7. Practical training configurations<a class="headerlink" href="#7-practical-training-configurations" title="Permanent link">&para;</a></h2>
<p><strong>Memory &amp; precision</strong></p>
<ul>
<li>Use mixed precision (<code>fp16</code> or <code>bf16</code>) to reduce memory and speed up training. Most GPUs and Transformers support <code>torch_dtype=torch.float16</code> or <code>accelerate</code> flags.</li>
<li>Consider gradient checkpointing to reduce activation memory at the cost of extra compute.</li>
</ul>
<p><strong>Batch size &amp; gradient accumulation</strong></p>
<ul>
<li>Small GPUs may need tiny per-device batch sizes; compensate with gradient accumulation to simulate larger batches.</li>
</ul>
<p><strong>Optimizers</strong></p>
<ul>
<li>AdamW is the default and works well for most <code>r</code> values.</li>
<li>If <code>r</code> is small, switching from AdamW to SGD yields minimal memory savings. For large <code>r</code> (e.g., 256), SGD can reduce optimizer memory because Adam stores additional moment estimates.</li>
</ul>
<p><strong>Epochs &amp; dataset iterations</strong></p>
<ul>
<li>For static/small instruction datasets, multiple epochs can harm performance through overfitting. Consider single-epoch passes with thorough data curation, or heavy data augmentation / mixing diverse sources.</li>
</ul>
<hr />
<h2 id="8-common-issues-and-concrete-solutions">8. Common issues and concrete solutions<a class="headerlink" href="#8-common-issues-and-concrete-solutions" title="Permanent link">&para;</a></h2>
<p>This section enumerates common pitfalls and what to do about them.</p>
<h3 id="issue-gpu-oom-during-training">Issue: GPU OOM during training<a class="headerlink" href="#issue-gpu-oom-during-training" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Lower <code>r</code> (rank) for LoRA adapters.</li>
<li>Use QLoRA (4-bit quantization) to reduce model memory footprint.</li>
<li>Enable <code>torch.cuda.amp</code> mixed precision (<code>fp16</code>/<code>bf16</code>).</li>
<li>Use gradient checkpointing.</li>
<li>Reduce batch size and use gradient accumulation.</li>
<li>Offload frozen model weights to CPU (Hugging Face <code>accelerate</code> device_map and <code>offload_folder</code> options).</li>
</ul>
<h3 id="issue-choosing-r-incorrectly-underoverfitting">Issue: Choosing <code>r</code> incorrectly (under/overfitting)<a class="headerlink" href="#issue-choosing-r-incorrectly-underoverfitting" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Start small (e.g., 8) and increase if validation performance plateaus/underfits.</li>
<li>If model capacity seems insufficient for the task, increase <code>r</code> incrementally. Monitor memory use.</li>
</ul>
<h3 id="issue-divergence-instability-in-training">Issue: Divergence / instability in training<a class="headerlink" href="#issue-divergence-instability-in-training" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Lower learning rate.</li>
<li>Reduce alpha or use <code>alpha = 2*r</code> heuristic.</li>
<li>Add small dropout to LoRA layers.</li>
<li>Use learning rate schedulers (cosine annealing / half-cycle often helps SGD more than Adam variants).</li>
</ul>
<h3 id="issue-quantization-compatibility-lora-with-8-bit-4-bit">Issue: Quantization compatibility (LoRA with 8-bit / 4-bit)<a class="headerlink" href="#issue-quantization-compatibility-lora-with-8-bit-4-bit" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use libs designed for quantization + adapters (e.g., <code>bitsandbytes</code> + HuggingFace + PEFT). Many community recipes load base model in 4-bit, add LoRA using PEFT, and train with paged optimizers.</li>
<li>Validate that numeric stability is acceptable; QLoRA tends to slightly increase runtime and may need hyperparameter tweaks.</li>
</ul>
<h3 id="issue-overfitting-on-small-static-datasets">Issue: Overfitting on small static datasets<a class="headerlink" href="#issue-overfitting-on-small-static-datasets" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use early stopping and validation monitoring.</li>
<li>Mix diverse datasets or use data augmentation.</li>
<li>Use smaller <code>r</code> and smaller lr; apply regularization.</li>
<li>Avoid unnecessary multiple epochs on small instruction-only sets.</li>
</ul>
<h3 id="issue-adapter-conflicts-when-stacking-multiple-lora-modules">Issue: Adapter conflicts when stacking multiple LoRA modules<a class="headerlink" href="#issue-adapter-conflicts-when-stacking-multiple-lora-modules" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Avoid enabling LoRA on the exact same submodule in conflicting adapters unless you plan adapter fusion.</li>
<li>Use sequential or non-overlapping layer adaptation.</li>
<li>Consider adapter fusion techniques if combining multiple task adapters.</li>
</ul>
<h3 id="issue-savingloading-mismatches">Issue: Saving/loading mismatches<a class="headerlink" href="#issue-savingloading-mismatches" title="Permanent link">&para;</a></h3>
<p><strong>Solutions:</strong></p>
<ul>
<li>Prefer PEFT's save/load utilities (<code>model.save_pretrained(...)</code> and <code>PeftModel.from_pretrained(...)</code>) to manage adapter metadata and ensure correct wiring on load.</li>
</ul>
<hr />
<h2 id="9-best-practices-checklist">9. Best practices &amp; checklist<a class="headerlink" href="#9-best-practices-checklist" title="Permanent link">&para;</a></h2>
<ul class="task-list">
<li class="task-list-item"><input type="checkbox" disabled/> Start with a small rank (<code>r = 4–16</code>) and <code>alpha = 2*r</code> as a baseline.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Freeze base model weights; only LoRA params should be trainable.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Use mixed precision and gradient checkpointing where possible.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Use PEFT/Hugging Face tooling to manage adapters reliably.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Monitor validation performance closely — be conservative with epoch count on small datasets.</li>
<li class="task-list-item"><input type="checkbox" disabled/> If memory-limited, try QLoRA and bitsandbytes-backed 4-bit loading.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Keep training logs and seed multiple runs if you require reproducibility — results are generally consistent but still benefit from plotting.</li>
</ul>
<hr />
<h2 id="10-faq-answers-summarised">10. FAQ (answers summarised)<a class="headerlink" href="#10-faq-answers-summarised" title="Permanent link">&para;</a></h2>
<p><strong>Q1: How important is the dataset?</strong></p>
<ul>
<li>Extremely. LoRA adapts model behavior based on the finetuning data. For instruction tuning and domain adaptation, richness and diversity of examples matter more than repeated passes over a small set.</li>
</ul>
<p><strong>Q2: Does LoRA work for domain adaptation?</strong></p>
<ul>
<li>Yes — LoRA is well-suited for domain adaptation because it allows task- or domain-specific lightweight adjustments without altering base knowledge.</li>
</ul>
<p><strong>Q3: How to avoid overfitting?</strong></p>
<ul>
<li>Use conservative epochs, regularization, smaller ranks/lr, mixed datasets, and early stopping. For instruction finetuning, multiple epochs on small datasets often degrade performance.</li>
</ul>
<p><strong>Q4: What other factors influence memory usage?</strong></p>
<ul>
<li>Model size, <code>r</code>, precision (<code>fp16</code>/<code>bf16</code> vs <code>fp32</code>), optimizer states, batch size, sequence length, gradient checkpointing, and whether you offload weights.</li>
</ul>
<p><strong>Q5: How does LoRA compare to full finetuning and RLHF?</strong></p>
<ul>
<li>LoRA is a parameter-efficient alternative to full finetuning (less memory, storage and faster). RLHF is a separate training paradigm (reward modeling and policy optimization) used to align models; LoRA is a mechanism to adapt parameters, not a replacement for RLHF.</li>
</ul>
<p><strong>Q6: Can LoRA weights be combined?</strong></p>
<ul>
<li>Yes — you can merge or sequentially apply adapters, and adapter fusion methods exist. Be careful about overlapping modifications to the same submodules.</li>
</ul>
<p><strong>Q7: What about layer-wise optimal rank adaptation?</strong></p>
<ul>
<li>It's plausible that different layers need different <code>r</code> values. Automated layer-wise rank selection is an active topic. A practical approach is to tune <code>r</code> for groups of layers or use heuristics (more adaptation for later layers may help in some tasks).</li>
</ul>
<hr />
<h2 id="12-references-further-reading">12. References &amp; further reading<a class="headerlink" href="#12-references-further-reading" title="Permanent link">&para;</a></h2>
<ul>
<li>Practical Tips for Finetuning LLMs Using LoRA — Sebastian Raschka (Ahead of AI / Magazine): <a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</a></li>
<li>LoRA paper and original references (Hu et al.) — search <code>LoRA arXiv Hu et al.</code></li>
<li>QLoRA (Dettmers et al.) — for 4-bit quantized LoRA approaches</li>
<li>Hugging Face PEFT documentation — parameter-efficient fine-tuning utilities</li>
</ul>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>