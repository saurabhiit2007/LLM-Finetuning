model:
  name: "mistralai/Mistral-7B-v0.1"
  torch_dtype: "float16"
  device_map: "auto"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  block_size: 128
  train_subset_size:
  eval_subset_size:

training:
  output_dir: "lora/output_vanilla_finetuned"
  results_logging: "lora/logs"
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  save_steps: 1000
  logging_steps: 500
  learning_rate: 2e-4
  fp16: true
  save_strategy: "epoch"
  evaluation_strategy: "epoch"
  load_best_model_at_end: true

generation:
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  test_prompt: "In a shocking discovery,"

